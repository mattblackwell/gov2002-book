<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A User’s Guide to Statistical Inference and Regression - 3&nbsp; Asymptotics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04_hypothesis_tests.html" rel="next">
<link href="./02_estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_estimation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_asymptotics.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hypothesis_tests.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis Tests</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_confidence_intervals.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Confidence intervals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_linear_model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">3.1</span>  Introduction</a></li>
  <li><a href="#why-convergence-with-probability-is-hard" id="toc-why-convergence-with-probability-is-hard" class="nav-link" data-scroll-target="#why-convergence-with-probability-is-hard"><span class="toc-section-number">3.2</span>  Why convergence with probability is hard</a></li>
  <li><a href="#convergence-in-probability-and-consistency" id="toc-convergence-in-probability-and-consistency" class="nav-link" data-scroll-target="#convergence-in-probability-and-consistency"><span class="toc-section-number">3.3</span>  Convergence in probability and consistency</a></li>
  <li><a href="#useful-inequalities" id="toc-useful-inequalities" class="nav-link" data-scroll-target="#useful-inequalities"><span class="toc-section-number">3.4</span>  Useful inequalities</a></li>
  <li><a href="#the-law-of-large-numbers" id="toc-the-law-of-large-numbers" class="nav-link" data-scroll-target="#the-law-of-large-numbers"><span class="toc-section-number">3.5</span>  The law of large numbers</a></li>
  <li><a href="#consistency-of-estimators" id="toc-consistency-of-estimators" class="nav-link" data-scroll-target="#consistency-of-estimators"><span class="toc-section-number">3.6</span>  Consistency of estimators</a></li>
  <li><a href="#convergence-in-distribution-and-the-central-limit-theorem" id="toc-convergence-in-distribution-and-the-central-limit-theorem" class="nav-link" data-scroll-target="#convergence-in-distribution-and-the-central-limit-theorem"><span class="toc-section-number">3.7</span>  Convergence in distribution and the central limit theorem</a></li>
  <li><a href="#delta-method" id="toc-delta-method" class="nav-link" data-scroll-target="#delta-method"><span class="toc-section-number">3.8</span>  Delta method</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Unif}{\text{Unif}}
\newcommand{\se}{\textsf{se}}
\newcommand{\au}{\underline{a}}
\newcommand{\du}{\underline{d}}
\newcommand{\Au}{\underline{A}}
\newcommand{\Du}{\underline{D}}
\newcommand{\xu}{\underline{x}}
\newcommand{\Xu}{\underline{X}}
\newcommand{\Yu}{\underline{Y}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\boldsymbol}
\newcommand{\U}{\mb{U}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\bbL}{\mathbb{L}}
\renewcommand{\u}{\mb{u}}
\renewcommand{\v}{\mb{v}}
\newcommand{\M}{\mb{M}}
\newcommand{\X}{\mb{X}}
\newcommand{\Xmat}{\mathbb{X}}
\newcommand{\bfx}{\mb{x}}
\newcommand{\y}{\mb{y}}
\renewcommand{\b}{\mb{\beta}}
\newcommand{\e}{\bs{\epsilon}}
\newcommand{\bhat}{\widehat{\mb{\beta}}}
\newcommand{\XX}{\Xmat'\Xmat}
\newcommand{\XXinv}{\left(\XX\right)^{-1}}
\newcommand{\hatsig}{\hat{\sigma}^2}
\newcommand{\red}[1]{\textcolor{red!60}{#1}}
\newcommand{\indianred}[1]{\textcolor{indianred}{#1}}
\newcommand{\blue}[1]{\textcolor{blue!60}{#1}}
\newcommand{\dblue}[1]{\textcolor{dodgerblue}{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\inprob}{\overset{p}{\to}}
\newcommand{\indist}{\overset{d}{\to}}
\newcommand{\eframe}{\end{frame}}
\newcommand{\bframe}{\begin{frame}}
\newcommand{\R}{\textsf{\textbf{R}}}
\newcommand{\Rst}{\textsf{\textbf{RStudio}}}
\newcommand{\rfun}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\rpack}[1]{\textbf{#1}}
\newcommand{\rexpr}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\filename}[1]{\texttt{\color{blue}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>In the last chapter, we defined estimators and started to investigate their finite-sample properties like unbiasedness and the sampling variance. We call these “finite-sample” properties because establishing them generally does not depend on the sample size. We saw that under iid data, the sample mean is unbiased for the population mean, but this result holds as much for <span class="math inline">\(n = 10\)</span> as it does for <span class="math inline">\(n = 1,000,000\)</span>. But these properties are also of limited use: we only learn the center and spread of the sampling distribution of <span class="math inline">\(\Xbar_n\)</span> from these results. What about the shape of the distribution? We can often derive the shape if we are willing to make certain assumptions on the underlying data (for example, if the data is normal, then the sample means will be normal as well), but this approach is brittle: if our parametric assumption is false, we’re back to square one.</p>
<p>In this chapter, we’re going to take a different approach and see what happens to the sampling distribution of estimators as the sample size gets large. The study of the estimators as the sample size goes to infinity is called <strong>asymptotic theory</strong>, but it’s important to understand everything we do with asymptotics will be an approximation. No one ever has infinite data, but we hope that as our samples get larger, the approximations will be closer to the truth. Why work in this asymptopia, though? It turns out that many expressions are much easier to derive in the limit than in finite samples.</p>
</section>
<section id="why-convergence-with-probability-is-hard" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="why-convergence-with-probability-is-hard"><span class="header-section-number">3.2</span> Why convergence with probability is hard</h2>
<p>It’s helpful to review the basic idea of convergence in deterministic sequences from calculus:</p>
<div id="def-limit" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 </strong></span>A sequence <span class="math inline">\(\{a_n: n = 1, 2, \ldots\}\)</span> has the <strong>limit</strong> <span class="math inline">\(a\)</span> written <span class="math inline">\(a_n \rightarrow a\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span> of <span class="math inline">\(\lim_{n\rightarrow \infty} a_n = a\)</span> if for all <span class="math inline">\(\epsilon &gt; 0\)</span> there is some <span class="math inline">\(n_{\epsilon} &lt; \infty\)</span> such that for all <span class="math inline">\(n \geq n_{\epsilon}\)</span>, <span class="math inline">\(|a_n - a| \leq \epsilon\)</span>.</p>
</div>
<p>We say that <span class="math inline">\(a_n\)</span> <strong>converges</strong> to <span class="math inline">\(a\)</span> if <span class="math inline">\(\lim_{n\rightarrow\infty} a_n = a\)</span>. Basically, a sequence converges to a number if the sequence gets closer and closer to that number as the sequence goes on.</p>
<p>Can we apply this same idea to sequences of random variables (like estimators)? Let’s look at a few examples that might help clarify the difficult in doing so.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Let’s say that we have a sequence of <span class="math inline">\(a_n = a\)</span> for all <span class="math inline">\(n\)</span> (that is, a constant sequence). Then obviously <span class="math inline">\(\lim_{n\rightarrow\infty} a_n = a\)</span>. Now let’s say we have a sequence of random variables, <span class="math inline">\(X_1, X_2, \ldots\)</span>, that are all independent with a standard normal distribution, <span class="math inline">\(N(0,1)\)</span>. From the analogy to the deterministic case, it is tempting to say that <span class="math inline">\(X_n\)</span> converges to <span class="math inline">\(X \sim N(0, 1)\)</span>, but notice that because they are all different random variables, <span class="math inline">\(\P(X_n = X) = 0\)</span>. Thus, we need to be careful about saying how one variable converges to another variable.</p>
<p>Another example highlights subtle problems with a sequence of random variables converging to a single value. Suppose we have a sequence of random variables <span class="math inline">\(X_1, X_2, \ldots\)</span> where <span class="math inline">\(X_n \sim N(0, 1/n)\)</span>. Clearly, <span class="math inline">\(X_n\)</span> will be concentrated around 0 for large values of <span class="math inline">\(n\)</span>, so it is tempting to say that <span class="math inline">\(X_n\)</span> converges to 0. But notice that <span class="math inline">\(\P(X_n = 0) = 0\)</span> because of the nature of continuous random variables.</p>
</section>
<section id="convergence-in-probability-and-consistency" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="convergence-in-probability-and-consistency"><span class="header-section-number">3.3</span> Convergence in probability and consistency</h2>
<p>There are several different ways that a sequence of random variance can converge. The first type of convergence deals with sequence converging to a single value.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div id="def-inprob" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 </strong></span>A sequence of random variables, <span class="math inline">\(X_1, X_2, \ldots\)</span>, is said to <strong>converge in probability</strong> to a value <span class="math inline">\(b\)</span> if for every <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math display">\[
\P(|X_n - b| &gt; \varepsilon) \rightarrow 0,
\]</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. We write this <span class="math inline">\(X_n \inprob b\)</span>.</p>
</div>
<p>With deterministic sequences, we said that <span class="math inline">\(a_n\)</span> converges to <span class="math inline">\(a\)</span> is it gets closer and closer to <span class="math inline">\(a\)</span> as <span class="math inline">\(n\)</span> gets bigger. For convergence in probability, the sequence of random variables converges to <span class="math inline">\(b\)</span> is the probability that random variables are far away from <span class="math inline">\(b\)</span> get smaller and smaller as <span class="math inline">\(n\)</span> gets big.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>You will sometimes see convergence in probability written as <span class="math inline">\(\text{plim}(Z_n) = b\)</span> if <span class="math inline">\(Z_n \inprob b\)</span>, <span class="math inline">\(\text{plim}\)</span> stands for “probability limit.”</p>
</div>
</div>
<p>Convergence in probability is incredibly useful for evaluating estimators. While we said that unbiasedness was not the be all and end all of properties of estimators, the following property is a fairly basic and fundamental property that we would like all good estimators to have.</p>
<div id="def-consistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 </strong></span>An estimator is <strong>consistent</strong> if <span class="math inline">\(\widehat{\theta}_n \inprob \theta\)</span>.</p>
</div>
<p>Consistency of an estimator implies that the sampling distribution of this estimator “collapses” on the true value as the sample size gets large. We say an estimator is inconsistent if it converges in probability to any other value, which is obviously a very bad property of an estimator. It means that as the sample size gets large, the probability that the estimator will be close to the truth will approach 0.</p>
<p>We can also define convergence in probability for a sequence of random vectors, <span class="math inline">\(\X_1, \X_2, \ldots\)</span>, where <span class="math inline">\(\X_i = (X_{i1}, \ldots, X_{ik})\)</span> is a random vector of length <span class="math inline">\(k\)</span>. This sequence convergences in probbaility to a vector <span class="math inline">\(\mb{b} = (b_1, \ldots, b_k)\)</span> if and only if each random variable in the vector converges to the corresponding element in <span class="math inline">\(\mb{b}\)</span>, or that <span class="math inline">\(X_{nj} \inprob b_j\)</span> for all <span class="math inline">\(j = 1, \ldots, k\)</span>.</p>
</section>
<section id="useful-inequalities" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="useful-inequalities"><span class="header-section-number">3.4</span> Useful inequalities</h2>
<p>At first glance, it appears establishing consistency of an estimator will be difficult. How can we know if a distribution will collapse to a specific value without knowing the shape or family of the distribution? It turns out that there are certain relationships between the mean and variance of a random variable and certain probability statements that hold for all distributions (that have finite variance at least). This will be incredibly helpful to us.</p>
<div id="thm-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Markov Inequality) </strong></span>For any r.v. <span class="math inline">\(X\)</span> and any <span class="math inline">\(\delta &gt;0\)</span>, <span class="math display">\[
\P(|X| \geq \delta) \leq \frac{\E[|X|]}{\delta}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Notice that we can let <span class="math inline">\(Y = |X|/\delta\)</span> and rewrite the statement as <span class="math inline">\(\P(Y \geq 1) \leq \E[Y]\)</span> (since <span class="math inline">\(E[|X|]/\delta = \E[|X|/\delta]\)</span> by the properties of expectation), which is what we will show. But notice that <span class="math display">\[
\mathbb{1}(Y \geq 1) \leq Y.
\]</span> Why does this hold? We can investigate the two possible values of the indicator function to see. If <span class="math inline">\(Y\)</span> is less than 1, then the indicator function will be 0, but <span class="math inline">\(Y\)</span> is non-negative so we know that it must be at least as big as 0 so that inequality holds. If <span class="math inline">\(Y \geq 1\)</span> then the indicator function 1 but we just said that <span class="math inline">\(Y \geq 1\)</span> so the inequality holds. If we take the expectation of both sides of this inequality, we obtain the result (remember the expectation of an indicator function is the probability of the event being indicated).</p>
</div>
<p>In words, Markov’s inequality says that the probability of a random variable being large in magnitude cannot be high if the average is not large in magnitude. Blitzstein and Hwang 2019) provide a nice intuition behind this result. Let <span class="math inline">\(X\)</span> be the income of a randomly selected individual in a population and set <span class="math inline">\(\delta = 2\E[X]\)</span>, so that the inequality becomes <span class="math inline">\(\P(X &gt; 2\E[X]) &lt; 1/2\)</span> (assuming that all income is nonnegative). Here, the inequality says that the share of the population that has an income twice the average must be less than 0.5, since if more than half the population was making twice the average income then the average would have to be higher.</p>
<p>It’s quite astounding how general this result is since it holds for all random variables. Of course, its generality comes at the expense of not being very informative. If <span class="math inline">\(\E[|X|] = 5\)</span>, for instance, the inequality tells us that <span class="math inline">\(\P(|X| \geq 1) \leq 5\)</span> which is not very helpful since we already know that probabilities are less than 1! If we are willing to make some assumptions about <span class="math inline">\(X\)</span>, we can get tighter bounds.</p>
<div id="thm-chebyshev" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Chebyshev Inequality) </strong></span>Suppose that <span class="math inline">\(X\)</span> is r.v. for which <span class="math inline">\(\V[X] &lt; \infty\)</span>. Then, for every real number <span class="math inline">\(\delta &gt; 0\)</span>, <span class="math display">\[
\P(|X-\E[X]| \geq \delta) \leq \frac{\V[X]}{\delta^2}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove this, we only need to square both sides of the inequality inside the probability statement and apply Markov’s inequality: <span class="math display">\[
\P\left( |X - \E[X]| \geq \delta \right) = \P((X-\E[X])^2 \geq \delta^2) \leq \frac{\E[(X - \E[X])^2]}{\delta^2} = \frac{\V[X]}{\delta^2},
\]</span> with the last equality holding by the definition of variance.</p>
</div>
<p>This is a straightforward extension of the Markov result: the probability of a random variable being far away from its mean (that is, <span class="math inline">\(|X-\E[X]|\)</span> being large) is limited by the variance of the random variable. If we let <span class="math inline">\(\delta = c\sigma\)</span>, where <span class="math inline">\(\sigma\)</span> is the standard deviation of <span class="math inline">\(X\)</span>, then we can use this result to bound the normalized: <span class="math display">\[
\P\left(\frac{|X - \E[X]|}{\sigma} &gt; c \right) \leq \frac{1}{c^2}.
\]</span> This says that the probability of being, say, 2 standard deviations away from the mean must be less than 1/4 = 0.25. Notice that this bound can be quite wide. If <span class="math inline">\(X\)</span> is normally distributed, then we know that just about 5% of draws will be greater than 2 SDs away from the mean, which is much lower than the 25% bound implied by Chebyshev’s inequality.</p>
</section>
<section id="the-law-of-large-numbers" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="the-law-of-large-numbers"><span class="header-section-number">3.5</span> The law of large numbers</h2>
<p>We can now use these inequalities to show how certain estimators are consistent for certain quantities of interest. Why are these inequalities useful for this purpose? Remember that convergence in probability was about the probability of an estimator being far away from a value going to zero. Chebyshev’s inequality shows that we can bound these exact probabilities.</p>
<p>The most famous consistency result has a special name.</p>
<div id="thm-lln" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (Weak Law of Large Numbers) </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be a an i.i.d. draws from a distribution with mean <span class="math inline">\(\mu = \E[X_i]\)</span> and variance <span class="math inline">\(\sigma^2 = \V[X_i] &lt; \infty\)</span>. Let <span class="math inline">\(\Xbar_n = \frac{1}{n} \sum_{i =1}^n X_i\)</span>. Then, <span class="math inline">\(\Xbar_n \inprob \mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that the sample mean is unbiased, so <span class="math inline">\(\E[\Xbar_n] = \mu\)</span> with sampling variance <span class="math inline">\(\sigma^2/n\)</span>. We can then simply apply Chebyshev to the sample mean to get <span class="math display">\[
\P(|\Xbar_n - \mu| \geq \delta) \leq \frac{\sigma^2}{n\delta^2}
\]</span> An <span class="math inline">\(n\rightarrow\infty\)</span>, the right-hand side goes to 0 which means that the left-hand side also must go to 0 which is the definition of <span class="math inline">\(\Xbar_n\)</span> converging in probability to <span class="math inline">\(\mu\)</span>.</p>
</div>
<p>The weak law of large numbers (WLLN) shows that, under general conditions, the sample mean gets closer to the population mean as <span class="math inline">\(n\rightarrow\infty\)</span>. In fact, this result holds even when the variance of the is infinite, though that’s a situation that most analysts will rarely face.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The naming of the “weak” law of large numbers seems to imply the existence of a “strong” law of large numbers (SLLN) and this is true. The SLLN states that the sample mean converges to the population mean with probability 1. This type of convergence, called <strong>almost sure convergence</strong>, is stronger than convergence in probability which only says that the probability of the sample mean being close to the population mean converges to 1. While it is nice to know that this stronger form of convergence holds for the sample mean under the same assumptions, it is very rare for folks outside of theoretical probability and statistics to need to rely on almost sure convergence.</p>
</div>
</div>
<div id="exm-lln" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 </strong></span>It can be helpful to see how the distribution of the sample mean changes as a function of the sample size to appreciate the WLLN. We can show this by taking repeated iid samples of different sizes from an exponential rv with rate 0.5 so that <span class="math inline">\(\E[X_i] = 2\)</span>. In <a href="#fig-lln-sim">Figure&nbsp;<span>3.1</span></a>, we show the distribution of the sample mean (across repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue), and 1000 (green). What we can see is how the distribution of the sample mean is “collapsing” on the true population mean, 2. The probability of being far away from 2 becomes progressively smaller.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-lln-sim" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="03_asymptotics_files/figure-html/fig-lln-sim-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.1: Sampling distribution of the sample mean as a function of sample size</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>The WLLN also holds for random vectors in addition to random variables. Let <span class="math inline">\((\X_1, \ldots, \X_n)\)</span> be an iid sample of random vectors of length <span class="math inline">\(k\)</span>, <span class="math inline">\(\mb{X}_i = (X_{i1}, \ldots, X_{ik})\)</span>. We can define the vector sample mean as just the vector of sample means for each of the entries:</p>
<p><span class="math display">\[
\overline{\mb{X}}_n = \frac{1}{n} \sum_{i=1}^n \mb{X}_i =
\begin{pmatrix}
\Xbar_{n,1} \\ \Xbar_{n,2} \\ \vdots \\ \Xbar_{n, k}
\end{pmatrix}
\]</span> Since this is just a vector of sample means, each random variable in the random vector will converge in probability to the mean of that random variable. Fortunately, this is the exact definition of convergence in probability for random vectors. We formally write this in the following theorem.</p>
<div id="thm-vector-wlln" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4 </strong></span>If <span class="math inline">\(\X_i \in \mathbb{R}^k\)</span> are iid draws from a distribution with <span class="math inline">\(\E[X_{ij}] &lt; \infty\)</span> for all <span class="math inline">\(j=1,\ldots,k\)</span> then as <span class="math inline">\(n\rightarrow\infty\)</span></p>
<p><span class="math display">\[
\overline{\mb{X}}_n \inprob \E[\X]  =
\begin{pmatrix}
\E[X_{i1}] \\ \E[X_{i2}] \\ \vdots \\ \E[X_{ik}]
\end{pmatrix}.
\]</span></p>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>You will have noticed that many of the formal results we have presented so far have “moment conditions” that certain moments are finite. For the vector WLLN, we saw that applied to the mean of each variable in the vector. Some books use a short hand for this: <span class="math inline">\(\E\Vert \X_i\Vert &lt; \infty\)</span>, where <span class="math display">\[
\Vert\X_i\Vert = \left(X_{i1}^2 + X_{i2}^2 + \ldots + X_{ik}^2\right)^{1/2}.
\]</span> This is slightly more compact notation, but why does it work? One can show that this function, called the <strong>Euclidean norm</strong> or <span class="math inline">\(L_2\)</span>-norm is a <strong>convex</strong> function, so we can apply Jensen’s inequality to show that: <span class="math display">\[
\E\Vert \X_i\Vert \geq \Vert \E[\X_i] \Vert = (\E[X_{i1}]^2 + \ldots + \E[X_{ik}]^2)^{1/2}.
\]</span> So if <span class="math inline">\(\E\Vert \X_i\Vert\)</span> is finite, it means that all the component means are finite otherwise the right-hand side of the previous equation would be infinite.</p>
</div>
</div>
</section>
<section id="consistency-of-estimators" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="consistency-of-estimators"><span class="header-section-number">3.6</span> Consistency of estimators</h2>
<p>The WLLN shows that the sample mean of iid draws is consistent for the population mean, which is a massive result given that so many estimators can be written as sample means. What about other estimators? The proof of the WLLN points to one way to determine if an estimator is consistent: if it is unbiased and the sampling variance shrinks as the sample size grows. The next theorem</p>
<div id="thm-consis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.5 </strong></span>For any estimator <span class="math inline">\(\widehat{\theta}_n\)</span>, if <span class="math inline">\(\text{bias}[\widehat{\theta}_n] \to 0\)</span> and <span class="math inline">\(\V[\widehat{\theta}_n] \rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, then <span class="math inline">\(\widehat{\theta}_n\)</span> is consistent.</p>
</div>
<p>Thus, if we can characterize the bias and sampling variance of an estimator, then we should be able to tell if it consistent or not. This is handy since working with the kinds of probability inequalities used for the WLLN can sometimes be quite confusing.</p>
<p>What do we do if it is difficult or impossible to characterize the bias? Consider a plug-in estimator like <span class="math inline">\(\widehat{\alpha} = \log(\Xbar_n)\)</span> where <span class="math inline">\(X_1, \ldots, X_n\)</span> are iid from a population with mean <span class="math inline">\(\mu\)</span>. We know that for nonlinear functions like logarithms we have <span class="math inline">\(\log\left(\E[Z]\right) \neq \E[\log(Z)]\)</span>, so <span class="math inline">\(\E[\widehat{\alpha}] \neq \log(\E[\Xbar_n])\)</span> and the plug-in estimator will be biased for <span class="math inline">\(\log(\mu)\)</span>. It will also be difficult to obtain an expression for the bias in terms of <span class="math inline">\(n\)</span>. Is all hope lost here? Must we give up on consistency? No, and in fact, consistency will be much simpler to show in this setting.</p>
<div id="thm-inprob-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.6 (Properties of convergence in probability) </strong></span>Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Z_n\)</span> be two sequences of random variables such that <span class="math inline">\(X_n \inprob a\)</span> and <span class="math inline">\(Z_n \inprob b\)</span>, and let <span class="math inline">\(g(\cdot)\)</span> be a continuous function. Then,</p>
<ol type="1">
<li><span class="math inline">\(g(X_n) \inprob g(a)\)</span> (continuous mapping theorem)</li>
<li><span class="math inline">\(X_n + Z_n \inprob a + b\)</span></li>
<li><span class="math inline">\(X_nZ_n \inprob ab\)</span></li>
<li><span class="math inline">\(X_n/Z_n \inprob a/b\)</span> if <span class="math inline">\(b &gt; 0\)</span>.</li>
</ol>
</div>
<p>We can now see that many of the nasty problems with expectations and nonlinear functions are made considerably easier with convergence in probability in the asymptotic setting. So while we know that <span class="math inline">\(\log(\Xbar_n)\)</span> is biased for <span class="math inline">\(\log(\mu)\)</span>, we know that it is consistent since <span class="math inline">\(\log(\Xbar_n) \inprob \log(\mu)\)</span> because <span class="math inline">\(\log\)</span> is a continuous function.</p>
<div id="exm-nonresponse" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 </strong></span>Suppose we implemented a survey by randomly selecting a sample from the population of size <span class="math inline">\(n\)</span>, but not everyone responded to our survey. Let the data consist of pairs of random variables, <span class="math inline">\((Y_1, R_1), \ldots, (Y_n, R_n)\)</span>, where <span class="math inline">\(Y_i\)</span> is the question of interest and <span class="math inline">\(R_i\)</span> is a binary indicator for if the respondent answered the question (<span class="math inline">\(R_i = 1\)</span>) or not (<span class="math inline">\(R_i = 0\)</span>). Our goal is to estimate the mean of the question for responders: <span class="math inline">\(\E[Y_i \mid R_i = 1]\)</span>. We can use the law of iterated expectation to which we can rewrite as <span class="math display">\[
\begin{aligned}
\E[Y_iR_i] &amp;= \E[Y_i \mid R_i = 1]\P(R_i = 1) + \E[ 0 \mid R_i = 0]\P(R_i = 0) \\
\implies \E[Y_i \mid R_i = 1] &amp;= \frac{\E[Y_iR_i]}{\P(R_i = 1)}
\end{aligned}
\]</span></p>
<p>The relevant estimator for this quantity is the mean of the of the outcome among those who responded, which is slightly more complicated than a typical sample mean because the denominator is a random variable: <span class="math display">\[
\widehat{\theta}_n = \frac{\sum_{i=1}^n Y_iR_i}{\sum_{i=1}^n R_i}.
\]</span> Notice that this estimator is the ratio of two random variables. The numerator has mean <span class="math inline">\(n\E[Y_iR_i]\)</span> and the denominator has mean <span class="math inline">\(n\P(R_i = 1)\)</span>. It is then tempting to say that we can take the ratio of these means as the mean of <span class="math inline">\(\widehat{\theta}_n\)</span>, but expectations are not preserved in nonlinear functions like this one.</p>
<p>We can establish consistency of our estimator, though, by noting that we can rewrite the estimator as a ratio of sample means <span class="math display">\[
\widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n Y_iR_i}{(1/n)\sum_{i=1}^n R_i},
\]</span> where by the WLLN the numerator <span class="math inline">\((1/n)\sum_{i=1}^n Y_iR_i \inprob \E[Y_iR_i]\)</span> and the denominator <span class="math inline">\((1/n)\sum_{i=1}^n R_i \inprob \P(R_i = 1)\)</span>. Thus, by <a href="#thm-inprob-properties">Theorem&nbsp;<span>3.6</span></a>, we have <span class="math display">\[
\widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n Y_iR_i}{(1/n)\sum_{i=1}^n R_i} \inprob \frac{\E[Y_iR_i]}{\P[R_i = 1]} = \E[Y_i \mid R_i = 1]
\]</span> so long as the probability of responding is greater than zero. This establishes that our sample mean among responders while biased for the conditional expectation among responders, it is consistent for that quantity.</p>
</div>
<p>It is very important to keep the difference between unbiased and consistent clear in your mind. There are very many silly unbiased estimators that are inconsistent. Let’s go back to our iid sample, <span class="math inline">\(X_1, \ldots, X_n\)</span> from a population with <span class="math inline">\(E[X_i] = \mu\)</span>. There is nothing in the rule book against defining an estimator <span class="math inline">\(\widehat{\theta}_{first} = X_1\)</span> that just uses the first observation as the estimate. This seems like an obviously silly estimator, but it is actually unbiased since <span class="math inline">\(\E[\widehat{\theta}_{first}] = \E[X_1] = \mu\)</span>. It is inconsistent since the sampling variance of this estimator is just the variance of the population distribution, <span class="math inline">\(\V[\widehat{\theta}_{first}] = \V[X_i] = \sigma^2\)</span>, which does not change as a function of the sample size. Generally speaking, we can regard “unbiased, but inconsistent” estimators as silly and not worth our time (along with bias and inconsistent estimators).</p>
<p>There are also estimators that are biased but consistent that are often much more interesting. We already saw one such estimator in <a href="#exm-nonresponse">Example&nbsp;<span>3.2</span></a>, but there are many more. Maximum likelihood estimators, for example, are (under some regularity conditions) consistent for the parameters of a parametric model, but they are often biased.</p>
<div id="exm-plug-in-variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Plug-in variance estimator) </strong></span>Last chapter, we introduced the plug-in estimator for the population variance, <span class="math display">\[
\widehat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \Xbar_n)^2,
\]</span> which we will now show is biased but consistent. To see the bias note that we can rewrite the sum of square deviations <span class="math display">\[\sum_{i=1}^n (X_i - \Xbar_n)^2 = \sum_{i=1}^n X_i^2 - n\Xbar_n. \]</span> Then, the expectation of the plug-in estimator is <span class="math display">\[
\begin{aligned}
\E[\widehat{\sigma}^2] &amp; = \E\left[\frac{1}{n}\sum_{i=1}^n X_i^2\right] - \E[\Xbar_n^2] \\
&amp;= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \E[X_iX_j] \\
&amp;= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j\neq i} \underbrace{\E[X_i]\E[X_j]}_{\text{independence}} \\
&amp;= \E[X_i^2] - \frac{1}{n}\E[X_i^2] - \frac{1}{n^2} n(n-1)\mu^2 \\
&amp;= \frac{n-1}{n} \left(\E[X_i^2] - \mu^2\right) \\
&amp;= \frac{n-1}{n} \sigma^2 = \sigma^2 - \frac{1}{n}\sigma^2
\end{aligned}.
\]</span> Thus, we can see that the bias of the plug-in estimator is <span class="math inline">\(-(1/n)\sigma^2\)</span> so it slightly underestimates the variance. Nicely, though, the bias shrinks as a function of the sample size, so according to <a href="#thm-consis">Theorem&nbsp;<span>3.5</span></a> it will be consistent so long as the sampling variance of <span class="math inline">\(\widehat{\sigma}^2\)</span> shrinks as a function of the sample size, which it does (though omit that proof here). Of course, simply multiplying this estimator by <span class="math inline">\(n/(n-1)\)</span> will give an unbiased and consistent estimator that is also the typical sample variance estimator.</p>
</div>
</section>
<section id="convergence-in-distribution-and-the-central-limit-theorem" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="convergence-in-distribution-and-the-central-limit-theorem"><span class="header-section-number">3.7</span> Convergence in distribution and the central limit theorem</h2>
<p>Convergence in probability and the law of large numbers are very useful for understanding how our estimators will (or will not) collapse to their estimand as the sample size increases. But what about the shape of the sampling distribution of our estimators? For the purposes of statistical inference, we would like to be able to make probability statements such as <span class="math inline">\(\P(a \leq \widehat{\theta}_n \leq b)\)</span>. These types of statements will be the basis of hypothesis testing and confidence intervals. But in order make those types of statements, we need to know the entire distribution of <span class="math inline">\(\widehat{\theta}_n\)</span>, not just the mean and variance. Luckily, there are established results that will allow us to approximate the sampling distribution of a huge swath of estimators when our sample sizes are large.</p>
<p>To see how we will develop these approximations, we need to first describe a weaker form of convergence to a distribution rather than to a single value.</p>
<div id="def-indist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4 </strong></span>Let <span class="math inline">\(X_1,X_2,\ldots\)</span>, be a sequence of r.v.s, and for <span class="math inline">\(n = 1,2, \ldots\)</span> let <span class="math inline">\(F_n(x)\)</span> be the c.d.f. of <span class="math inline">\(X_n\)</span>. Then it is said that <span class="math inline">\(X_1,X_2, \ldots\)</span> <strong>converges in distribution</strong> to r.v. <span class="math inline">\(X\)</span> with c.d.f. <span class="math inline">\(F(x)\)</span> if <span class="math display">\[
\lim_{n\rightarrow \infty} F_n(x) = F(x),
\]</span> for all values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(F(x)\)</span> is continuous. We write this as <span class="math inline">\(X_n \indist X\)</span> or sometimes <span class="math inline">\(X_n ⇝ X\)</span>.</p>
</div>
<p>Essentially, convergence in distribution means that as <span class="math inline">\(n\)</span> gets large, the distribution of <span class="math inline">\(X_n\)</span> becomes more and more similar to the distribution of <span class="math inline">\(X\)</span>, which we often call the <strong>asymptotic distribution</strong> of <span class="math inline">\(X_n\)</span> (other names include the <strong>large-sample distribution</strong>). If we know that <span class="math inline">\(X_n \indist X\)</span>, then we can use the distribution of <span class="math inline">\(X\)</span> as an approximation to the distribution of <span class="math inline">\(X_n\)</span> and that distribution can be fairly accurate.</p>
<p>One of the most remarkable results in probability and statistics is that a large class of estimators will converge in distribution to one particular family of distributions: the normal. This is one reason that we study the normal so much and why investing in building intuition about it will pay off across many domains of applied work. We call this broad class of results the “central limit theorem,” (CLT) but it would probably be more accurate to refer to them as “central limit theorems” since much of statistics is devoted to showing the result in different settings. We now present the simplest CLT for the sample mean.</p>
<div id="thm-clt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.7 (Central Limit Theorem) </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be i.i.d. r.v.s from a distribution with mean <span class="math inline">\(\mu = \E[X_i]\)</span> and variance <span class="math inline">\(\sigma^2 = \V[X_i]\)</span>. Then if <span class="math inline">\(\E[X_i^2] &lt; \infty\)</span>, we have <span class="math display">\[
\frac{\Xbar_n - \mu}{\sqrt{\V[\Xbar_n]}} = \frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{\sigma} \indist \N(0, 1).
\]</span></p>
</div>
<p>In words: the sample mean of a random sample from a population with finite mean and variance will be approximately normally distributed in large samples. Notice how we have not made any assumptions about the distribution of the underlying random variables, <span class="math inline">\(X_i\)</span>. They could binary, event count, continuous, anything. This means the CLT is incredibly broadly applicable.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why do we state the CLT in terms of the sample mean after centering and scaling by its standard error? If we don’t normalize the sample mean in this way, it’s difficult to talk about convergence in distribution because we know from the WLLN that <span class="math inline">\(\Xbar_n \inprob \mu\)</span> so in the limit the distribution of <span class="math inline">\(\Xbar_n\)</span> is concentrated at point mass around that value. Normalizing by centering and rescaling ensures that the variance of the resulting quantity will be fixed as a function of <span class="math inline">\(n\)</span>, so it makes sense to talk about its distribution converging. Sometimes you will see the equivalent result as <span class="math display">\[
\sqrt{n}\left(\Xbar_n - \mu\right) \indist \N(0, \sigma^2).
\]</span></p>
</div>
</div>
<p>We can use this result to state approximations that we can use when discussing estimators such as <span class="math display">\[
\Xbar_n \overset{a}{\sim} N(\mu, \sigma^2/n),
\]</span> where we use <span class="math inline">\(\overset{a}{\sim}\)</span> to be “approximately distributed as in large samples.” This allow us to say things like: “in large samples, we should expect the sample mean to between within <span class="math inline">\(2\sigma/\sqrt{n}\)</span> of the true mean in 95% of repeated samples.” As you might guess, this will be very important for hypothesis tests and confidence intervals! Estimators so often follow the CLT that we have an expression for this property.</p>
<div id="def-asymptotically-normal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.5 </strong></span>An estimator <span class="math inline">\(\widehat{\theta}_n\)</span> is <strong>asymptotically normal</strong> if for some <span class="math inline">\(\theta\)</span> <span class="math display">\[
\sqrt{n}\left( \widehat{\theta}_n - \theta \right) \indist N\left(0,\V[\widehat{\theta}_n]\right).
\]</span></p>
</div>
<div id="exm-bin-clt" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 </strong></span>To illustrate how the CLT works, we can simulate the sampling distribution of the (normalized) sample mean at different sample sizes. Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be iid samples from a Bernoulli with probability of success 0.25. We then draw repeated samples of size <span class="math inline">\(n=30\)</span> and <span class="math inline">\(n=100\)</span> and calculate <span class="math inline">\(\sqrt{n}(\Xbar_n - 0.25)/\sigma\)</span> for each random sample. <a href="#fig-clt">Figure&nbsp;<span>3.2</span></a> plots the density of these two sampling distributions along with a standard normal reference. We can see that even at <span class="math inline">\(n=30\)</span>, the rough shape of the density looks normal, with spikes and valleys due to the discrete nature of the data (the sample mean can only take on 31 possible values in this case). By <span class="math inline">\(n=100\)</span>, the sampling distribution is very close to the true standard normal.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-clt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="03_asymptotics_files/figure-html/fig-clt-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.2: Sampling distributions of the normalized sample mean at n=30 and n=100.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>There are several properties of convergence in distribution that are helpful to us.</p>
<div id="thm-indist-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.8 (Properties of convergence in distribution) </strong></span>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables <span class="math inline">\(X_1,X_2,\ldots\)</span> that converges in distribution to some rv <span class="math inline">\(X\)</span> and let <span class="math inline">\(Y_n\)</span> be a sequence of random variables <span class="math inline">\(Y_1,Y_2,\ldots\)</span> that converges in probability to some number, <span class="math inline">\(c\)</span>. Then,</p>
<ol type="1">
<li><span class="math inline">\(g(X_n) \indist g(X)\)</span> for all continuous functions <span class="math inline">\(g\)</span>.</li>
<li><span class="math inline">\(X_nY_n\)</span> converges in distribution to <span class="math inline">\(cX\)</span></li>
<li><span class="math inline">\(X_n + Y_n\)</span> converges in distribution to <span class="math inline">\(X + c\)</span></li>
<li><span class="math inline">\(X_n / Y_n\)</span> converges in distribution to <span class="math inline">\(X / c\)</span> if <span class="math inline">\(c \neq 0\)</span></li>
</ol>
</div>
<p>The last 3 of these results are sometimes referred to as <strong>Slutsky’s theorem</strong>. These results are very commonly used when trying to determine the asymptotic distribution of an estimator.</p>
<p>One important application of Slutsky’s theorem is when we replace the (unknown) popoulation variance in the CLT with an estimate. Recall the definition of the <strong>sample variance</strong> as <span class="math display">\[
s^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \Xbar_n)^2,
\]</span> with the <strong>sample standard deviation</strong> defined as <span class="math inline">\(s = \sqrt{s^2}\)</span>. It’s easy to show that these are consistent estimators for their respective population parameters <span class="math display">\[
s^2 \inprob \sigma^2 = \V[X_i], \qquad s \inprob \sigma,
\]</span> which by Slutsky’s theorem implies that <span class="math display">\[
\frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{s} \indist \N(0, 1)
\]</span> Comparing this result to the statement of CLT, we see that replacing the population variance with a consistent estimate of the variance (or standard deviation) does not affect the asymptotic distribution.</p>
<p>Like with the WLLN, the CLT holds for random vectors of sample means, where their centered and scaled versions converge to a multivariate normal distribution with a covariance matrix equal to covariance matrix of the underlying random vectors of data, <span class="math inline">\(\X_i\)</span>.</p>
<div id="thm-multivariate-clt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.9 </strong></span>If <span class="math inline">\(\mb{X}_i \in \mathbb{R}^k\)</span> are i.i.d. and <span class="math inline">\(\E\Vert \mb{X}_i \Vert^2 &lt; \infty\)</span>, then as <span class="math inline">\(n \to \infty\)</span>, <span class="math display">\[
\sqrt{n}\left( \overline{\mb{X}}_n - \mb{\mu}\right) \indist \N(0, \mb{\Sigma}),
\]</span> where <span class="math inline">\(\mb{\mu} = \E[\mb{X}_i]\)</span> and <span class="math inline">\(\mb{\Sigma} = \V[\mb{X}_i] = \E\left[(\mb{X}_i-\mb{\mu})(\mb{X}_i - \mb{\mu})'\right]\)</span>.</p>
</div>
<p>Here, notice that <span class="math inline">\(\mb{\mu}\)</span> is the vector of population means for all the random variables in <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(\mb{\Sigma}\)</span> is the variance-covariance matrix for that vector.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As with the notation alert with the WLLN, we are using a shorthand here, <span class="math inline">\(\E\Vert \mb{X}_i \Vert^2 &lt; \infty\)</span>, which implies that <span class="math inline">\(\E[X_{ij}^2] &lt; \infty\)</span> for all <span class="math inline">\(j = 1,\ldots, k\)</span>, or equivalently, that the variances of each variable in the sample means has finite variance.</p>
</div>
</div>
</section>
<section id="delta-method" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="delta-method"><span class="header-section-number">3.8</span> Delta method</h2>
<p>Suppose that we know that an estimator follows the CLT and so we have <span class="math display">\[
\sqrt{n}\left(\widehat{\theta}_n - \theta  \right) \indist \N(0, V),
\]</span> but we actually want to estimate <span class="math inline">\(h(\theta)\)</span> so we use the plug-in estimator, <span class="math inline">\(h(\widehat{\theta}_n)\)</span>. It seems like we should be able to apply part 1 of <a href="#thm-indist-properties">Theorem&nbsp;<span>3.8</span></a>, the CLT established the large-sample distribution of the centered and scaled random sequence, <span class="math inline">\(\sqrt{n}(\widehat{\theta}_n - \theta)\)</span>, not to the original estimator itself like we would need to investigate the asymptotic distribution of <span class="math inline">\(h(\widehat{\theta}_n)\)</span>. We can use a little bit of calculus to get an approximation to the distribution we need.</p>
<div id="thm-delta-method" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.10 </strong></span>If <span class="math inline">\(\sqrt{n}\left(\widehat{\theta}_n - \theta\right) \indist \N(0, V)\)</span> and <span class="math inline">\(h(u)\)</span> is continuously differentiable in a neighborhood around <span class="math inline">\(\theta\)</span>, then as <span class="math inline">\(n\to\infty\)</span>, <span class="math display">\[
\sqrt{n}\left(h(\widehat{\theta}_n) - h(\theta)  \right) \indist \N(0, (h'(\theta))^2 V).
\]</span></p>
</div>
<p>It’s useful to understand what’s happening here since it might help give intuition as to when this might go wrong. Why do we focus on continuously differentiable functions, <span class="math inline">\(h()\)</span>? These are functions that can be well-approximated with a line in a neighborhood around a given point like <span class="math inline">\(\theta\)</span>. In <a href="#fig-delta">Figure&nbsp;<span>3.3</span></a>, we show this where the tangent line at <span class="math inline">\(\theta_0\)</span>, which has slope <span class="math inline">\(h'(\theta_0)\)</span>, is very similar to <span class="math inline">\(h(\theta)\)</span> for values close to <span class="math inline">\(\theta_0\)</span>. Because of this, we can approximate the difference between <span class="math inline">\(h(\widehat{\theta}_n)\)</span> and <span class="math inline">\(h(\theta_0)\)</span> with the what this tangent line would give us: <span class="math display">\[
\underbrace{\left(h(\widehat{\theta_n}) - h(\theta_0)\right)}_{\text{change in } y} \approx \underbrace{h'(\theta_0)}_{\text{slope}} \underbrace{\left(\widehat{\theta}_n - \theta_0\right)}_{\text{change in } x},
\]</span> and then multiplying both sides by the <span class="math inline">\(\sqrt{n}\)</span> gives <span class="math display">\[
\sqrt{n}\left(h(\widehat{\theta_n}) - h(\theta_0)\right) \approx h'(\theta_0)\sqrt{n}\left(\widehat{\theta}_n - \theta_0\right).
\]</span> The right-hand side of this approximation converges to <span class="math inline">\(h'(\theta_0)Z\)</span>, where <span class="math inline">\(Z\)</span> is a random variable variable with <span class="math inline">\(\N(0, V)\)</span>. The variance of this quantity will be <span class="math display">\[
\V[h'(\theta_0)Z] = (h'(\theta_0))^2\V[Z] = (h'(\theta_0))^2V,
\]</span> by the properties of variances.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-delta" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="03_asymptotics_files/figure-html/fig-delta-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.3: Linear approximation to nonlinear functions</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div id="exm-log" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 </strong></span>Let’s return to the iid sample <span class="math inline">\(X_1, \ldots, X_n\)</span> with mean <span class="math inline">\(\mu = \E[X_i]\)</span> and variance <span class="math inline">\(\sigma^2 = \V[X_i]\)</span>. From the CLT, we know that <span class="math inline">\(\sqrt{n}(\Xbar_n - \mu) \indist \N(0, \sigma^2)\)</span>. Suppose that we want to estimate <span class="math inline">\(\log(\mu)\)</span> so we use the plug-in estimator <span class="math inline">\(\log(\Xbar_n)\)</span> (assuming that <span class="math inline">\(X_i &gt; 0\)</span> for all <span class="math inline">\(i\)</span> so that we can actually take the log). What is the asymptotic distribution of this estimator? This is a situation where <span class="math inline">\(\widehat{\theta}_n = \Xbar_n\)</span> and <span class="math inline">\(h(\mu) = \log(\mu)\)</span>. From basic calculus we know that <span class="math display">\[
h'(\mu) = \frac{\partial \log(\mu)}{\partial \mu} = \frac{1}{\mu},
\]</span> so applying the delta method, we can determine that <span class="math display">\[
\sqrt{n}\left(\log(\Xbar_n) - \log(\mu)\right) \indist \N\left(0,\frac{\sigma^2}{\mu^2} \right).
\]</span></p>
</div>
<div id="exm-exp" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6 </strong></span>What about if we want to estimate the <span class="math inline">\(\exp(\mu)\)</span> with <span class="math inline">\(\exp(\Xbar_n)\)</span>? Recall that <span class="math display">\[
h'(\mu) = \frac{\partial \exp(\mu)}{\partial \mu} = \exp(\mu)
\]</span> so applying the delta method, we have <span class="math display">\[
\sqrt{n}\left(\exp(\Xbar_n) - \exp(\mu)\right) \indist \N(0, \exp(2mu)\sigma^2),
\]</span> since <span class="math inline">\(\exp(\mu)^2 = \exp(2\mu)\)</span>.</p>
</div>
<p>Like all of the results in this chapter, there is a multivariate version of the delta method that is incredibly useful in practical applications. This is because we often will take two different estimators (or two different estimated parameters) and combine them to estimate another quantity. We now let <span class="math inline">\(\mb{h}(\mb{\theta}) = (h_1(\mb{\theta}), \ldots, h_m(\mb{\theta}))\)</span> map from <span class="math inline">\(\mathbb{R}^k \to \mathbb{R}^m\)</span> and be continuously differentiable (we make the function bold since it ). It will help us use more compact matrix notation if we introduce a <span class="math inline">\(m \times k\)</span> Jacobian matrix of all partial derivatives <span class="math display">\[
\mb{H}(\mb{\theta}) = \mb{\nabla}_{\mb{\theta}}\mb{h}(\mb{\theta}) = \begin{pmatrix}
  \frac{\partial h_1(\mb{\theta})}{\partial \theta_1} &amp; \frac{\partial h_1(\mb{\theta})}{\partial \theta_2} &amp; \cdots &amp; \frac{\partial h_1(\mb{\theta})}{\partial \theta_k} \\
  \frac{\partial h_2(\mb{\theta})}{\partial \theta_1} &amp; \frac{\partial h_2(\mb{\theta})}{\partial \theta_2} &amp; \cdots &amp; \frac{\partial h_2(\mb{\theta})}{\partial \theta_k} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \frac{\partial h_m(\mb{\theta})}{\partial \theta_1} &amp; \frac{\partial h_m(\mb{\theta})}{\partial \theta_2} &amp; \cdots &amp; \frac{\partial h_m(\mb{\theta})}{\partial \theta_k}
\end{pmatrix},
\]</span> which we can use to generate the equivalent multivariate linear approximation <span class="math display">\[
\left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0)\right) \approx \mb{H}(\mb{\theta}_0)'\left(\widehat{\mb{\theta}}_n - \mb{\theta}_0\right).
\]</span> We can use this fact to derive the multivariate delta method.</p>
<div id="thm-multivariate-delta" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.11 </strong></span>Suppose that <span class="math inline">\(\sqrt{n}\left(\widehat{\mb{\theta}}_n - \mb{\theta}_0 \right) \indist \N(0, \mb{\Sigma})\)</span>, then for any function <span class="math inline">\(\mb{h}\)</span> that is continuously differentiable in a neighborhood of <span class="math inline">\(\mb{\theta}_0\)</span>, we have <span class="math display">\[
\sqrt{n}\left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0) \right) \indist \N(0, \mb{H}\mb{\Sigma}\mb{H}'),
\]</span> where <span class="math inline">\(\mb{H} = \mb{H}(\mb{\theta}_0)\)</span>.</p>
</div>
<p>This result follows from the approximation above plus rules about variances of random vectors. Remember that for any compatible matrix of constants, <span class="math inline">\(\mb{A}\)</span>, we have <span class="math inline">\(\V[\mb{A}'\mb{Z}] = \mb{A}\V[\mb{Z}]\mb{A}'\)</span>. You can see that the matrix of constants appears twice here, sort of like the matrix version of “squaring the constant” rule for variance.</p>
<p>The delta method is a very useful for generating closed-form approximations for asymptotic standard errors, but the math is often quite complex for even simple estimators. For applied researchers, it is usually more straightforward to use computational tools like the bootstrap to approximate the standard errors we need. This has the trade-off of taking more computational time to implement than the delta method, but is more easily adaptable across different estimators and domains with little human thinking time.</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Due to Wasserman (2004), Chapter 5.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Technically, a sequence can also convergence in probability to another random variable, but the use case of converging to a single number is much more commonly used in evaluating estimators.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_estimation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04_hypothesis_tests.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis Tests</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>