<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Linear regression – A User's Guide to Statistical Inference and Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./least_squares.html" rel="next">
<link href="./hypothesis_tests.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    macros: {
      RR: "{\\bf R}",
      bs: ["\\boldsymbol{#1}", 1],
      mb: ["\\mathbf{#1}", 1],
      E: "\\mathbb{E}",
      V: "\\mathbb{V}",
      P: "\\mathbb{P}",
      var: "\\text{var}",
      cov: "\\text{cov}",
      N: "\\mathcal{N}",
      Bern: "\\text{Bern}",
      Bin: "\\text{Bin}",
      Pois: "\\text{Pois}",
      Unif: "\\text{Unif}",
      se: "\\textsf{se}",
      U: "\\mb{U}",
      Xbar: "\\overline{X}",
      Ybar: "\\overline{Y}",
      real: "\\mathbb{R}",
      bbL: "\\mathbb{L}",
      u: "\\mb{u}",
      v: "\\mb{v}",
      M: "\\mb{M}",
      X: "\\mb{X}",
      Xmat: "\\mathbb{X}",
      bfx: "\\mb{x}",
      y: "\\mb{y}",
      bfbeta: "\\bs{\\beta}",
      e: "\\bs{\\epsilon}",
      bhat: "\\widehat{\\bs{\\beta}}",
      XX: "\\Xmat'\\Xmat",
      XXinv: "\\left(\\Xmat'\\Xmat\\right)^{-1}",
      hatsig: "\\widehat{\\sigma}^2",
      red: ["\\textcolor{red!60}{#1}", 1],
      indianred: ["\\textcolor{indianred}{#1}", 1],
      blue: ["\\textcolor{blue!60}{#1}", 1],
      dblue: ["\\textcolor{dodgerblue}{#1}", 1],
      indep: "\\perp\\!\\!\\!\\perp",
      inprob: "\\overset{p}{\\to}",
      indist: "\\overset{d}{\\to}",
      argmax: ["\\operatorname\{arg\,max\}"],
      argmin: ["\\operatorname\{arg\,min\}"]      
    }
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linear_model.html">Regression</a></li><li class="breadcrumb-item"><a href="./linear_model.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mattblackwell/gov2002-book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./users-guide.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Design-based Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis_tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_model.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols_properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-do-we-need-models" id="toc-why-do-we-need-models" class="nav-link active" data-scroll-target="#why-do-we-need-models"><span class="header-section-number">5.1</span> Why do we need models?</a></li>
  <li><a href="#sec-linear-projection" id="toc-sec-linear-projection" class="nav-link" data-scroll-target="#sec-linear-projection"><span class="header-section-number">5.2</span> Population linear regression</a>
  <ul class="collapse">
  <li><a href="#bivariate-linear-regression" id="toc-bivariate-linear-regression" class="nav-link" data-scroll-target="#bivariate-linear-regression"><span class="header-section-number">5.2.1</span> Bivariate linear regression</a></li>
  <li><a href="#beyond-linear-approximations" id="toc-beyond-linear-approximations" class="nav-link" data-scroll-target="#beyond-linear-approximations"><span class="header-section-number">5.2.2</span> Beyond linear approximations</a></li>
  <li><a href="#linear-prediction-with-multiple-covariates" id="toc-linear-prediction-with-multiple-covariates" class="nav-link" data-scroll-target="#linear-prediction-with-multiple-covariates"><span class="header-section-number">5.2.3</span> Linear prediction with multiple covariates</a></li>
  <li><a href="#projection-error" id="toc-projection-error" class="nav-link" data-scroll-target="#projection-error"><span class="header-section-number">5.2.4</span> Projection error</a></li>
  </ul></li>
  <li><a href="#linear-cefs-without-assumptions" id="toc-linear-cefs-without-assumptions" class="nav-link" data-scroll-target="#linear-cefs-without-assumptions"><span class="header-section-number">5.3</span> Linear CEFs without assumptions</a></li>
  <li><a href="#interpretation-of-the-regression-coefficients" id="toc-interpretation-of-the-regression-coefficients" class="nav-link" data-scroll-target="#interpretation-of-the-regression-coefficients"><span class="header-section-number">5.4</span> Interpretation of the regression coefficients</a>
  <ul class="collapse">
  <li><a href="#polynomial-functions-of-the-covariates" id="toc-polynomial-functions-of-the-covariates" class="nav-link" data-scroll-target="#polynomial-functions-of-the-covariates"><span class="header-section-number">5.4.1</span> Polynomial functions of the covariates</a></li>
  <li><a href="#interactions" id="toc-interactions" class="nav-link" data-scroll-target="#interactions"><span class="header-section-number">5.4.2</span> Interactions</a></li>
  </ul></li>
  <li><a href="#sec-fwl" id="toc-sec-fwl" class="nav-link" data-scroll-target="#sec-fwl"><span class="header-section-number">5.5</span> Multiple regression from bivariate regression</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias"><span class="header-section-number">5.6</span> Omitted variable bias</a></li>
  <li><a href="#drawbacks-of-the-blp" id="toc-drawbacks-of-the-blp" class="nav-link" data-scroll-target="#drawbacks-of-the-blp"><span class="header-section-number">5.7</span> Drawbacks of the BLP</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">5.8</span> Summary</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mattblackwell/gov2002-book/edit/main/linear_model.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linear_model.html">Regression</a></li><li class="breadcrumb-item"><a href="./linear_model.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-regression" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Regression is simply a set of tools for evaluating the relationship between an <strong>outcome variable</strong>, <span class="math inline">\(Y_i\)</span>, and a set of <strong>covariates</strong>, <span class="math inline">\(\X_i\)</span>. In particular, these tools show how the conditional mean of <span class="math inline">\(Y_i\)</span> varies as a function of <span class="math inline">\(\X_i\)</span>. For example, we may want to know how wait times at voting precincts vary as a function of various socioeconomic features of the precinct, like income and racial composition. We can accomplish this by estimating the <strong>regression function</strong> or <strong>conditional expectation function</strong> (CEF) of the outcome given the covariates, <span class="math display">\[
\mu(\bfx) = \E[Y_i \mid \X_i = \bfx].
\]</span> Why are estimation and inference for this regression function special? Why can’t we just use the approaches we have seen for the mean, variance, covariance, and so on? The fundamental problem with the CEF is that there may be many values <span class="math inline">\(\bfx\)</span> that can occur and many different conditional expectations that we will need to estimate. If any variable in <span class="math inline">\(\X_i\)</span> is continuous, we must estimate an infinite number of possible values of <span class="math inline">\(\mu(\bfx)\)</span>, and this worsens as we add covariates to <span class="math inline">\(\X_i\)</span>. Because of that, we refer to this problem as the <strong>curse of dimensionality</strong>. How can we resolve this with our measly finite data?</p>
<p>In this chapter, we will explore two ways of “solving” the curse of dimensionality: (1) assuming it away, and (2) changing the quantity of interest to something easier to estimate.</p>
<p>Regression is so ubiquitous across many scientific fields that it has generated a lot of acquired notational baggage. In particular, the labels of the <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\X_i\)</span> vary greatly:</p>
<ul>
<li>The outcome can also be called: the response variable, the dependent variable, the labels (in machine learning), the left-hand side variable, or the regressand</li>
<li>The covariates are also called: the explanatory variables, the independent variables, the predictors, the right-hand side variables, the regressors, inputs, or features</li>
</ul>
<section id="why-do-we-need-models" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="why-do-we-need-models"><span class="header-section-number">5.1</span> Why do we need models?</h2>
<p>At first glance, the connection between the CEF and parametric models might be hazy. For example, imagine we are interested in estimating the average wait times at a voting precinct (<span class="math inline">\(Y_i\)</span>) for Black voters (<span class="math inline">\(X_i = 1\)</span>) versus non-Black voters (<span class="math inline">\(X_i=0\)</span>). In that case, there are two parameters to estimate, <span class="math display">\[
\mu(1) = \E[Y_i \mid X_i = 1] \quad \text{and}\quad \mu(0) = \E[Y_i \mid X_i = 0],
\]</span> which we could estimate by using the plug-in estimators that replace the population averages with their sample counterparts, <span class="math display">\[
\widehat{\mu}(1) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 1)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 1)} \qquad \widehat{\mu}(0) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 0)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 0)}.
\]</span> These are just the sample averages of the wait times for Black and non-Black voters, respectively. And because the race variable here is discrete, we are simply estimating sample means within subpopulations defined by race. The same logic would apply if we had <span class="math inline">\(k\)</span> racial categories: we would have <span class="math inline">\(k\)</span> conditional expectations to estimate and <span class="math inline">\(k\)</span> (conditional) sample means.</p>
<p>Now imagine that we want to know how the average wait time varies as a function of income so that <span class="math inline">\(X_i\)</span> is (essentially) continuous. (Perhaps the theory here is that wait times may be lower in precincts with more affluent voters.) Now we have a different conditional expectation for every possible dollar amount from 0 to however much the wealthiest earner makes. Suppose we choose one particular income, $42,238, and that we are interested in the conditional expectation <span class="math inline">\(\mu(42,238)= \E[Y_{i}\mid X_{i} = 42,238]\)</span>. We could use the same plug-in estimator as in the discrete case, <span class="math display">\[
\widehat{\mu}(42,238) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 42,238)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 42,238)}.
\]</span> This is straightforward, but there is one glaring problem with this estimator: in all likelihood, no units in the particular dataset have that exact income, meaning this estimator is undefined because we would be dividing by zero.</p>
<p>One solution to this problem is to use <strong>subclassification</strong> to turn the continuous variable into a discrete one and then proceed with the discrete approach above. For example, we could group incomes into $25,000 bins and then calculate the average wait times of anyone between, say, $25,000 and $50,000 income. When we make this estimator switch for practical purposes, we need to connect it back to the DGP of interest. We could <strong>assume</strong> that the CEF of interest only depends on these binned means, giving us:<br>
<span class="math display">\[
\mu(x) =
\begin{cases}
  \E[Y_{i} \mid 0 \leq X_{i} &lt; 25,000] &amp;\text{if } 0 \leq x &lt; 25,000 \\
  \E[Y_{i} \mid 25,000 \leq X_{i} &lt; 50,000] &amp;\text{if } 25,000 \leq x &lt; 50,000\\
  \E[Y_{i} \mid 50,000 \leq X_{i} &lt; 100,000] &amp;\text{if } 50,000 \leq x &lt; 100,000\\
  \vdots \\
  \E[Y_{i} \mid 200,000 \leq X_{i}] &amp;\text{if } 200,000 \leq x\\
\end{cases}
\]</span> This approach assumes, perhaps incorrectly, that the average wait time does not vary within the bins. <a href="#fig-cef-binned" class="quarto-xref">Figure&nbsp;<span>5.1</span></a> shows a hypothetical joint distribution between income and wait times with the true CEF, <span class="math inline">\(\mu(x)\)</span>, shown in red. The figure also shows the bins created by subclassification and the implied CEF if we assume bin-constant means in blue. Note that the blue function approximates the true CEF but deviates from it close to the bin edges. The trade-off is that once we make the assumption that wait times do not vary within the bins, we only have to estimate one mean for every bin rather than an infinite number of means for each possible income.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cef-binned" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cef-binned-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linear_model_files/figure-html/fig-cef-binned-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cef-binned-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Hypothetical joint distribution of income and poll wait times (contour plot), conditional expectation function (red), and the conditional expectation of the binned income (blue).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Similarly, we could <strong>assume</strong> that the CEF follows a simple functional form such as a line: <span class="math display">\[
\mu(x) = \E[Y_{i}\mid X_{i} = x] = \beta_{0} + \beta_{1} x.
\]</span> This assumption reduces our infinite number of unknowns (the conditional mean at every possible income) to just two unknowns: (1) the slope and (2) the intercept. As we will see, we can use the standard ordinary least squares to estimate these parameters. Note that if the true CEF is nonlinear, this assumption is incorrect, and any estimate based on this assumption might be biased or even inconsistent.</p>
<p>We call the binning and linear assumptions on <span class="math inline">\(\mu(x)\)</span> <strong>functional form</strong> assumptions because they restrict the class of functions that <span class="math inline">\(\mu(x)\)</span> can take. While powerful, these types of assumptions can muddy the roles of defining the quantity of interest and estimation. If our estimator <span class="math inline">\(\widehat{\mu}(x)\)</span> performs poorly, it will be difficult to tell if this is because the estimator is flawed or our functional form assumptions are incorrect.</p>
<p>To clarify these issues, we will pursue a different approach: understanding what linear regression can estimate under minimal assumptions and then investigating how well this estimand approximates the true CEF.</p>
</section>
<section id="sec-linear-projection" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-linear-projection"><span class="header-section-number">5.2</span> Population linear regression</h2>
<section id="bivariate-linear-regression" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="bivariate-linear-regression"><span class="header-section-number">5.2.1</span> Bivariate linear regression</h3>
<p>Let’s set aside the idea of the conditional expectation function and instead focus on finding the <strong>linear</strong> function of a single covariate <span class="math inline">\(X_i\)</span> that best predicts the outcome. Recall from your earlier mathematical training that linear functions have the form <span class="math inline">\(a + bX_i\)</span>. The <strong>best linear predictor</strong> (BLP) or <strong>population linear regression</strong> of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_i\)</span> is defined as <span class="math display">\[
m(x) = \beta_0 + \beta_1 x \quad\text{where, }\quad (\beta_{0}, \beta_{1}) = \argmin_{(b_{0}, b_{1}) \in \mathbb{R}^{2}}\; \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}].
\]</span> The expression being minimized is the expected prediction error, or the (squared) distance between the observed outcome and the outcome as predicted with a particular slope and intercept. The best linear predictor is the line (that is, slope and intercept values) that results in the lowest expected prediction error. Note that this function is a feature of the joint distribution of the data—the DGP—and so we cannot observe it directly. It must be estimated. The BLP is an alternative to the CEF for summarizing the relationship between the outcome and the covariate, though as we discuss later they will sometimes be equal. We call <span class="math inline">\((\beta_{0}, \beta_{1})\)</span> the <strong>population linear regression coefficients</strong>. Note that <span class="math inline">\(m(x)\)</span> could differ greatly from the CEF <span class="math inline">\(\mu(x)\)</span> if the latter is nonlinear.</p>
<p>We can solve for the best linear predictor using standard calculus (taking the derivative with respect to each coefficient, setting those equations equal to 0, and solving the system of equations). The first-order conditions, in this case, are <span class="math display">\[
\begin{aligned}
  \frac{\partial \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]}{\partial b_{0}} = \E[-2(Y_{i} - \beta_{0} - \beta_{1}X_{i})] = 0 \\
  \frac{\partial \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]}{\partial b_{1}} = \E[-2(Y_{i} - \beta_{0} - \beta_{1}X_{i})X_{i}] = 0
\end{aligned},
\]</span> where we have evaluated the derivatives at the optimum values, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, in the first equality of both lines.</p>
<p>Given the linearity of expectations, it is easy to solve for <span class="math inline">\(\beta_0\)</span> in terms of <span class="math inline">\(\beta_1\)</span>, <span class="math display">\[
\beta_{0} = \E[Y_{i}] - \beta_{1}\E[X_{i}].
\]</span> We can plug this into the first-order condition for <span class="math inline">\(\beta_1\)</span> to get <span class="math display">\[
\begin{aligned}
  0 &amp;= \E[Y_{i}X_{i}] - (\E[Y_{i}] - \beta_{1}\E[X_{i}])\E[X_{i}] - \beta_{1}\E[X_{i}^{2}] \\
    &amp;= \E[Y_{i}X_{i}] - \E[Y_{i}]\E[X_{i}] - \beta_{1}(\E[X_{i}^{2}] - \E[X_{i}]^{2}) \\
    &amp;= \cov(X_{i},Y_{i}) - \beta_{1}\V[X_{i}]\\
  \beta_{1} &amp;= \frac{\cov(X_{i},Y_{i})}{\V[X_{i}]}
\end{aligned}
\]</span></p>
<p>Thus, the slope on the population linear regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_i\)</span> is equal to the ratio of the covariance of the two variables divided by the variance of <span class="math inline">\(X_i\)</span>. It follows from this that the covariance will determine the sign of the slope: positive covariances will lead to positive <span class="math inline">\(\beta_1\)</span> and negative covariances will lead to negative <span class="math inline">\(\beta_1\)</span>. In addition, if <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> are independent, <span class="math inline">\(\beta_1 = 0\)</span>. The slope scales this covariance by the variance of the covariate, so slopes will be lower for more spread-out covariates and higher for less spread-out covariates. If we define the correlation between these variables as <span class="math inline">\(\rho_{YX}\)</span>, then we can relate the coefficient to this quantity as <span class="math display">\[
\beta_1 = \rho_{YX}\sqrt{\frac{\V[Y_i]}{\V[X_i]}}.
\]</span></p>
<p>Collecting these various results together, we can write the population linear regression as <span class="math display">\[
m(x) = \beta_0 + \beta_1x = \E[Y_i] + \beta_1(x - \E[X_i]),
\]</span> which shows how we adjust our best guess about <span class="math inline">\(Y_i\)</span> from the mean of the outcome using the covariate.</p>
<p>Be sure to remember that the BLP, <span class="math inline">\(m(x)\)</span>, and the CEF, <span class="math inline">\(\mu(x)\)</span>, are distinct entities. If the CEF is nonlinear, as in <a href="#fig-cef-blp" class="quarto-xref">Figure&nbsp;<span>5.2</span></a>, there will be a difference between these functions, meaning that the BLP might produce subpar predictions. We will derive a formal connection between the BLP and the CEF below.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cef-blp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cef-blp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linear_model_files/figure-html/fig-cef-blp-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cef-blp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Comparison of the CEF and the best linear predictor.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="beyond-linear-approximations" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="beyond-linear-approximations"><span class="header-section-number">5.2.2</span> Beyond linear approximations</h3>
<p>The linear part of the “best linear predictor” is less restrictive than it appears at first glance. We can easily modify the minimum MSE problem to find the best quadratic, cubic, or general polynomial function of <span class="math inline">\(X_i\)</span> that predicts <span class="math inline">\(Y_i\)</span>. For example, the quadratic function of <span class="math inline">\(X_i\)</span> that best predicts <span class="math inline">\(Y_i\)</span> would be <span class="math display">\[
m(X_i, X_i^2) = \beta_0 + \beta_1X_i + \beta_2X_i^2 \quad\text{where}\quad \argmin_{(b_0,b_1,b_2) \in \mathbb{R}^3}\;\E[(Y_{i} - b_{0} - b_{1}X_{i} - b_{2}X_{i}^{2})^{2}].
\]</span> While the equation is now a quadratic function of the covariates, it is still a linear function of the unknown parameters <span class="math inline">\((\beta_{0}, \beta_{1}, \beta_{2})\)</span>, so we still call this a best linear predictor.</p>
<p>We could include higher-order terms of <span class="math inline">\(X_i\)</span> in the same manner, and, including more polynomial terms, <span class="math inline">\(X_i^p\)</span>, will allow the BLP to be a more flexible function of <span class="math inline">\(X_i\)</span>. When we estimate the BLP, however, we usually pay for this flexibility with overfitting and high variance in our estimates.</p>
</section>
<section id="linear-prediction-with-multiple-covariates" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="linear-prediction-with-multiple-covariates"><span class="header-section-number">5.2.3</span> Linear prediction with multiple covariates</h3>
<p>We now generalize the idea of a best linear predictor to a setting with an arbitrary number of covariates, which more flexibly captures real-life empirical research scenarios. In this setting, recall that the linear function will be</p>
<p><span class="math display">\[
\bfx'\bfbeta = x_{1}\beta_{1} + x_{2}\beta_{2} + \cdots + x_{k}\beta_{k}.
\]</span> We will define the <strong>best linear predictor</strong> (BLP) to be <span class="math display">\[
m(\bfx) = \bfx'\bfbeta, \quad \text{where}\quad \bfbeta = \argmin_{\mb{b} \in \real^k}\; \E\bigl[ \bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2\bigr]
\]</span></p>
<p>This BLP solves the same fundamental optimization problem as in the bivariate case: it chooses the set of coefficients that minimizes the expected mean-squared error, where the expectation is over the joint distribution of the data.</p>
<!-- TODO: make this #assumption -->
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Best linear projection assumptions
</div>
</div>
<div class="callout-body-container callout-body">
<p>Without some assumptions on the joint distribution of the data, the following “regularity conditions” will ensure the existence of the BLP:</p>
<ol type="1">
<li><span class="math inline">\(\E[Y^2] &lt; \infty\)</span> (outcome has finite mean/variance)</li>
<li><span class="math inline">\(\E\Vert \mb{X} \Vert^2 &lt; \infty\)</span> (<span class="math inline">\(\mb{X}\)</span> has finite means/variances/covariances)</li>
<li><span class="math inline">\(\mb{Q}_{\mb{XX}} = \E[\mb{XX}']\)</span> is positive definite (columns of <span class="math inline">\(\X\)</span> are linearly independent)<br>
</li>
</ol>
</div>
</div>
<p>Under these assumptions, it is possible to derive a closed-form expression for the <strong>population coefficients</strong> <span class="math inline">\(\bfbeta\)</span> using matrix calculus. To set up the optimization problem, we find the first-order condition by taking the derivative of the expectation of the squared errors. First, take the derivative of the squared prediction errors using the chain rule: <span class="math display">\[
\begin{aligned}
  \frac{\partial}{\partial \mb{b}}\left(Y_{i} - \X_{i}'\mb{b}\right)^{2}
  &amp;= 2\left(Y_{i} - \X_{i}'\mb{b}\right)\frac{\partial}{\partial \mb{b}}(Y_{i} - \X_{i}'\mb{b}) \\
  &amp;= -2\left(Y_{i} - \X_{i}'\mb{b}\right)\X_{i} \\
  &amp;= -2\X_{i}\left(Y_{i} - \X_{i}'\mb{b}\right) \\
  &amp;= -2\left(\X_{i}Y_{i} - \X_{i}\X_{i}'\mb{b}\right),
\end{aligned}
\]</span> where the third equality comes from the fact that <span class="math inline">\((Y_{i} - \X_{i}'\bfbeta)\)</span> is a scalar. We can plug this into the expectation to get the first-order condition and solve for <span class="math inline">\(\bfbeta\)</span>, <span class="math display">\[
\begin{aligned}
  0 &amp;= -2\E[\X_{i}Y_{i} - \X_{i}\X_{i}'\bfbeta ] \\
  \E[\X_{i}\X_{i}'] \bfbeta &amp;= \E[\X_{i}Y_{i}],
\end{aligned}
\]</span> which implies the population coefficients are <span class="math display">\[
\bfbeta = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] = \mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y}
\]</span> This gives us an expression for the coefficients for the population best linear predictor in terms of the joint distribution <span class="math inline">\((Y_{i}, \X_{i})\)</span>.</p>
<p>A couple of facts might be useful for interpreting this expression substantively. Recall that <span class="math inline">\(\mb{Q}_{\mb{XX}} = \E[\X_{i}\X_{i}']\)</span> is a <span class="math inline">\(k\times k\)</span> matrix and <span class="math inline">\(\mb{Q}_{\X Y} = \E[\X_{i}Y_{i}]\)</span> is a <span class="math inline">\(k\times 1\)</span> column vector, which implies that <span class="math inline">\(\bfbeta\)</span> is also a <span class="math inline">\(k \times 1\)</span> column vector.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>What does the expression for the population regression coefficients mean? It is helpful to separate the intercept or constant term so that we have <span class="math display">\[
Y_{i} = \beta_{0} + \X'\bfbeta + e_{i},
\]</span> so <span class="math inline">\(\bfbeta\)</span> refers to just the vector of coefficients for the covariates. In this case, we can write the coefficients in a more interpretable way: <span class="math display">\[
\bfbeta = \V[\X]^{-1}\cov(\X, Y), \qquad \beta_0 = \mu_Y - \mb{\mu}'_{\mb{X}}\bfbeta
\]</span></p>
<p>Thus, the population coefficients take the covariance between the outcome and the covariates and “divide” it by information about variances and covariances of the covariates. The intercept recenters the regression so that projection errors are mean zero. This means that these coefficients generalize the bivariate formula to this multiple covariate context.</p>
</div>
</div>
<p>With an expression for the population linear regression coefficients, we can write the linear projection as <span class="math display">\[
m(\X_{i}) = \X_{i}'\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] = \X_{i}'\mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y}
\]</span></p>
</section>
<section id="projection-error" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="projection-error"><span class="header-section-number">5.2.4</span> Projection error</h3>
<p>The <strong>projection error</strong> or is the difference between the actual value of <span class="math inline">\(Y_i\)</span> and the projection, <span class="math display">\[
e_{i} = Y_{i} - m(\X_{i}) = Y_i - \X_{i}'\bfbeta,
\]</span> where we have made no assumptions about this error yet. The projection error is simply the prediction error of the best linear prediction for a particular unit in the data. Rewriting this definition, we can see that we can always write the outcome as the linear projection plus the projection error, <span class="math display">\[
Y_{i} = \X_{i}'\bfbeta + e_{i}.
\]</span> Notice that this looks suspiciously similar to a linearity assumption on the CEF, but we haven’t made any assumptions here. Instead, we just used the definition of the projection error to write a tautological statement: <span class="math display">\[
Y_{i} = \X_{i}'\bfbeta + e_{i} = \X_{i}'\bfbeta + Y_{i} - \X_{i}'\bfbeta = Y_{i}.
\]</span> The critical difference between this representation and the usual linear model assumption is what properties <span class="math inline">\(e_{i}\)</span> possesses.</p>
<p>A key property of the projection errors is that when the covariate vector includes an “intercept” or constant term, the projection errors are uncorrelated with the covariates. To see this, first note that <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span> since <span class="math display">\[
\begin{aligned}
  \E[\X_{i}e_{i}] &amp;= \E[\X_{{i}}(Y_{i} - \X_{i}'\bfbeta)] \\
                  &amp;= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\bfbeta \\
                  &amp;= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] \\
  &amp;= \E[\X_{i}Y_{i}] - \E[\X_{i}Y_{i}] = 0
\end{aligned}
\]</span> Thus, for every <span class="math inline">\(X_{ij}\)</span> in <span class="math inline">\(\X_{i}\)</span>, we have <span class="math inline">\(\E[X_{ij}e_{i}] = 0\)</span>. If one of the entries in <span class="math inline">\(\X_i\)</span> is a constant 1, then this also implies that <span class="math inline">\(\E[e_{i}] = 0\)</span>. Together, these facts imply that the projection error is uncorrelated with each <span class="math inline">\(X_{ij}\)</span>, since <span class="math display">\[
\cov(X_{ij}, e_{i}) = \E[X_{ij}e_{i}] - \E[X_{ij}]\E[e_{i}] = 0 - 0 = 0
\]</span> Note that we still have made no assumptions about these projection errors except for some mild regularity conditions on the joint distribution of the outcome and covariates. Thus, in very general settings, we can write the linear projection model <span class="math inline">\(Y_i = \X_i'\bfbeta + e_i\)</span> where <span class="math inline">\(\bfbeta = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]\)</span> and conclude that <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span> by definition, not by assumption.</p>
<p>The projection error is uncorrelated with the covariates, so does this mean that the CEF is linear? Unfortunately, no. Recall that while independence implies this lack of correlation, the reverse does not hold. So when we look at the CEF, we have <span class="math display">\[
\E[Y_{i} \mid \X_{i}] = \X_{i}'\bfbeta + \E[e_{i} \mid \X_{i}],
\]</span> and the last term <span class="math inline">\(\E[e_{i} \mid \X_{i}]\)</span> would only be 0 if the errors were independent of the covariates, so <span class="math inline">\(\E[e_{i} \mid \X_{i}] = \E[e_{i}] = 0\)</span>. But nowhere in the linear projection model did we assume this. So while we can (almost) always write the outcome as <span class="math inline">\(Y_i = \X_i'\bfbeta + e_i\)</span> and have those projection errors be uncorrelated with the covariates, it will require additional assumptions to ensure that the true CEF is, in fact, linear <span class="math inline">\(\E[Y_{i} \mid \X_{i}] = \X_{i}'\bfbeta\)</span>.</p>
<p>To step back for a moment, what have we shown here? In a nutshell, we showed that a population linear regression exists under very general conditions and that we can write the coefficients of that population linear regression as a function of expectations of the joint distribution of the data. We did not, however, assume that the CEF was linear nor that the projection errors were normally distributed.</p>
<p>Why is this important? The ordinary least squares estimator, the workhorse regression estimator of the social sciences, targets this quantity of interest in large samples, regardless of whether the true CEF is linear or not. Thus, even when a linear CEF assumption is incorrect and the projection errors are not normally distributed, OLS still targets a perfectly valid quantity of interest: the coefficients from this population linear projection.</p>
</section>
</section>
<section id="linear-cefs-without-assumptions" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="linear-cefs-without-assumptions"><span class="header-section-number">5.3</span> Linear CEFs without assumptions</h2>
<p>What is the relationship between the best linear predictor (which we just saw generally exists) and the CEF? To draw the connection, remember that the conditional expectation is importantly the function of <span class="math inline">\(\X_i\)</span> that best predicts <span class="math inline">\(Y_{i}\)</span>. The population regression was the best <strong>linear</strong> predictor, but the CEF is the best predictor among all nicely behaved functions of <span class="math inline">\(\X_{i}\)</span>, linear or nonlinear. In particular, if we label <span class="math inline">\(L_2\)</span> to be the set of all functions of the covariates <span class="math inline">\(g()\)</span> that have finite squared expectation, <span class="math inline">\(\E[g(\X_{i})^{2}] &lt; \infty\)</span>, then we can show that the CEF has the lowest squared prediction error in this class of functions: <span class="math display">\[
\mu(\X) = \E[Y_{i} \mid \X_{i}] = \argmin_{g(\X_i) \in L_2}\; \E\left[(Y_{i} - g(\X_{i}))^{2}\right],
\]</span></p>
<p>So we have established that the CEF is the best predictor and the population linear regression <span class="math inline">\(m(\X_{i})\)</span> is the best linear predictor. These two facts allow us to connect the CEF to the population regression.</p>
<div id="thm-cef-blp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1</strong></span> If <span class="math inline">\(\mu(\X_{i})\)</span> is a linear function of <span class="math inline">\(\X_i\)</span>, then <span class="math inline">\(\mu(\X_{i}) = m(\X_{i}) = \X_i'\bfbeta\)</span>.</p>
</div>
<p>This theorem says that if the true CEF is linear, it must equal the population linear regression. The proof of this is straightforward: the CEF is the best predictor, so if it is linear, it must also be the best linear predictor.</p>
<p>In general, we are in the business of learning about the CEF, so we are unlikely to know if it genuinely is linear or not. In some situations, however, we can show that the CEF is linear without any additional assumptions. These are situations where the covariates take on a finite number of possible values. Going back to the example from the chapter introduction, suppose we are interested in the CEF of wait times at voting precincts for Black (<span class="math inline">\(X_i = 1\)</span>) vs.&nbsp;non-Black (<span class="math inline">\(X_i = 0\)</span>) voters. In this case, there are two possible values of the CEF, <span class="math inline">\(\mu(1) = \E[Y_{i}\mid X_{i}= 1]\)</span>, the average wait time for Black voters, and <span class="math inline">\(\mu(0) = \E[Y_{i}\mid X_{i} = 0]\)</span>, the average wait time for non-Black voters. Notice that we can write the CEF as <span class="math display">\[
\mu(x) = x \mu(1) + (1 - x) \mu(0) = \mu(0) + x\left(\mu(1) - \mu(0)\right)= \beta_0 + x\beta_1,
\]</span> which is clearly a linear function of <span class="math inline">\(x\)</span>. Based on this derivation, we obtain coefficients of this linear CEF that have clear substantive interpretations:</p>
<ul>
<li><span class="math inline">\(\beta_0 = \mu(0)\)</span>: the expected wait time for a Black voter.</li>
<li><span class="math inline">\(\beta_1 = \mu(1) - \mu(0)\)</span>: the difference in average wait times between Black and non-Black voters. How <span class="math inline">\(X_{i}\)</span> is defined here is important since the intercept will always be the average outcome when <span class="math inline">\(X_i = 0\)</span>, and the slope will always be the difference in means between the <span class="math inline">\(X_i = 1\)</span> group and the <span class="math inline">\(X_i = 0\)</span> group.</li>
</ul>
<p>What about a categorical covariate with more than two levels? For example, we may be interested in wait times by party identification, where <span class="math inline">\(X_i = 1\)</span> indicates Democratic voters, <span class="math inline">\(X_i = 2\)</span> indicates Republican voters, and <span class="math inline">\(X_i = 3\)</span> indicates Independent voters. We could write the CEF of wait times as a linear function of this variable, but that would assume that the difference between Democrats and Republicans is exactly the same as for Independents and Republicans – which is probably false. With more than two levels, we can represent a categorical variable as a vector of binary variables, <span class="math inline">\(\X_i = (X_{i1}, X_{i2})\)</span>, where <span class="math display">\[
\begin{aligned}
  X_{{i1}} &amp;= \begin{cases}
                1&amp;\text{if Republican} \\
                   0 &amp; \text{if not Republican}
              \end{cases} \\
X_{{i2}} &amp;= \begin{cases}
                1&amp;\text{if independent} \\
                   0 &amp; \text{if not independent}
              \end{cases} \\
\end{aligned}
\]</span> These two indicator variables encode the same information as the original single three-level variable, <span class="math inline">\(X_{i}\)</span>, so if we know the values of <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span>, then we know exactly to which party <span class="math inline">\(i\)</span> belongs. Thus, the CEFs for <span class="math inline">\(X_i\)</span> and the pair of indicator variables, <span class="math inline">\(\X_i\)</span>, are precisely the same, but the latter allows for a lovely linear representation, <span class="math display">\[
\E[Y_i \mid X_{i1}, X_{i2}] = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2},
\]</span> where</p>
<ul>
<li><span class="math inline">\(\beta_0 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\)</span> is the average wait time for the group who does not get an indicator variable (Democrats in this case). This group is sometimes called the baseline group or the omitted group.</li>
<li><span class="math inline">\(\beta_1 = \E[Y_{i} \mid X_{i1} = 1, X_{i2} = 0] - \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\)</span> is the difference in means between Republican voters and Democratic voters, or the difference between the first indicator group and the baseline group.</li>
<li><span class="math inline">\(\beta_2 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 1] - \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\)</span> is the difference in means between independent voters and Democratic voters, or the difference between the second indicator group and the baseline group.</li>
</ul>
<p>This approach easily generalizes to categorical variables with an arbitrary number of levels.</p>
<p>What have we shown? The CEF is linear without additional assumptions when there is a categorical covariate. We can show that this continues to hold even when we have multiple categorical variables. We now have two binary covariates: <span class="math inline">\(X_{i1}=1\)</span> indicating a Black voter, and <span class="math inline">\(X_{i2} = 1\)</span> indicating a retired voter versus a working-age voter. These two binary variables give us four possible values of the CEF: <span class="math display">\[
\mu(x_1, x_2) = \begin{cases}
\mu_{00} &amp; \text{if } x_1 = 0 \text{ and } x_2 = 0 \text{ (non-Black, working age)} \\
\mu_{10} &amp; \text{if } x_1 = 1 \text{ and } x_2 = 0 \text{ (Black, working age)} \\
\mu_{01} &amp; \text{if } x_1 = 0 \text{ and } x_2 = 1 \text{ (non-Black, retired)} \\
\mu_{11} &amp; \text{if } x_1 = 1 \text{ and } x_2 = 1 \text{ (Black, retired)}
\end{cases}
\]</span> We can write this as <span class="math display">\[
\mu(x_{1}, x_{2}) = (1 - x_{1})(1 - x_{2})\mu_{00} + x_{1}(1 -x_{2})\mu_{10} + (1-x_{1})x_{2}\mu_{01} + x_{1}x_{2}\mu_{11},
\]</span> which we can rewrite as <span class="math display">\[
\mu(x_1, x_2) = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3,
\]</span> where the substantive interpretations are</p>
<ul>
<li><span class="math inline">\(\beta_0 = \mu_{00}\)</span>: average wait times for working-age non-Black voters.</li>
<li><span class="math inline">\(\beta_1 = \mu_{10} - \mu_{00}\)</span>: difference in means for working-age Black vs.&nbsp;working-age non-Black voters.</li>
<li><span class="math inline">\(\beta_2 = \mu_{01} - \mu_{00}\)</span>: difference in means for retired non-Black vs.&nbsp;working-age non-Black voters.</li>
<li><span class="math inline">\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\)</span>: difference in retired racial difference vs working-age racial difference.</li>
</ul>
<p>Thus, we can write the CEF with two binary covariates as linear when the linear specification includes a multiplicative interaction between them (<span class="math inline">\(x_1x_2\)</span>). This result holds for all pairs of binary covariates, and we can generalize the interpretation of the coefficients in the CEF as</p>
<ul>
<li><span class="math inline">\(\beta_0 = \mu_{00}\)</span>: average outcome when both variables are 0.</li>
<li><span class="math inline">\(\beta_1 = \mu_{10} - \mu_{00}\)</span>: difference in average outcomes for the first covariate when the second covariate is 0.</li>
<li><span class="math inline">\(\beta_2 = \mu_{01} - \mu_{00}\)</span>: difference in average outcomes for the second covariate when the first covariate is 0.</li>
<li><span class="math inline">\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\)</span>: change in the “effect” of the first (second) covariate when the second (first) covariate goes from 0 to 1.</li>
</ul>
<p>This result also generalizes to an arbitrary number of binary covariates. If we have <span class="math inline">\(p\)</span> binary covariates, then the CEF will be linear with all two-way interactions, <span class="math inline">\(x_1x_2\)</span>, all three-way interactions, <span class="math inline">\(x_1x_2x_3\)</span>, up to the <span class="math inline">\(p\)</span>-way interaction <span class="math inline">\(x_1\times\cdots\times x_p\)</span>. Furthermore, we can generalize to arbitrary numbers of categorical variables by expanding each into a series of binary variables and then including all interactions between the resulting binary variables.</p>
<p>We have established that when we have a set of categorical covariates, the true CEF will be linear, and we have seen the various ways to represent that CEF. Note that when we use, for example, ordinary least squares, we are free to choose how to include our variables. We could run a regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span> without an interaction term, but this model will only be correct if <span class="math inline">\(\beta_3\)</span> is equal to 0, and so the interaction term is irrelevant. We call a model <strong>saturated</strong> if there are as many coefficients as the CEF’s unique values. A saturated model can, by its nature, always be written as a linear function without assumptions. The above examples show how to construct saturated models in various situations.</p>
</section>
<section id="interpretation-of-the-regression-coefficients" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="interpretation-of-the-regression-coefficients"><span class="header-section-number">5.4</span> Interpretation of the regression coefficients</h2>
<p>We have seen how to interpret population regression coefficients when the CEF is linear without assumptions. How do we interpret the population coefficients <span class="math inline">\(\bfbeta\)</span> in other settings?</p>
<p>Consider the simplest case, one in which every entry in <span class="math inline">\(\X_{i}\)</span> represents a different covariate and no covariate is any function of another (we will see why this caveat is necessary below). In this simple case, the <span class="math inline">\(k\)</span>th coefficient, <span class="math inline">\(\beta_{k}\)</span>, represents the change in the predicted outcome for a one-unit change in the <span class="math inline">\(k\)</span>th covariate <span class="math inline">\(X_{ik}\)</span>, holding all other covariates fixed. We can see this from <span class="math display">\[
\begin{aligned}
  m(x_{1} + 1, x_{2}) &amp; = \beta_{0} + \beta_{1}(x_{1} + 1) + \beta_{2}x_{2} \\
  m(x_{1}, x_{2}) &amp;= \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2},
\end{aligned}
\]</span> so that the change in the predicted outcome for increasing <span class="math inline">\(X_{i1}\)</span> by one unit is <span class="math display">\[
m(x_{1} + 1, x_{2}) - m(x_{1}, x_{2}) = \beta_1
\]</span> Notice that nothing changes in this interpretation when adding more covariates to the vector, <span class="math display">\[
m(x_{1} + 1, \bfx_{2}) - m(x_{1}, \bfx_{2}) = \beta_1,
\]</span> The coefficient on a particular variable is the change in the predicted outcome corresponding to a one-unit change in the covariate holding all other covariates constant. Each coefficient summarizes the “all else equal” difference in the predicted outcome for each covariate.</p>
<section id="polynomial-functions-of-the-covariates" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="polynomial-functions-of-the-covariates"><span class="header-section-number">5.4.1</span> Polynomial functions of the covariates</h3>
<p>The interpretation of the population regression coefficients becomes more complicated when including nonlinear functions of the covariates. In that case, multiple coefficients control how a change in a covariate will change the predicted value of <span class="math inline">\(Y_i\)</span>. For example, suppose we have a quadratic function of <span class="math inline">\(X_{i1}\)</span>, <span class="math display">\[
m(x_1, x_1^2, x_{2}) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2} + \beta_{3}x_{2},
\]</span> and try to look at a one-unit change in <span class="math inline">\(x_1\)</span>, <span class="math display">\[
\begin{aligned}
  m(x_{1} + 1, (x_{1} + 1)^{2}, x_{2}) &amp; = \beta_{0} + \beta_{1}(x_{1} + 1) + \beta_{2}(x_{1} + 1)^{2}+ \beta_{3}x_{2} \\
  m(x_{1}, x_{1}^{2}, x_{2}) &amp;= \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2} + \beta_{3}x_{2},
\end{aligned}
\]</span> resulting in <span class="math inline">\(\beta_1 + \beta_2(2x_{1} + 1)\)</span>. This formula might be an interesting quantity, but we more commonly use the derivative of <span class="math inline">\(m(\bfx)\)</span> with respect to <span class="math inline">\(x_1\)</span> as a measure of the marginal effect of <span class="math inline">\(X_{i1}\)</span> on the predicted value of <span class="math inline">\(Y_i\)</span> (holding all other variables constant), where “marginal” here means the change in prediction for a very small change in <span class="math inline">\(X_{i1}\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In the case of the quadratic covariate, we have <span class="math display">\[
\frac{\partial m(x_{1}, x_{1}^{2}, x_{2})}{\partial x_{1}} = \beta_{1} + 2\beta_{2}x_{1},
\]</span> so the marginal effect on prediction varies as a function of <span class="math inline">\(x_1\)</span>. From this, we see that the individual interpretations of the coefficients are less interesting: <span class="math inline">\(\beta_1\)</span> is the marginal effect when <span class="math inline">\(X_{i1} = 0\)</span> and <span class="math inline">\(\beta_2 / 2\)</span> describes how a one-unit change in <span class="math inline">\(X_{i1}\)</span> changes the marginal effect. As is hopefully clear, it will often be more straightforward to visualize the nonlinear predictor function (perhaps using the orthogonalization techniques in <a href="#sec-fwl" class="quarto-xref"><span>Section 5.5</span></a>).</p>
</section>
<section id="interactions" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="interactions"><span class="header-section-number">5.4.2</span> Interactions</h3>
<p>Another common nonlinear function occurs when including <strong>interaction terms</strong> or covariates that are products of two other covariates, <span class="math display">\[
m(x_{1}, x_{2}, x_{1}x_{2}) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{1}x_{2}.
\]</span> In these situations, we can use the derivative of the BLP to measure the marginal effect of one variable or the other on the predicted value of <span class="math inline">\(Y_i\)</span>. In particular, we have <span class="math display">\[
\begin{aligned}
  \frac{\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\partial x_1} &amp;= \beta_1 + \beta_3x_2, \\
  \frac{\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\partial x_2} &amp;= \beta_2 + \beta_3x_1.
\end{aligned}
\]</span> Here, the coefficients are slightly more interpretable:</p>
<ul>
<li><span class="math inline">\(\beta_1\)</span>: the marginal effect of <span class="math inline">\(X_{i1}\)</span> on predicted <span class="math inline">\(Y_i\)</span> when <span class="math inline">\(X_{i2} = 0\)</span>.</li>
<li><span class="math inline">\(\beta_2\)</span>: the marginal effect of <span class="math inline">\(X_{i2}\)</span> on predicted <span class="math inline">\(Y_i\)</span> when <span class="math inline">\(X_{i1} = 0\)</span>.</li>
<li><span class="math inline">\(\beta_3\)</span>: the change in the marginal effect of <span class="math inline">\(X_{i1}\)</span> due to a one-unit change in <span class="math inline">\(X_{i2}\)</span> <strong>OR</strong> the change in the marginal effect of <span class="math inline">\(X_{i2}\)</span> due to a one-unit change in <span class="math inline">\(X_{i1}\)</span>.</li>
</ul>
<p>If we add more covariates to this BLP, these interpretations change to “holding all other covariates constant.”</p>
<p>Interactions are a standard part of social science research because they allow us to assess how the relationship between the outcome and an independent variable varies by the values of another variable. In the context of our study of wait times at a voting precinct, if <span class="math inline">\(X_{i1}\)</span> is income and <span class="math inline">\(X_{i2}\)</span> is the Black/non-Black voter indicator, then <span class="math inline">\(\beta_3\)</span> represents the change in the slope of the wait time-income relationship between Black and non-Black voters.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Centering variables to improve interpretability
</div>
</div>
<div class="callout-body-container callout-body">
<p>In many cases, the so-called marginal coefficients on the lower-order terms (<span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(\beta_2\)</span> for <span class="math inline">\(X_{i2}\)</span>) are uninteresting because they represent the marginal effect of one variable when the other is 0. If <span class="math inline">\(X_{i1}\)</span> is age and <span class="math inline">\(X_{i2}\)</span> is the Black/non-Black indicator, then <span class="math inline">\(\beta_2\)</span> is the estimated difference in average voter wait times for voters who are zero years old, an obviously nonsensical parameter. We can improve the interpretability of the coefficient by recentering the age variable. Suppose we include a mean-centered version of age, <span class="math display">\[
\widetilde{X}_{i1} = X_{i1} - \overline{X}_1
\]</span> in place of <span class="math inline">\(X_{i1}\)</span>. That is, we regress <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(\widetilde{X}_{i1}\)</span>, <span class="math inline">\(X_{i2}\)</span>, and <span class="math inline">\(\widetilde{X}_{i1}X_{i2}\)</span>. In this case, <span class="math inline">\(\beta_2\)</span> (the coefficient on the race indicator <span class="math inline">\(X_{i2}\)</span>) is the marginal effect of <span class="math inline">\(X_{i2}\)</span> when <span class="math inline">\(\widetilde{X}_{i1} = 0\)</span> or when <span class="math inline">\(X_{i1} = \overline{X}_1\)</span>. Thus, this coefficient is now the estimated difference in average voter wait times for the average-aged voter, which is far more interpretable. This recentering has no effect on either <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(\beta_3\)</span>, which is rather remarkable.</p>
</div>
</div>
</section>
</section>
<section id="sec-fwl" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-fwl"><span class="header-section-number">5.5</span> Multiple regression from bivariate regression</h2>
<p>With a regression of an outcome on two covariates, understanding how the coefficients of one variable relate to the other is helpful. Consider the following best linear projection: <span id="eq-two-var-blp"><span class="math display">\[
(\alpha, \beta, \gamma) = \argmin_{(a,b,c) \in \mathbb{R}^{3}} \; \E[(Y_{i} - (a + bX_{i} + cZ_{i}))^{2}]
\tag{5.1}\]</span></span> Can we understand the <span class="math inline">\(\beta\)</span> coefficient here in terms of a bivariate regression? As it turns out, yes. From the above results, we know that the intercept has a simple form: <span class="math display">\[
\alpha = \E[Y_i] - \beta\E[X_i] - \gamma\E[Z_i].
\]</span> Let’s investigate the first order condition for <span class="math inline">\(\beta\)</span>: <span class="math display">\[
\begin{aligned}
  0 &amp;= \E[Y_{i}X_{i}] - \alpha\E[X_{i}] - \beta\E[X_{i}^{2}] - \gamma\E[X_{i}Z_{i}] \\
    &amp;= \E[Y_{i}X_{i}] - \E[Y_{i}]\E[X_{i}] + \beta\E[X_{i}]^{2} + \gamma\E[X_{i}]\E[Z_{i}] - \beta\E[X_{i}^{2}] - \gamma\E[X_{i}Z_{i}] \\
  &amp;= \cov(Y, X) - \beta\V[X_{i}] - \gamma \cov(X_{i}, Z_{i})
\end{aligned}
\]</span> We can see from this that if <span class="math inline">\(\cov(X_{i}, Z_{i}) = 0\)</span>, then the coefficient on <span class="math inline">\(X_i\)</span> will be the same as in the simple regression case, <span class="math inline">\(\cov(Y_{i}, X_{i})/\V[X_{i}]\)</span>. When <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Z_i\)</span> are uncorrelated, we sometimes call them <strong>orthogonal</strong>.</p>
<p>To write a simple formula for <span class="math inline">\(\beta\)</span> when the covariates are not orthogonal, we <strong>orthogonalize</strong> <span class="math inline">\(X_i\)</span> by obtaining the prediction errors from a population linear regression of <span class="math inline">\(X_i\)</span> on <span class="math inline">\(Z_i\)</span>: <span class="math display">\[
\widetilde{X}_{i} = X_{i} - (\delta_{0} + \delta_{1}Z_{i}) \quad\text{where}\quad (\delta_{0}, \delta_{1}) = \argmin_{(d_{0},d_{1}) \in \mathbb{R}^{2}} \; \E[(X_{i} - (d_{0} + d_{1}Z_{i}))^{2}]
\]</span> Given the properties of projection errors, we know that this orthogonalized version of <span class="math inline">\(X_{i}\)</span> will be uncorrelated with <span class="math inline">\(Z_{i}\)</span> since <span class="math inline">\(\E[\widetilde{X}_{i}Z_{i}] = 0\)</span>. Remarkably, the coefficient on <span class="math inline">\(X_i\)</span> from the “long” BLP in <a href="#eq-two-var-blp" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> is the same as the regression of <span class="math inline">\(Y_i\)</span> on this orthogonalized <span class="math inline">\(\widetilde{X}_i\)</span>, <span class="math display">\[
\beta = \frac{\cov(Y_{i}, \widetilde{X}_{i})}{\V[\widetilde{X}_{i}]}
\]</span></p>
<p>We can expand this idea to when there are several other covariates. Suppose now that we are interested in a regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(\X_i\)</span> and we are interested in the coefficient on the <span class="math inline">\(k\)</span>th covariate. Let <span class="math inline">\(\X_{i,-k}\)</span> be the vector of covariates omitting the <span class="math inline">\(k\)</span>th entry and let <span class="math inline">\(m_k(\X_{i,-k})\)</span> represent the BLP of <span class="math inline">\(X_{ik}\)</span> on these other covariates. We can define <span class="math inline">\(\widetilde{X}_{ik} = X_{ik} - m_{k}(\X_{i,-k})\)</span> as the <span class="math inline">\(k\)</span>th variable orthogonalized with respect to the rest of the variables and we can write the coefficient on <span class="math inline">\(X_{ik}\)</span> as <span class="math display">\[
\beta_k = \frac{\cov(Y_i, \widetilde{X}_{ik})}{\V[\widetilde{X}_{ik}]}.
\]</span> Thus, the population regression coefficient in the BLP is the same as from a bivariate regression of the outcome on the projection error for <span class="math inline">\(X_{ik}\)</span> projected on all other covariates. One interpretation of coefficients in a population multiple regression is that they represent the relationship between the outcome and the covariate after removing the linear relationships of all other variables.</p>
</section>
<section id="omitted-variable-bias" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="omitted-variable-bias"><span class="header-section-number">5.6</span> Omitted variable bias</h2>
<p>In many situations, we may need to choose whether to include a variable in a regression, so it can be helpful to understand how this choice might affect the population coefficients on the other variables in the regression. Suppose we have a variable <span class="math inline">\(Z_i\)</span> that we may add to our regression, which currently has <span class="math inline">\(\X_i\)</span> as the covariates. We can write this new projection as <span class="math display">\[
m(\X_i, Z_i) = \X_i'\bfbeta + Z_i\gamma, \qquad m(\X_{i}) = \X_i'\bs{\delta},
\]</span> where we often refer to <span class="math inline">\(m(\X_i, Z_i)\)</span> as the long regression and <span class="math inline">\(m(\X_i)\)</span> as the short regression.</p>
<p>From the definition of the BLP, we can write the short coefficients as <span class="math display">\[
\bs{\delta} = \left(\E[\X_{i}\X_{i}']\right)^{-1} \E[\X_{i}Y_{i}].
\]</span> Letting <span class="math inline">\(e_i = Y_i - m(\X_{i}, Z_{i})\)</span> be the projection errors from the long regression, we can write this as <span class="math display">\[
\begin{aligned}
  \bs{\delta} &amp;= \left(\E[\X_{i}\X_{i}']\right)^{-1} \E[\X_{i}(\X_{i}'\bfbeta + Z_{i}\gamma + e_{i})] \\
              &amp;= \left(\E[\X_{i}\X_{i}']\right)^{-1}(\E[\X_{i}\X_{i}']\bfbeta + \E[\X_{i}Z_{i}]\gamma + \E[\X_{i}e_{i}]) \\
              &amp;= \bfbeta + \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Z_{i}]\gamma
\end{aligned}
\]</span> Note that the vector in the second term is the vector of linear projection coefficients of a population linear regression of <span class="math inline">\(Z_i\)</span> on the <span class="math inline">\(\X_i\)</span>. If we call these coefficients <span class="math inline">\(\bs{\pi}\)</span>, then the short coefficients are <span class="math display">\[
\bs{\delta} = \bfbeta + \bs{\pi}\gamma.
\]</span></p>
<p>We can rewrite this to show that the difference between the coefficients in these two projections is <span class="math inline">\(\bs{\delta} - \bfbeta= \bs{\pi}\gamma\)</span> or the product of the coefficient on the “excluded” <span class="math inline">\(Z_i\)</span> and the coefficient of the included <span class="math inline">\(\X_i\)</span> on the excluded. Most textbooks refer to this difference as the <strong>omitted variable bias</strong> of omitting <span class="math inline">\(Z_i\)</span> under the idea that <span class="math inline">\(\bfbeta\)</span> is the true target of inference. But the result is much broader than this since it tells us how to relate the coefficients of two nested projections.</p>
<p>The last two results (multiple regressions from bivariate and omitted variable bias) are sometimes presented as results for the ordinary least squares estimator that we will show in the next chapter. We introduce them here as features of a particular population quantity, the linear projection or population linear regression.</p>
</section>
<section id="drawbacks-of-the-blp" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="drawbacks-of-the-blp"><span class="header-section-number">5.7</span> Drawbacks of the BLP</h2>
<p>The best linear predictor is, of course, a <em>linear</em> approximation to the CEF, and this approximation could be quite poor if the true CEF is highly nonlinear. A more subtle issue with the BLP is that it is sensitive to the marginal distribution of the covariates when the CEF is nonlinear. Let’s return to our example of wait times at voting precincts and income. In <a href="#fig-blp-limits" class="quarto-xref">Figure&nbsp;<span>5.3</span></a>, we show the true CEF and the BLP when we restrict income below $50,000 or above $100,000. The BLP can vary quite dramatically here. This figure is an extreme example, but the essential point still holds as the marginal distribution of <span class="math inline">\(X_i\)</span> changes.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-blp-limits" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blp-limits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linear_model_files/figure-html/fig-blp-limits-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blp-limits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Linear projections for when truncating income distribution below $50k and above $100k.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="summary"><span class="header-section-number">5.8</span> Summary</h2>
<p>As we discussed in this chapter, with even a moderate number of covariates, conditional expectation functions (also known as regressions) become very difficult to estimate because of the high dimensionality involved. To avoid this problem, we can focus on a different quantity, the <strong>best linear predictor</strong> which is the linear function of the covariates that best predicts the outcome in mean-squared error. The BLP exists under very mild conditions and has very interpretable parameters. Another strategy is to impose a linearity assumption on the <strong>conditional expectation function</strong> to make it more estimable, in which case, the BLP and the CEF are the same function. With a small number of discrete covariates it is possible to <strong>saturate</strong> a model so that linearity holds mechanically. Coefficients on population linear regressions with multiple independent variables can always be written in terms of a regression of the outcome on one variable orthogonalized relative to the rest of the independent variables. The <strong>omitted variable bias</strong> formula shows how leaving a variable out of the best linear affects the coefficients on other independent variables. In the next chapter, we will turn to using data to estimate the coefficients for these population linear regressions.</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note the choice of language here. The marginal effect is on the predicted value of <span class="math inline">\(Y_i\)</span>, not on <span class="math inline">\(Y_i\)</span> itself. So these marginal effects are associational, not necessarily causal quantities.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./hypothesis_tests.html" class="pagination-link" aria-label="Hypothesis tests">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./least_squares.html" class="pagination-link" aria-label="The mechanics of least squares">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mattblackwell/gov2002-book/edit/main/linear_model.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>