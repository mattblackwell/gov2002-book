<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A User’s Guide to Statistical Inference and Regression - 6&nbsp; The mechanics of least squares</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./08_ols_properties.html" rel="next">
<link href="./06_linear_model.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mattblackwell/gov2002-book/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Inference</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_estimation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hypothesis_tests.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Regression</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_linear_model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_least_squares.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_ols_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#deriving-the-ols-estimator" id="toc-deriving-the-ols-estimator" class="nav-link active" data-scroll-target="#deriving-the-ols-estimator"><span class="toc-section-number">6.1</span>  Deriving the OLS estimator</a></li>
  <li><a href="#model-fit" id="toc-model-fit" class="nav-link" data-scroll-target="#model-fit"><span class="toc-section-number">6.2</span>  Model fit</a></li>
  <li><a href="#matrix-form-of-ols" id="toc-matrix-form-of-ols" class="nav-link" data-scroll-target="#matrix-form-of-ols"><span class="toc-section-number">6.3</span>  Matrix form of OLS</a></li>
  <li><a href="#sec-rank" id="toc-sec-rank" class="nav-link" data-scroll-target="#sec-rank"><span class="toc-section-number">6.4</span>  Rank, linear independence, and multicollinearity</a></li>
  <li><a href="#ols-coefficients-for-binary-and-categorical-regressors" id="toc-ols-coefficients-for-binary-and-categorical-regressors" class="nav-link" data-scroll-target="#ols-coefficients-for-binary-and-categorical-regressors"><span class="toc-section-number">6.5</span>  OLS coefficients for binary and categorical regressors</a></li>
  <li><a href="#projection-and-geometry-of-least-squares" id="toc-projection-and-geometry-of-least-squares" class="nav-link" data-scroll-target="#projection-and-geometry-of-least-squares"><span class="toc-section-number">6.6</span>  Projection and geometry of least squares</a></li>
  <li><a href="#projection-and-annihilator-matrices" id="toc-projection-and-annihilator-matrices" class="nav-link" data-scroll-target="#projection-and-annihilator-matrices"><span class="toc-section-number">6.7</span>  Projection and annihilator matrices</a></li>
  <li><a href="#residual-regression" id="toc-residual-regression" class="nav-link" data-scroll-target="#residual-regression"><span class="toc-section-number">6.8</span>  Residual regression</a></li>
  <li><a href="#outliers-leverage-points-and-influential-observations" id="toc-outliers-leverage-points-and-influential-observations" class="nav-link" data-scroll-target="#outliers-leverage-points-and-influential-observations"><span class="toc-section-number">6.9</span>  Outliers, leverage points, and influential observations</a>
  <ul class="collapse">
  <li><a href="#sec-leverage" id="toc-sec-leverage" class="nav-link" data-scroll-target="#sec-leverage"><span class="toc-section-number">6.9.1</span>  Leverage points</a></li>
  <li><a href="#outliers-and-leave-one-out-regression" id="toc-outliers-and-leave-one-out-regression" class="nav-link" data-scroll-target="#outliers-and-leave-one-out-regression"><span class="toc-section-number">6.9.2</span>  Outliers and leave-one-out regression</a></li>
  <li><a href="#influence-points" id="toc-influence-points" class="nav-link" data-scroll-target="#influence-points"><span class="toc-section-number">6.9.3</span>  Influence points</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mattblackwell/gov2002-book/edit/main/07_least_squares.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\mathbf}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Unif}{\text{Unif}}
\newcommand{\se}{\textsf{se}}
\newcommand{\au}{\underline{a}}
\newcommand{\du}{\underline{d}}
\newcommand{\Au}{\underline{A}}
\newcommand{\Du}{\underline{D}}
\newcommand{\xu}{\underline{x}}
\newcommand{\Xu}{\underline{X}}
\newcommand{\Yu}{\underline{Y}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\U}{\mb{U}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\bbL}{\mathbb{L}}
\renewcommand{\u}{\mb{u}}
\renewcommand{\v}{\mb{v}}
\newcommand{\M}{\mb{M}}
\newcommand{\X}{\mb{X}}
\newcommand{\Xmat}{\mathbb{X}}
\newcommand{\bfx}{\mb{x}}
\newcommand{\y}{\mb{y}}
\renewcommand{\bfbeta}{\bs{\beta}}
\newcommand{\e}{\bs{\epsilon}}
\newcommand{\bhat}{\widehat{\bs{\beta}}}
\newcommand{\XX}{\Xmat'\Xmat}
\newcommand{\XXinv}{\left(\XX\right)^{-1}}
\newcommand{\hatsig}{\hat{\sigma}^2}
\newcommand{\red}[1]{\textcolor{red!60}{#1}}
\newcommand{\indianred}[1]{\textcolor{indianred}{#1}}
\newcommand{\blue}[1]{\textcolor{blue!60}{#1}}
\newcommand{\dblue}[1]{\textcolor{dodgerblue}{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\inprob}{\overset{p}{\to}}
\newcommand{\indist}{\overset{d}{\to}}
\newcommand{\eframe}{\end{frame}}
\newcommand{\bframe}{\begin{frame}}
\newcommand{\R}{\textsf{\textbf{R}}}
\newcommand{\Rst}{\textsf{\textbf{RStudio}}}
\newcommand{\rfun}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\rpack}[1]{\textbf{#1}}
\newcommand{\rexpr}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\filename}[1]{\texttt{\color{blue}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ols-mechanics" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This chapter explores the most widely used estimator for population linear regressions: <strong>ordinary least squares</strong> (OLS). OLS is a plug-in estimator for the best linear projection (or population linear regression) described in the last chapter. Its popularity is partly due to its ease of interpretation, computational simplicity, and statistical efficiency.</p>
<p>In this chapter, we focus on motivating the estimator and the mechanical or algebraic properties of the OLS estimator. In the next chapter, we will investigate its statistical assumptions. Textbooks often introduce OLS under an assumption of a linear model for the conditional expectation, but this is unnecessary if we view the inference target as the best linear predictor. We discuss this point more fully in the next chapter.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ajr-scatter" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="07_least_squares_files/figure-html/fig-ajr-scatter-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.1: Relationship between political institutions and economic development from Acemoglu, Johnson, and Robinson (2001)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="deriving-the-ols-estimator" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="deriving-the-ols-estimator"><span class="header-section-number">6.1</span> Deriving the OLS estimator</h2>
<p>In the last chapter on the linear model and the best linear projection, we operated purely in the population, not samples. We derived the population regression coefficients <span class="math inline">\(\bfbeta\)</span>, representing the coefficients on the line of best fit in the population. We now take these as our quantity of interest.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>The variables <span class="math inline">\(\{(Y_1, \X_1), \ldots, (Y_i,\X_i), \ldots, (Y_n, \X_n)\}\)</span> are i.i.d. draws from a common distribution <span class="math inline">\(F\)</span>.</p>
</div>
</div>
<p>Recall the population linear coefficients (or best linear predictor coefficients) that we derived in the last chapter, <span class="math display">\[
\bfbeta = \argmin_{\mb{b} \in \real^k}\; \E\bigl[ \bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2\bigr] = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]
\]</span></p>
<p>We will consider two different ways to derive the OLS estimator for these coefficients, both of which are versions of the plug-in principle. The first approach is to use the closed-form representation of the coefficients and replace any expectations with sample means, <span class="math display">\[
\bhat = \left(\frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\frac{1}{n} \sum_{i=1}^n \X_{i}Y_{i} \right),
\]</span> which exists if <span class="math inline">\(\sum_{i=1}^n \X_i\X_i'\)</span> is <strong>positive definite</strong> and thus invertible. We will return to this assumption below.</p>
<p>In a simple bivariate linear projection model <span class="math inline">\(m(X_{i}) = \beta_0 + \beta_1X_{i}\)</span>, we saw that the population slope was <span class="math inline">\(\beta_1= \text{cov}(Y_{i},X_{i})/ \V[X_{i}]\)</span> and this approach would have our estimator for the slope be the ratio of the sample covariance of <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> to the sample variance of <span class="math inline">\(X_i\)</span>, or <span class="math display">\[
\widehat{\beta}_{1} = \frac{\widehat{\sigma}_{Y,X}}{\widehat{\sigma}^{2}_{X}} = \frac{ \frac{1}{n-1}\sum_{i=1}^{n} (Y_{i} - \overline{Y})(X_{i} - \overline{X})}{\frac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \Xbar)^{2}}.
\]</span></p>
<p>This plug-in approach is widely applicable and tends to have excellent properties in large samples under iid data. But this approach also hides some of the geometry of the setting.</p>
<p>The second approach applies the plug-in principle not to the closed-form expression for the coefficients but to the optimization problem itself. We call this the <strong>least squares</strong> estimator because it minimizes the empirical (or sample) squared prediction error, <span class="math display">\[
\bhat  = \argmin_{\mb{b} \in \real^k}\; \frac{1}{n} \sum_{i=1}^{n}\bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2 = \argmin_{\mb{b} \in \real^k}\; SSR(\mb{b}),
\]</span> where, <span class="math display">\[
SSR(\mb{b}) = \sum_{i=1}^{n}\bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2
\]</span> is the sum of the squared residuals. To distinguish it from other, more complicated least squares estimators, we call this the <strong>ordinary least squares</strong> estimator or OLS.</p>
<p>Let’s solve this minimization problem! We can write down the first-order conditions as <span class="math display">\[
0=\frac{\partial SSR(\bhat)}{\partial \bfbeta} = 2 \left(\sum_{i=1}^{n} \X_{i}Y_{i}\right) - 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat.
\]</span> We can rearrange this system of equations to <span class="math display">\[
\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat = \left(\sum_{i=1}^{n} \X_{i}Y_{i}\right).
\]</span> To obtain the solution for <span class="math inline">\(\bhat\)</span>, notice that <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is a <span class="math inline">\((k+1) \times (k+1)\)</span> matrix and <span class="math inline">\(\bhat\)</span> and <span class="math inline">\(\sum_{i=1}^{n} \X_{i}Y_{i}\)</span> are both <span class="math inline">\(k+1\)</span> length column vectors. If <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is invertible, then we can multiply both sides of this equation by that inverse to arrive at <span class="math display">\[
\bhat = \left(\sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right),
\]</span> which is the same expression as the plug-in estimator (after canceling the <span class="math inline">\(1/n\)</span> terms). To confirm that we have found a minimum, we also need to check the second-order condition, <span class="math display">\[
\frac{\partial^{2} SSR(\bhat)}{\partial \bfbeta\bfbeta'} = 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right) &gt; 0.
\]</span> What does it mean for a matrix to be “positive”? In matrix algebra, this condition means that the matrix <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is <strong>positive definite</strong>, a condition that we discuss in <a href="#sec-rank"><span>Section&nbsp;6.4</span></a>.</p>
<p>Using the plug-in or least squares approaches, we arrive at the same estimator for the best linear predictor/population linear regression coefficients.</p>
<div id="thm-ols" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1 </strong></span>If the <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is positive definite, then the ordinary least squares estimator is <span class="math display">\[
\bhat = \left(\sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right).
\]</span></p>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Formula for the OLS slopes
</div>
</div>
<div class="callout-body-container callout-body">
<p>Almost all regression will contain an intercept term usually represented as a constant 1 in the covariate vector. It is also possible to separate the intercept to arrive at the set of coefficients on the “real” covariates: <span class="math display">\[
Y_{i} = \alpha + \X_{i}'\bfbeta + \e_{i}
\]</span> Defined this way, we can write the OLS estimator for the “slopes” on <span class="math inline">\(\X_i\)</span> as the OLS estimator with all variables demeaned <span class="math display">\[
\bhat = \left(\frac{1}{n} \sum_{i=1}^{n} (\X_{i} - \overline{\X})(\X_{i} - \overline{\X})'\right) \left(\frac{1}{n} \sum_{i=1}^{n}(\X_{i} - \overline{\X})(Y_{i} - \overline{Y})\right)
\]</span> which is the inverse of the sample covariance matrix of <span class="math inline">\(\X_i\)</span> times the sample covariance of <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(Y_i\)</span>. The intercept is <span class="math display">\[
\widehat{\alpha} = \overline{Y} - \overline{\X}'\bhat.
\]</span></p>
</div>
</div>
<p>When dealing with actual data, we refer to the prediction errors <span class="math inline">\(\widehat{e}_{i} = Y_i - \X_i'\bhat\)</span> as the <strong>residuals</strong> and the predicted value itself, <span class="math inline">\(\widehat{Y}_i = \X_{i}'\bhat\)</span> is also called the <strong>fitted value</strong>. With the population linear regression, we saw that the projection errors <span class="math inline">\(e_i = Y_i - \X_i'\bfbeta\)</span> were mean zero and uncorrelated with the covariates <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span>. The residuals have a similar property with respect to the covariates in the sample: <span class="math display">\[
\sum_{i=1}^n \X_i\widehat{e}_i = 0.
\]</span> The residuals are <em>exactly</em> uncorrelated with the covariates (when the covariates include a constant/intercept term), which is mechanically true of the OLS estimator.</p>
<p><a href="#fig-ssr-comp">Figure&nbsp;<span>6.2</span></a> shows how OLS works in the bivariate case. Here we see three possible regression lines and the sum of the squared residuals for each line. OLS aims to find the line that minimizes the function on the right.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ssr-comp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="07_least_squares_files/figure-html/fig-ssr-comp-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.2: Different possible lines and their corresponding sum of squared residuals.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-fit" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="model-fit"><span class="header-section-number">6.2</span> Model fit</h2>
<p>We have learned how to use OLS to obtain an estimate of the best linear predictor, but we may ask how good that prediction is. Does using <span class="math inline">\(\X_i\)</span> help us predict <span class="math inline">\(Y_i\)</span>? To investigate this, we can consider two different prediction errors: those using covariates and those that do not.</p>
<p>We have already seen the prediction error when using the covariates; it is just the <strong>sum of the squared residuals</strong> <span class="math display">\[
SSR = \sum_{i=1}^n (Y_i - \X_{i}'\bhat)^2.
\]</span> Recall that the best predictor for <span class="math inline">\(Y_i\)</span> without any covariates is simply its sample mean, <span class="math inline">\(\overline{Y}\)</span> and so the prediction error without covariates is what we call the <strong>total sum of squares</strong>, <span class="math display">\[
TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2.
\]</span> <a href="#fig-ssr-vs-tss">Figure&nbsp;<span>6.3</span></a> shows the difference between these two types of prediction errors.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ssr-vs-tss" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="07_least_squares_files/figure-html/fig-ssr-vs-tss-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.3: Total sum of squares vs.&nbsp;the sum of squared residuals</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We can use the <strong>proportion reduction in prediction error</strong> from adding those covariates to measure how much those covariates improve the regression’s predictive ability. This value, called the <strong>coefficient of determination</strong> or <span class="math inline">\(R^2\)</span> is simply <span class="math display">\[
R^2 = \frac{TSS - SSR}{TSS} = 1-\frac{SSR}{TSS},
\]</span> which is the reduction in error moving from <span class="math inline">\(\overline{Y}\)</span> to <span class="math inline">\(\X_i'\bhat\)</span> as the predictor relative to the prediction error using <span class="math inline">\(\overline{Y}\)</span>. We can think of this as the fraction of the total prediction error eliminated by using <span class="math inline">\(\X_i\)</span> to predict <span class="math inline">\(Y_i\)</span>. One thing to note is that OLS will <em>always</em> improve in-sample fit so that <span class="math inline">\(TSS \geq SSR\)</span> even if <span class="math inline">\(\X_i\)</span> is unrelated to <span class="math inline">\(Y_i\)</span>. This phantom improvement occurs because the whole point of OLS is to minimize the SSR, and it will do that even if it is just chasing noise.</p>
<p>Since regression always improves in-sample fit, <span class="math inline">\(R^2\)</span> will fall between 0 and 1. A value 0 zero would indicate exactly 0 estimated coefficients on all covariates (except the intercept) so that <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\X_i\)</span> are perfectly orthogonal in the data (this is very unlikely to occur because there will likely be some minimal but nonzero relationship by random chance). A value of 1 indicates a perfect linear fit.</p>
</section>
<section id="matrix-form-of-ols" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="matrix-form-of-ols"><span class="header-section-number">6.3</span> Matrix form of OLS</h2>
<p>While we derived the OLS estimator above, there is a much more common representation of the estimator that relies on vectors and matrices. We usually write the linear model for a generic unit, <span class="math inline">\(Y_i = \X_i'\bfbeta + e_i\)</span>, but obviously, there are <span class="math inline">\(n\)</span> of these equations, <span class="math display">\[
\begin{aligned}
  Y_1 &amp;= \X_1'\bfbeta + e_1 \\
  Y_2 &amp;= \X_2'\bfbeta + e_2 \\
  &amp;\vdots \\
  Y_n &amp;= \X_n'\bfbeta + e_n \\
\end{aligned}
\]</span> We can write this system of equations in a more compact form using matrix algebra. In particular, let’s combine the variables here into random vectors/matrices: <span class="math display">\[
\mb{Y} = \begin{pmatrix}
Y_1 \\ Y_2 \\ \vdots \\ Y_n
  \end{pmatrix}, \quad
  \mathbb{X} =  \begin{pmatrix}
\X'_1 \\
\X'_2 \\
\vdots \\
\X'_n
  \end{pmatrix} =
  \begin{pmatrix}
    1 &amp; X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1k} \\
    1 &amp; X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2k} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
    1 &amp; X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{nk} \\
  \end{pmatrix},
  \quad
  \mb{e} = \begin{pmatrix}
e_1 \\ e_2 \\ \vdots \\ e_n
  \end{pmatrix}
\]</span> Then we can write the above system of equations as <span class="math display">\[
\mb{Y} = \mathbb{X}\bfbeta + \mb{e},
\]</span> where notice now that <span class="math inline">\(\mathbb{X}\)</span> is a <span class="math inline">\(n \times (k+1)\)</span> matrix and <span class="math inline">\(\bfbeta\)</span> is a <span class="math inline">\(k+1\)</span> length column vector.</p>
<p>A critical link between the definition of OLS above to the matrix notation comes from representing sums in matrix form. In particular, we have <span class="math display">\[
\begin{aligned}
  \sum_{i=1}^n \X_i\X_i' &amp;= \Xmat'\Xmat \\
  \sum_{i=1}^n \X_iY_i &amp;= \Xmat'\mb{Y},
\end{aligned}
\]</span> which means we can write the OLS estimator in the more recognizable form as <span class="math display">\[
\bhat = \left( \mathbb{X}'\mathbb{X} \right)^{-1}  \mathbb{X}'\mb{Y}.
\]</span></p>
<p>Of course, we can also define the vector of residuals, <span class="math display">\[
\widehat{\mb{e}} = \mb{Y} - \mathbb{X}\bhat = \left[
\begin{array}{c}
    Y_1 \\
    Y_2 \\
    \vdots \\
    Y_n
    \end{array}
\right]  -
\left[
\begin{array}{c}
    1\widehat{\beta}_0  + X_{11}\widehat{\beta}_1 +  X_{12}\widehat{\beta}_2 + \dots + X_{1k} \widehat{\beta}_k \\
    1\widehat{\beta}_0 + X_{21}\widehat{\beta}_1 + X_{22}\widehat{\beta}_2  + \dots + X_{2k}\widehat{\beta}_k \\
   \vdots  \\
   1\widehat{\beta}_0 + X_{n1}\widehat{\beta}_1 + X_{n2}\widehat{\beta}_2  + \dots + X_{nk}\widehat{\beta}_k
\end{array}
\right],
\]</span> and so the sum of the squared residuals, in this case, becomes <span class="math display">\[
SSR(\bfbeta) = \Vert\mb{Y} - \mathbb{X}\bfbeta\Vert^{2} =  (\mb{Y} - \mathbb{X}\bfbeta)'(\mb{Y} - \mathbb{X}\bfbeta),
\]</span> where the double vertical lines mean the Euclidean norm of the argument, <span class="math inline">\(\Vert \mb{z} \Vert = \sqrt{\sum_{i=1}^n z_i^{2}}\)</span>. The OLS minimization problem, then, is <span class="math display">\[
\bhat = \argmin_{\mb{b} \in \mathbb{R}^{(k+1)}}\; \Vert\mb{Y} - \mathbb{X}\mb{b}\Vert^{2}
\]</span> Finally, we can write the orthogonality of the covariates and the residuals as <span class="math display">\[
\mathbb{X}'\widehat{\mb{e}} = \sum_{i=1}^{n} \X_{i}\widehat{e}_{i} = 0.
\]</span></p>
</section>
<section id="sec-rank" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-rank"><span class="header-section-number">6.4</span> Rank, linear independence, and multicollinearity</h2>
<p>When introducing the OLS estimator, we noted that it would exist when <span class="math inline">\(\sum_{i=1}^n \X_i\X_i'\)</span> is positive definite or that there is “no multicollinearity.” This assumption is equivalent to saying the matrix <span class="math inline">\(\mathbb{X}\)</span> is full column rank, meaning that <span class="math inline">\(\text{rank}(\mathbb{X}) = (k+1)\)</span>, where <span class="math inline">\(k+1\)</span> is the number of columns of <span class="math inline">\(\mathbb{X}\)</span>. Recall from matrix algebra that the column rank is the number of linearly independent columns in the matrix, and <strong>linear independence</strong> means that if <span class="math inline">\(\mathbb{X}\mb{b} = 0\)</span> if and only if <span class="math inline">\(\mb{b}\)</span> is a column vector of 0s. In other words, we have <span class="math display">\[
b_{1}\mathbb{X}_{1} + b_{2}\mathbb{X}_{2} + \cdots + b_{k+1}\mathbb{X}_{k+1} = 0 \quad\iff\quad b_{1} = b_{2} = \cdots = b_{k+1} = 0,  
\]</span> where <span class="math inline">\(\mathbb{X}_j\)</span> is the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbb{X}\)</span>. Thus, full column rank says that all the columns are linearly independent or that there is no “multicollinearity.”</p>
<p>How could this be violated? Suppose we accidentally included a linear function of one variable so that <span class="math inline">\(\mathbb{X}_2 = 2\mathbb{X}_1\)</span>. Then we have, <span class="math display">\[
\begin{aligned}
  \mathbb{X}\mb{b} &amp;= b_{1}\mathbb{X}_{1} + b_{2}2\mathbb{X}_1+ b_{3}\mathbb{X}_{3}+ \cdots + b_{k+1}\mathbb{X}_{k+1} \\
  &amp;= (b_{1} + 2b_{2})\mathbb{X}_{1} + b_{3}\mathbb{X}_{3} + \cdots + b_{k+1}\mathbb{X}_{k+1}
\end{aligned}
\]</span> In this case, this expression equals 0 when <span class="math inline">\(b_3 = b_4 = \cdots = b_{k+1} = 0\)</span> and <span class="math inline">\(b_1 = -2b_2\)</span>. Thus, the collection of columns is linearly dependent, so we know that the rank of <span class="math inline">\(\mathbb{X}\)</span> must be less than full column rank (that is, less than <span class="math inline">\(k+1\)</span>). Hopefully, it is also clear that if we removed the problematic column <span class="math inline">\(\mathbb{X}_2\)</span>, the resulting matrix would have <span class="math inline">\(k\)</span> linearly independent columns, implying that <span class="math inline">\(\mathbb{X}\)</span> is rank <span class="math inline">\(k\)</span>.</p>
<p>Why does this rank condition matter for the OLS estimator? A key property of full column rank matrices is that <span class="math inline">\(\Xmat\)</span> if of full column rank if and only if <span class="math inline">\(\Xmat'\Xmat\)</span> is non-singular and a matrix is invertible if and only if it is non-singular. Thus, the columns of <span class="math inline">\(\Xmat\)</span> being linearly independent means that the inverse <span class="math inline">\((\Xmat'\Xmat)^{-1}\)</span> exists and so does <span class="math inline">\(\bhat\)</span>. Further, this full rank condition also implies that <span class="math inline">\(\Xmat'\Xmat = \sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is positive definite, implying that the estimator is truly finding the minimal sum of squared residuals.</p>
<p>What are common situations that lead to violations of no multicollinearity? We have seen one above, with one variable being a linear function of another. But this problem can come out in more subtle ways. Suppose that we have a set of dummy variables corresponding to a single categorical variable, like the region of the country. In the US, this might mean we have <span class="math inline">\(X_{i1} = 1\)</span> for units in the West (0 otherwise), <span class="math inline">\(X_{i2} = 1\)</span> for units in the Midwest (0 otherwise), <span class="math inline">\(X_{i3} = 1\)</span> for units in the South (0 otherwise), and <span class="math inline">\(X_{i4} = 1\)</span> for units in the Northeast (0 otherwise). Each unit has to be in one of these four regions, so there is a linear dependence between these variables, <span class="math display">\[
X_{i4} = 1 - X_{i1} - X_{i2} - X_{i3}.
\]</span> That is, if I know that you are not in the West, Midwest, or South regions, I know that you are in the Northeast. We would get a linear dependence if we tried to include all of these variables in our regression with an intercept. (Note the 1 in the relationship between <span class="math inline">\(X_{i4}\)</span> and the other variables, that’s why there will be linear dependence when including a constant.) Thus, we usually omit one dummy variable from each categorical variable. In that case, the coefficients on the remaining dummies are differences in means between that category and the omitted one (perhaps conditional on other variables included, if included). So if we omitted <span class="math inline">\(X_{i4}\)</span>, then the coefficient on <span class="math inline">\(X_{i1}\)</span> would be the difference in mean outcomes between units in the West and Northeast regions.</p>
<p>Another way collinearity can occur is if you include both an intercept term and a variable that does not vary. This issue can often happen if we mistakenly subset our data to, say, the West region but still include the West dummy variable in the regression.</p>
<p>Finally, note that most statistical software packages will “solve” the multicollinearity by arbitrarily removing as many linearly dependent covariates as is necessary to achieve full rank. R will show the estimated coefficients as <code>NA</code> in those cases.</p>
</section>
<section id="ols-coefficients-for-binary-and-categorical-regressors" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="ols-coefficients-for-binary-and-categorical-regressors"><span class="header-section-number">6.5</span> OLS coefficients for binary and categorical regressors</h2>
<p>Suppose that the covariates include just the intercept and a single binary variable, <span class="math inline">\(\X_i = (1\; X_{i})'\)</span>, where <span class="math inline">\(X_i \in \{0,1\}\)</span>. In this case, the OLS coefficient on <span class="math inline">\(X_i\)</span>, <span class="math inline">\(\widehat{\beta_{1}}\)</span>, is exactly equal to the difference in sample means of <span class="math inline">\(Y_i\)</span> in the <span class="math inline">\(X_i = 1\)</span> group and the <span class="math inline">\(X_i = 0\)</span> group: <span class="math display">\[
\widehat{\beta}_{1} = \frac{\sum_{i=1}^{n} X_{i}Y_{i}}{\sum_{i=1}^{n} X_{i}} - \frac{\sum_{i=1}^{n} (1 - X_{i})Y_{i}}{\sum_{i=1}^{n} 1- X_{i}} = \overline{Y}_{X =1} - \overline{Y}_{X=0}
\]</span> This result is not an approximation. It holds exactly for any sample size.</p>
<p>We can generalize this idea to discrete variables more broadly. Suppose we have our region variables from the last section and include in our covariates a constant and the dummies for the West, Midwest, and South regions. Then coefficient on the West dummy will be <span class="math display">\[
\widehat{\beta}_{\text{west}} = \overline{Y}_{\text{west}} - \overline{Y}_{\text{northeast}},
\]</span> which is exactly the difference in sample means of <span class="math inline">\(Y_i\)</span> between the West region and units in the “omitted region,” the Northeast.</p>
<p>Note that these interpretations only hold when the regression consists solely of the binary variable or the set of categorical dummy variables. These exact relationships fail when other covariates are added to the model.</p>
</section>
<section id="projection-and-geometry-of-least-squares" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="projection-and-geometry-of-least-squares"><span class="header-section-number">6.6</span> Projection and geometry of least squares</h2>
<p>OLS has a very nice geometric interpretation that can add a lot of intuition for various aspects of the method. In this geometric approach, we view <span class="math inline">\(\mb{Y}\)</span> as an <span class="math inline">\(n\)</span>-dimesnional vector in <span class="math inline">\(\mathbb{R}^n\)</span>. As we saw above, OLS in matrix form is about finding a linear combination of the covariate matrix <span class="math inline">\(\Xmat\)</span> closest to this vector in terms of the Euclidean distance (which is just the sum of squares).</p>
<p>Let <span class="math inline">\(\mathcal{C}(\Xmat) = \{\Xmat\mb{b} : \mb{b} \in \mathbb{R}^2\}\)</span> be the <strong>column space</strong> of the matrix <span class="math inline">\(\Xmat\)</span>. This set is all linear combinations of the columns of <span class="math inline">\(\Xmat\)</span> or the set of all possible linear predictions we could obtain from <span class="math inline">\(\Xmat\)</span>. Notice that the OLS fitted values, <span class="math inline">\(\Xmat\bhat\)</span>, is in this column space. If, as we assume, <span class="math inline">\(\Xmat\)</span> has full column rank of <span class="math inline">\(k+1\)</span>, then the column space <span class="math inline">\(\mathcal{C}(\Xmat)\)</span> will be a <span class="math inline">\(k+1\)</span>-dimensional surface inside of the larger <span class="math inline">\(n\)</span>-dimensional space. If <span class="math inline">\(\Xmat\)</span> has two columns, the column space will be a plane.</p>
<p>Another interpretation of the OLS estimator is that it finds the linear predictor as the closest point in the column space of <span class="math inline">\(\Xmat\)</span> to the outcome vector <span class="math inline">\(\mb{Y}\)</span>. This is called the <strong>projection</strong> of <span class="math inline">\(\mb{Y}\)</span> onto <span class="math inline">\(\mathcal{C}(\Xmat)\)</span>. <a href="#fig-projection">Figure&nbsp;<span>6.4</span></a> shows this projection for a case with <span class="math inline">\(n=3\)</span> and 2 columns in <span class="math inline">\(\Xmat\)</span>. The shaded blue region represents the plane of the column space of <span class="math inline">\(\Xmat\)</span>, and we can see that <span class="math inline">\(\Xmat\bhat\)</span> is the closest point to <span class="math inline">\(\mb{Y}\)</span> in that space. That’s the whole idea of the OLS estimator: find the linear combination of the columns of <span class="math inline">\(\Xmat\)</span> (a point in the column space) that minimizes the Euclidean distance between that point and the outcome vector (the sum of squared residuals).</p>
<div id="fig-projection" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="assets/img/projection-drawing.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.4: Projection of Y on the column space of the covariates</figcaption><p></p>
</figure>
</div>
<p>This figure shows that the residual vector, which is the difference between the <span class="math inline">\(\mb{Y}\)</span> vector and the projection <span class="math inline">\(\Xmat\bhat\)</span> is perpendicular or orthogonal to the column space of <span class="math inline">\(\Xmat\)</span>. This orthogonality is a consequence of the residuals being orthogonal to all the columns of <span class="math inline">\(\Xmat\)</span>, <span class="math display">\[
\Xmat'\mb{e} = 0,
\]</span> as we established above. Being orthogonal to all the columns means it will also be orthogonal to all linear combinations of the columns.</p>
</section>
<section id="projection-and-annihilator-matrices" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="projection-and-annihilator-matrices"><span class="header-section-number">6.7</span> Projection and annihilator matrices</h2>
<p>Now that we have the idea of projection to the column space of <span class="math inline">\(\Xmat\)</span>, we can define a way to project any vector into that space. The <span class="math inline">\(n\times n\)</span> <strong>projection matrix</strong> <span class="math display">\[
\mb{P}_{\Xmat} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat',
\]</span> projects a vector into <span class="math inline">\(\mathcal{C}(\Xmat)\)</span>. In particular, we can see that this gives us the fitted values for <span class="math inline">\(\mb{Y}\)</span>: <span class="math display">\[
\mb{P}_{\Xmat}\mb{Y} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\mb{Y} = \Xmat\bhat.
\]</span> Because we sometimes write the linear predictor as <span class="math inline">\(\widehat{\mb{Y}} = \Xmat\bhat\)</span>, the projection matrix is also called the <strong>hat matrix</strong>. With either name, multiplying a vector by <span class="math inline">\(\mb{P}_{\Xmat}\)</span> gives the best linear predictor of that vector as a function of <span class="math inline">\(\Xmat\)</span>. Intuitively, any vector that is already a linear combination of the columns of <span class="math inline">\(\Xmat\)</span> (so is in <span class="math inline">\(\mathcal{C}(\Xmat)\)</span>) should be unaffected by this projection: the closest point in <span class="math inline">\(\mathcal{C}(\Xmat)\)</span> to a point already in <span class="math inline">\(\mathcal{C}(\Xmat)\)</span> is itself. We can also see this algebraically for any linear combination <span class="math inline">\(\Xmat\mb{c}\)</span> <span class="math display">\[
\mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\Xmat\mb{c} = \Xmat\mb{c}
\]</span> because <span class="math inline">\((\Xmat'\Xmat)^{-1} \Xmat'\Xmat\)</span> simplifies to the identity matrix. In particular, the projection of <span class="math inline">\(\Xmat\)</span> onto itself is just itself: <span class="math inline">\(\mb{P}_{\Xmat}\Xmat = \Xmat\)</span>.</p>
<p>The second matrix related to projection is the <strong>annihilator matrix</strong>, <span class="math display">\[
\mb{M}_{\Xmat} = \mb{I}_{n} - \mb{P}_{\Xmat},
\]</span> which projects any vector into the orthogonal complement to the column space of <span class="math inline">\(\Xmat\)</span>, <span class="math display">\[
\mathcal{C}^{\perp}(\Xmat) = \{\mb{c} \in \mathbb{R}^n\;:\; \Xmat\mb{c} = 0 \},
\]</span> This matrix is called the annihilator matrix because if you apply it to any linear combination of <span class="math inline">\(\Xmat\)</span>, you get 0: <span class="math display">\[
\mb{M}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c} - \mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c} - \Xmat\mb{c} = 0,
\]</span> and in particular, <span class="math inline">\(\mb{M}_{\Xmat}\Xmat = 0\)</span>. Why should we care about this matrix? Perhaps a more evocative name might be the <strong>residual maker</strong> since it makes residuals when applied to <span class="math inline">\(\mb{Y}\)</span>, <span class="math display">\[
\mb{M}_{\Xmat}\mb{Y} = (\mb{I}_{n} - \mb{P}_{\Xmat})\mb{Y} = \mb{Y} - \mb{P}_{\Xmat}\mb{Y} = \mb{Y} - \Xmat\bhat = \widehat{\mb{e}}.
\]</span></p>
<p>There are several fundamental property properties of the projection matrix that are useful:</p>
<ul>
<li><p><span class="math inline">\(\mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}\)</span> are <strong>idempotent</strong>, which means that when applied to itself, it simply returns itself: <span class="math inline">\(\mb{P}_{\Xmat}\mb{P}_{\Xmat} = \mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}\mb{M}_{\Xmat} = \mb{M}_{\Xmat}\)</span>.</p></li>
<li><p><span class="math inline">\(\mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}\)</span> are symmetric <span class="math inline">\(n \times n\)</span> matrices so that <span class="math inline">\(\mb{P}_{\Xmat}' = \mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}' = \mb{M}_{\Xmat}\)</span>.</p></li>
<li><p>The rank of <span class="math inline">\(\mb{P}_{\Xmat}\)</span> is <span class="math inline">\(k+1\)</span> (the number of columns of <span class="math inline">\(\Xmat\)</span>) and the rank of <span class="math inline">\(\mb{M}_{\Xmat}\)</span> is <span class="math inline">\(n - k - 1\)</span>.</p></li>
</ul>
<p>We can use the projection and annihilator matrices to arrive at an orthogonal decomposition of the outcome vector: <span class="math display">\[
\mb{Y} = \Xmat\bhat + \widehat{\mb{e}} = \mb{P}_{\Xmat}\mb{Y} + \mb{M}_{\Xmat}\mb{Y}.
\]</span></p>
</section>
<section id="residual-regression" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="residual-regression"><span class="header-section-number">6.8</span> Residual regression</h2>
<p>There are many situations where we can partition the covariates into two groups, and we might wonder if it is possible how to express or calculate the OLS coefficients for just one set of covariates. In particular, let the columns of <span class="math inline">\(\Xmat\)</span> be partitioned into <span class="math inline">\([\Xmat_{1} \Xmat_{2}]\)</span>, so that linear prediction we are estimating is <span class="math display">\[
\mb{Y} = \Xmat_{1}\bfbeta_{1} + \Xmat_{2}\bfbeta_{2} + \mb{e},
\]</span> with estimated coefficients and residuals <span class="math display">\[
\mb{Y} = \Xmat_{1}\bhat_{1} + \Xmat_{2}\bhat_{2} + \widehat{\mb{e}}.
\]</span></p>
<p>We now document another way to obtain the estimator <span class="math inline">\(\bhat_1\)</span> from this regression using a technique called <strong>residual regression </strong>, <strong>partitioned regression</strong>, or the <strong>Frisch-Waugh-Lovell theorem</strong>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Residual regression approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>The residual regression approach is:</p>
<ol type="1">
<li>Use OLS to regress <span class="math inline">\(\mb{Y}\)</span> on <span class="math inline">\(\Xmat_2\)</span> and obtain residuals <span class="math inline">\(\widetilde{\mb{e}}_2\)</span>.</li>
<li>Use OLS to regress each column of <span class="math inline">\(\Xmat_1\)</span> on <span class="math inline">\(\Xmat_2\)</span> and obtain residuals <span class="math inline">\(\widetilde{\Xmat}_1\)</span>.</li>
<li>Use OLS to regression <span class="math inline">\(\widetilde{\mb{e}}_{2}\)</span> on <span class="math inline">\(\widetilde{\Xmat}_1\)</span>.</li>
</ol>
</div>
</div>
<div id="thm-fwl" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2 (Frisch-Waugh-Lovell) </strong></span>The OLS coefficients from a regression of <span class="math inline">\(\widetilde{\mb{e}}_{2}\)</span> on <span class="math inline">\(\widetilde{\Xmat}_1\)</span> are equivalent to the coefficients on <span class="math inline">\(\Xmat_{1}\)</span> from the regression of <span class="math inline">\(\mb{Y}\)</span> on both <span class="math inline">\(\Xmat_{1}\)</span> and <span class="math inline">\(\Xmat_2\)</span>.</p>
</div>
<p>One implication of this theorem is the regression coefficient for a given variable captures the relationship between the residual variation in the outcome and that variable after accounting for the other covariates. In particular, this coefficient focuses on the variation orthogonal to those other covariates.</p>
<p>While perhaps unexpected, this result may not appear particularly useful. We can just run the long regression, right? This trick can be handy when <span class="math inline">\(\Xmat_2\)</span> consists of dummy variables (or “fixed effects”) for a categorical variable with many categories. For example, suppose <span class="math inline">\(\Xmat_2\)</span> consists of indicators for the county of residence for a respondent. In that case, that will have over 3,000 columns, meaning that direct calculation of the <span class="math inline">\(\bhat = (\bhat_{1}, \bhat_{2})\)</span> will require inverting a matrix that is bigger than <span class="math inline">\(3,000 \times 3,000\)</span>. Computationally, this process will be very slow. But above, we saw that predictions of an outcome on a categorical variable are just the sample mean within each level of the variable. Thus, in this case, the residuals <span class="math inline">\(\widetilde{\mb{e}}_2\)</span> and <span class="math inline">\(\Xmat_1\)</span> can be computed by demeaning the outcome and <span class="math inline">\(\Xmat_1\)</span> within levels of the dummies in <span class="math inline">\(\Xmat_2\)</span>, which can be considerably faster computationally.</p>
<p>Finally, there are data visualization reasons to use residual regression. It is often difficult to see if the linear functional form for some covariate is appropriate once you begin to control for other variables. One can check the relationship using this approach with a scatterplot of <span class="math inline">\(\widetilde{\mb{e}}_2\)</span> on <span class="math inline">\(\Xmat_1\)</span> (when it is a single column).</p>
</section>
<section id="outliers-leverage-points-and-influential-observations" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="outliers-leverage-points-and-influential-observations"><span class="header-section-number">6.9</span> Outliers, leverage points, and influential observations</h2>
<p>Given that OLS finds the coefficients that minimize the sum of the squared residuals, it is helpful to ask how much impact each residual has on that solution. Let <span class="math inline">\(\bhat_{(-i)}\)</span> be the OLS estimates if we omit unit <span class="math inline">\(i\)</span>. Intuitively, <strong>influential observations</strong> should significantly impact the estimated coefficients so that <span class="math inline">\(\bhat_{(-i)} - \bhat\)</span> is large in absolute value.</p>
<p>Under what conditions will we have influential observations? OLS tries to minimize the sum of <strong>squared</strong> residuals, so it will move more to shrink larger residuals than smaller ones. Where are large residuals likely to occur? Well, notice that any OLS regression line with a constant will go through the means of the outcome and the covariates: <span class="math inline">\(\overline{Y} = \overline{\X}\bhat\)</span>. Thus, by definition, This means that when an observation is close to the average of the covariates, <span class="math inline">\(\overline{\X}\)</span>, it cannot have that much influence because OLS forces the regression line to go through <span class="math inline">\(\overline{Y}\)</span>. Thus, we should look for influential points that have two properties:</p>
<ol type="1">
<li>Have high <strong>leverage</strong>, where leverage roughly measures how far <span class="math inline">\(\X_i\)</span> is from <span class="math inline">\(\overline{\X}\)</span>, and</li>
<li>Be an <strong>outlier</strong> in the sense of having a large residual (if left out of the regression).</li>
</ol>
<p>We’ll take each of these in turn.</p>
<section id="sec-leverage" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="sec-leverage"><span class="header-section-number">6.9.1</span> Leverage points</h3>
<p>We can define the <strong>leverage</strong> of an observation by <span class="math display">\[
h_{ii} = \X_{i}'\left(\Xmat'\Xmat\right)^{-1}\X_{i},
\]</span> which is the <span class="math inline">\(i\)</span>th diagonal entry of the projection matrix, <span class="math inline">\(\mb{P}_{\Xmat}\)</span>. Notice that <span class="math display">\[
\widehat{\mb{Y}} = \mb{P}\mb{Y} \qquad \implies \qquad \widehat{Y}_i = \sum_{j=1}^n h_{ij}Y_j,
\]</span> so that <span class="math inline">\(h_{ij}\)</span> is the importance of observation <span class="math inline">\(j\)</span> for the fitted value for observation <span class="math inline">\(i\)</span>. The leverage, then, is the importance of the observation for its own fitted value. We can also interpret these values in terms of the distribution of <span class="math inline">\(\X_{i}\)</span>. Roughly speaking, these values are the weighted distance <span class="math inline">\(\X_i\)</span> is from <span class="math inline">\(\overline{\X}\)</span>, where the weights normalize to the empirical variance/covariance structure of the covariates (so that the scale of each covariate is roughly the same). We can see this most clearly when we fit a simple linear regression (with one covariate and an intercept) with OLS when the leverage is <span class="math display">\[
h_{ii} = \frac{1}{n} + \frac{(X_i - \overline{X})^2}{\sum_{j=1}^n (X_j - \overline{X})^2}
\]</span></p>
<p>Leverage values have three key properties:</p>
<ol type="1">
<li><span class="math inline">\(0 \leq h_{ii} \leq 1\)</span></li>
<li><span class="math inline">\(h_{ii} \geq 1/n\)</span> if the model contains an intercept</li>
<li><span class="math inline">\(\sum_{i=1}^{n} h_{ii} = k + 1\)</span></li>
</ol>
</section>
<section id="outliers-and-leave-one-out-regression" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="outliers-and-leave-one-out-regression"><span class="header-section-number">6.9.2</span> Outliers and leave-one-out regression</h3>
<p>In the context of OLS, an <strong>outlier</strong> is an observation with a large prediction error for a particular OLS specification. <a href="#fig-outlier">Figure&nbsp;<span>6.5</span></a> shows an example of an outlier.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-outlier" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="07_least_squares_files/figure-html/fig-outlier-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.5: An example of an outlier</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Intuitively, it seems as though we could use the residual <span class="math inline">\(\widehat{e}_i\)</span> to assess the prediction error for a given unit. But the residuals are not valid predictions because the OLS estimator is designed to make those as small as possible (in machine learning parlance, these were in the training set). In particular, if an outlier is influential, we already noted that it might “pull” the regression line toward it, and the resulting residual might be pretty small.</p>
<p>To assess prediction errors more cleanly, we can use <strong>leave-one-out regression</strong> (LOO), which regresses<span class="math inline">\(\mb{Y}_{(-i)}\)</span> on <span class="math inline">\(\Xmat_{(-i)}\)</span>, where these omit unit <span class="math inline">\(i\)</span>: <span class="math display">\[
\bhat_{(-i)} = \left(\Xmat'_{(-i)}\Xmat_{(-i)}\right)^{-1}\Xmat_{(-i)}\mb{Y}_{(-i)}.
\]</span> We can then calculate LOO prediction errors as <span class="math display">\[
\widetilde{e}_{i} = Y_{i} - \X_{i}'\bhat_{(-i)}.
\]</span> Calculating these LOO prediction errors for each unit appears to be computationally costly because it seems as though we have to fit OLS <span class="math inline">\(n\)</span> times. Fortunately, there is a closed-form expression for the LOO coefficients and prediction errors in terms of the original regression, <span id="eq-loo-coefs"><span class="math display">\[
\bhat_{(-i)} = \bhat - \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i \qquad \widetilde{e}_i = \frac{\widehat{e}_i}{1 - h_{ii}}.
\tag{6.1}\]</span></span> We can see from this that the LOO prediction errors will differ from the residuals when the leverage of a unit is high. This makes sense! We said earlier that observations with low leverage would be close to <span class="math inline">\(\overline{\X}\)</span>, where the outcome values have relatively little impact on the OLS fit (because the regression line must go through <span class="math inline">\(\overline{Y}\)</span>).</p>
</section>
<section id="influence-points" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="influence-points"><span class="header-section-number">6.9.3</span> Influence points</h3>
<p>An influence point is an observation that has the power to change the coefficients and fitted values for a particular OLS specification. <a href="#fig-influence">Figure&nbsp;<span>6.6</span></a> shows an example of such an influence point.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-influence" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="07_least_squares_files/figure-html/fig-influence-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.6: An example of an influence point</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>One measure of influence is called DFBETA<span class="math inline">\(_i\)</span> measures how much <span class="math inline">\(i\)</span> changes the estimated coefficient vector <span class="math display">\[
\bhat - \bhat_{(-i)} = \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i,
\]</span> so there is one value for each observation-covariate pair. When divided by the standard error of the estimated coefficients, this is called DFBETA<strong>S</strong> (where the “S” is for standardized). These are helpful if we focus on a particular coefficient.</p>
<p>When we want to summarize how much an observation matters for the fit, we can use a compact measure of the influence of an observation by comparing the fitted value from the entire sample to the fitted value from the leave-one-out regression. Using the DFBETA above, we have <span class="math display">\[
\widehat{Y}_i - \X_{i}\bhat_{(-1)} = \X_{i}'(\bhat -\bhat_{(-1)}) = \X_{i}'\left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i = h_{ii}\widetilde{e}_i,
\]</span> so the influence of an observation is its leverage times how much of an outlier it is. This value is sometimes called DFFIT (difference in fit). One transformation of this quantity, <strong>Cook’s distance</strong>, standardizes this by the sum of the squared residuals: <span class="math display">\[
D_i = \frac{n-k-1}{k+1}\frac{h_{ii}\widetilde{e}_{i}^{2}}{\widehat{\mb{e}}'\widehat{\mb{e}}}.
\]</span> Various rules exist for establishing cutoffs for identifying an observation as “influential” based on these metrics, but they tend to be ad hoc. In any case, it’s better to focus on the holistic question of “how much does this observation matter for my substantive interpretation” rather than the narrow question of a particular threshold.</p>
<p>It’s all well and good to find influential points, but what should you do about it? The first thing to check is that the data is not corrupted somehow. Sometimes influence points occur because of a coding or data entry error. If you have control over that coding, you should fix those errors. You may consider removing the observation if the error appears in the data acquired from another source. Still, when writing up your analyses, you should be extremely clear about this choice. Another approach is to consider a transformation of the dependent or independent variables, like the natural logarithm, that might dampen the effects of outliers. Finally, consider using methods that are robust to outliers.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./06_linear_model.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./08_ols_properties.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>