<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A User’s Guide to Statistical Inference and Regression - 7&nbsp; The statistics of least squares</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./07_least_squares.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mattblackwell/gov2002-book/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Stastisitcal Inference</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_estimation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hypothesis_tests.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Regression</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_linear_model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_least_squares.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_ols_properties.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#large-sample-properties-of-ols" id="toc-large-sample-properties-of-ols" class="nav-link active" data-scroll-target="#large-sample-properties-of-ols"><span class="toc-section-number">7.1</span>  Large-sample properties of OLS</a></li>
  <li><a href="#variance-estimation-for-ols" id="toc-variance-estimation-for-ols" class="nav-link" data-scroll-target="#variance-estimation-for-ols"><span class="toc-section-number">7.2</span>  Variance estimation for OLS</a></li>
  <li><a href="#inference-for-multiple-parameters" id="toc-inference-for-multiple-parameters" class="nav-link" data-scroll-target="#inference-for-multiple-parameters"><span class="toc-section-number">7.3</span>  Inference for multiple parameters</a></li>
  <li><a href="#finite-sample-properties-with-a-linear-cef" id="toc-finite-sample-properties-with-a-linear-cef" class="nav-link" data-scroll-target="#finite-sample-properties-with-a-linear-cef"><span class="toc-section-number">7.4</span>  Finite-sample properties with a linear CEF</a>
  <ul class="collapse">
  <li><a href="#linear-cef-model-under-homoskedasticity" id="toc-linear-cef-model-under-homoskedasticity" class="nav-link" data-scroll-target="#linear-cef-model-under-homoskedasticity"><span class="toc-section-number">7.4.1</span>  Linear CEF model under homoskedasticity</a></li>
  </ul></li>
  <li><a href="#the-normal-linear-model" id="toc-the-normal-linear-model" class="nav-link" data-scroll-target="#the-normal-linear-model"><span class="toc-section-number">7.5</span>  The normal linear model</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mattblackwell/gov2002-book/edit/main/08_ols_properties.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\mathbf}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Unif}{\text{Unif}}
\newcommand{\se}{\textsf{se}}
\newcommand{\au}{\underline{a}}
\newcommand{\du}{\underline{d}}
\newcommand{\Au}{\underline{A}}
\newcommand{\Du}{\underline{D}}
\newcommand{\xu}{\underline{x}}
\newcommand{\Xu}{\underline{X}}
\newcommand{\Yu}{\underline{Y}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\U}{\mb{U}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\bbL}{\mathbb{L}}
\renewcommand{\u}{\mb{u}}
\renewcommand{\v}{\mb{v}}
\newcommand{\M}{\mb{M}}
\newcommand{\X}{\mb{X}}
\newcommand{\Xmat}{\mathbb{X}}
\newcommand{\bfx}{\mb{x}}
\newcommand{\y}{\mb{y}}
\renewcommand{\bfbeta}{\bs{\beta}}
\newcommand{\e}{\bs{\epsilon}}
\newcommand{\bhat}{\widehat{\bs{\beta}}}
\newcommand{\XX}{\Xmat'\Xmat}
\newcommand{\XXinv}{\left(\XX\right)^{-1}}
\newcommand{\hatsig}{\hat{\sigma}^2}
\newcommand{\red}[1]{\textcolor{red!60}{#1}}
\newcommand{\indianred}[1]{\textcolor{indianred}{#1}}
\newcommand{\blue}[1]{\textcolor{blue!60}{#1}}
\newcommand{\dblue}[1]{\textcolor{dodgerblue}{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\inprob}{\overset{p}{\to}}
\newcommand{\indist}{\overset{d}{\to}}
\newcommand{\eframe}{\end{frame}}
\newcommand{\bframe}{\begin{frame}}
\newcommand{\R}{\textsf{\textbf{R}}}
\newcommand{\Rst}{\textsf{\textbf{RStudio}}}
\newcommand{\rfun}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\rpack}[1]{\textbf{#1}}
\newcommand{\rexpr}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\filename}[1]{\texttt{\color{blue}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In the last chapter, we derived the least squares estimators and investigated many of its mechanical properties. These properties are important for the practical application of OLS, but we should also understand its statistical properties as well, such as the ones described in Part I: unbiasedness, sampling variance, consistency, asymptotic normality. As we saw then, these properties fall into finite-sample (unbiasedness, sampling variance) and asymptotic (consistency, asymptotic normality).</p>
<p>In this chapter, we will focus first on the asymptotic properties of OLS because those properties hold under the fairly mild conditions of the linear projection model introduced in <a href="06_linear_model.html#sec-linear-projection"><span>Section&nbsp;5.2</span></a>. We will see that OLS consistently estimates a coherent quantity of interest (the best linear predictor) regardless of whether the conditional expectation is linear. That is, for the asymptotic properties of the estimator, we will not need the commonly invoked linearity assumption. Later, when we investigate the finite-sample properties, we will show how linearity will help us establish unbiasedness and how normality of the errors can allow us to conduct exact, finite-sample inference. But these assumptions are very strong and so it’s important to understand what we can say about OLS without making them.</p>
<section id="large-sample-properties-of-ols" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="large-sample-properties-of-ols"><span class="header-section-number">7.1</span> Large-sample properties of OLS</h2>
<p>We begin by setting out the assumptions we will need for establishing the large-sample properties of OLS, which are the same as the assumptions needed to ensure that the best linear predictor, <span class="math inline">\(\bhat = \E[\X_{i}\X_{i}']^{-1}\E[\X_{i}Y_{i}]\)</span>, is well-defined and unique.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Linear projection assumptions
</div>
</div>
<div class="callout-body-container callout-body">
<p>The linear projection model makes the follow assumptions:</p>
<ol type="1">
<li><p><span class="math inline">\(\{(Y_{i}, \X_{i})\}_{i=1}^n\)</span> are iid random vectors.</p></li>
<li><p><span class="math inline">\(\E[Y_{i^{2}}] &lt; \infty\)</span> (finite outcome variance)</p></li>
<li><p><span class="math inline">\(\E[\Vert \X_{i}\Vert^{2}] &lt; \infty\)</span> (finite variances and covariances of covariates)</p></li>
<li><p><span class="math inline">\(\E[\X_{i}\X_{i}']\)</span> is positive definite (no linear dependence in the covariates)</p></li>
</ol>
</div>
</div>
<p>Recall that these are mild conditions on the joint distribution of <span class="math inline">\((Y_{i}, \X_{i})\)</span> and in particular, we are <strong>not</strong> assuming linearity of the CEF, <span class="math inline">\(\E[Y_{i} \mid \X_{i}]\)</span>, nor are we assuming any specific distribution for the data.</p>
<p>We can helpfully decompose the OLS estimator into the true BLP coefficient plus estimation error as <span class="math display">\[
\bhat = \left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n \X_iY_i \right) = \bfbeta +  \underbrace{\left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n \X_ie_i \right)}_{\text{estimation error}}.
\]</span></p>
<p>This decomposition will help us quickly establish the consistency of <span class="math inline">\(\bhat\)</span>. By the law of large numbers, we know that sample means will converge in probability to population expectations, so we have <span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \X_i\X_i' \inprob \E[\X_i\X_i'] \equiv \mb{Q}_{\X\X} \qquad \frac{1}{n} \sum_{i=1}^n \X_ie_i \inprob \E[\X_{i} e_{i}] = \mb{0},
\]</span> which implies that <span class="math display">\[
\bhat \inprob \beta +  \mb{Q}_{\X\X}^{-1}\E[\X_ie_i] = \beta,
\]</span> by the continuous mapping theorem (the inverse is a continuous function). The linear projection assumptions ensure that LLN applies to these sample means and to ensure that <span class="math inline">\(\E[\X_{i}\X_{i}']\)</span> is invertible.</p>
<div id="thm-ols-consistency" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.1 </strong></span>Under the above linear projection assumptions, the OLS estimator is consistent for the best linear projection coefficients, <span class="math inline">\(\bhat \inprob \bfbeta\)</span>.</p>
</div>
<p>Thus, under fairly mild conditions, OLS should be close to the population linear regression in large samples. Remember, though, that this might not be equal to the conditional expectation if the CEF is nonlinear. What we can say here is that OLS does converge to the best <em>linear</em> approximation to the CEF. Of course, this also means that if the CEF is linear then OLS will consistently estimate the coefficients of the CEF.</p>
<p>Just to emphasize here: the only assumption we made about the dependent variable is that it has finite variance and that it’s iid. Under this assumption it could be continuous, categorical, binary, or event count.</p>
<p>Next, we would like to establish an asymptotic normality result for the OLS coefficients. We first review some key ideas about the central limit theorem.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
CLT reminder
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose that we have a function of the data iid random vectors <span class="math inline">\(\X_1, \ldots, \X_n\)</span>, <span class="math inline">\(g(\X_{i})\)</span> where <span class="math inline">\(\E[g(\X_{i})] = 0\)</span> and so <span class="math inline">\(\V[g(\X_{i})] = \E[g(\X_{i})g(\X_{i})']\)</span>. Then if <span class="math inline">\(\E[\Vert g(\X_{i})\Vert^{2}] &lt; \infty\)</span>, the CLT implies that <span id="eq-clt-mean-zero"><span class="math display">\[
\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} g(\X_{i}) - \E[g(\X_{i})]\right) = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} g(\X_{i}) \indist \N(0, \E[g(\X_{i})g(\X_{i}')])
\tag{7.1}\]</span></span></p>
</div>
</div>
<p>We now manipulate our decomposition to arrive at the <em>stabilized</em> version of the estimator, <span class="math display">\[
\sqrt{n}\left( \bhat - \bfbeta\right) =  \left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{\sqrt{n}} \sum_{i=1}^n \X_ie_i \right).
\]</span> We have already established that the first term on the right-hand side will converge in probability to <span class="math inline">\(\mb{Q}_{\X\X}^{-1}\)</span>. Notice that <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span>, so we can apply <a href="#eq-clt-mean-zero">Equation&nbsp;<span>7.1</span></a> to the second term. The covariance matrix of <span class="math inline">\(\X_ie_{i}\)</span> is <span class="math display">\[
\mb{\Omega} = \V[\X_{i}e_{i}] = \E[\X_{i}e_{i}(\X_{i}e_{i})'] = \E[e_{i}^{2}\X_{i}\X_{i}'].
\]</span> The CLT will imply that <span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n \X_ie_i \indist \N(0, \mb{\Omega}).
\]</span> Combining these facts with Slutsky’s Theorem implies the following theorem.</p>
<div id="thm-ols-asymptotic-normality" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.2 </strong></span>Suppose that the linear projection assumptions hold and, in addition, we have <span class="math inline">\(\E[Y_{i}^{4}] &lt; \infty\)</span> and <span class="math inline">\(\E[\lVert\X_{i}\rVert^{4}] &lt; \infty\)</span>. Then the OLS estimator is asymptotically normal with <span class="math display">\[
\sqrt{n}\left( \bhat - \bfbeta\right) \indist \N(0, \mb{V}_{\bfbeta}),
\]</span> where <span class="math display">\[
\mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1} = \left( \E[\X_i\X_i'] \right)^{-1}\E[e_i^2\X_i\X_i']\left( \E[\X_i\X_i'] \right)^{-1}.
\]</span></p>
</div>
<p>This means that if the sample size is large enough, we can approximate the distribution of <span class="math inline">\(\bhat\)</span> with a multivariate normal with mean <span class="math inline">\(\bfbeta\)</span> and covariance matrix <span class="math inline">\(\mb{V}_{\bfbeta}/n\)</span>. In particular, the square root of the <span class="math inline">\(j\)</span>th diagonals of this matrix will be standard errors for <span class="math inline">\(\widehat{\beta}_j\)</span>. Knowing the shape of mutlivariate distribution of the OLS estimator is going to allow us to conduct hypothesis tests and generate confidence intervals for both individual coefficients and groups of coefficients. But first, we need an estimate of the covariance matrix!</p>
</section>
<section id="variance-estimation-for-ols" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="variance-estimation-for-ols"><span class="header-section-number">7.2</span> Variance estimation for OLS</h2>
<p>The asymptotic normality of OLS from the last section is of limited value without some way to estimate the covariance matrix, <span class="math display">\[
\mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1}.
\]</span> Given that each of these are population means, this is an ideal place to drop a plug-in estimator. In particular, let’s use the following estimators: <span class="math display">\[
\begin{aligned}
  \mb{Q}_{\X\X} &amp;= \E[\X_{i}\X_{i}'] &amp; \widehat{\mb{Q}}_{\X\X} &amp;= \frac{1}{n} \sum_{i=1}^{n} \X_{i}\X_{i}' = \frac{1}{n}\Xmat'\Xmat \\
  \mb{\Omega} &amp;= \E[e_i^2\X_i\X_i'] &amp; \widehat{\mb{\Omega}} &amp; = \frac{1}{n}\sum_{i=1}^n\widehat{e}_i^2\X_i\X_i'.
\end{aligned}
\]</span> Under the assumptions of <a href="#thm-ols-asymptotic-normality">Theorem&nbsp;<span>7.2</span></a>, the LLN will imply that these are consistent for their targets, <span class="math inline">\(\widehat{\mb{Q}}_{\X\X} \inprob \mb{Q}_{\X\X}\)</span> and <span class="math inline">\(\widehat{\mb{\Omega}} \inprob \mb{\Omega}\)</span>. We can plug these into the variance formula to arrive at <span class="math display">\[
\begin{aligned}
  \widehat{\mb{V}}_{\bfbeta} &amp;= \widehat{\mb{Q}}_{\X\X}^{-1}\widehat{\mb{\Omega}}\widehat{\mb{Q}}_{\X\X}^{-1} \\
  &amp;= \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i' \right)  \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1},
\end{aligned}
\]</span> which by the continuous mapping theorem is consistent, <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta} \inprob \mb{V}_{\bfbeta}\)</span>.</p>
<p>This estimator is sometimes called the <strong>robust variance estimator</strong> or, more accurately, the <strong>heteroskedasticity-consistent (HC) variance estimator</strong>. How is this robust? Consider the standard <strong>homoskedasticity</strong> assumption that most statistical software packages make when estimating OLS variances: the variance of the errors does not depend on the covariates: <span class="math inline">\(\V[e_{i}^{2} \mid \X_{i}] = \V[e_{i}^{2}]\)</span>. This is actually stronger than we need and we can rely on a weaker assumption that the squared errors are uncorrelated with a certain function of the covariates: <span class="math display">\[
\E[e_{i}^{2}\X_{i}\X_{i}'] = \E[e_{i}^{2}]\E[\X_{i}\X_{i}'] = \sigma^{2}\mb{Q}_{\X\X},
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the variance of the residuals (since <span class="math inline">\(\E[e_{i}] = 0\)</span>). Homoskedasticity simplifies the asymptotic variance of the stabilized estimator, <span class="math inline">\(\sqrt{n}(\bhat - \bfbeta)\)</span>, to <span class="math display">\[
\mb{V}^{\texttt{lm}}_{\bhat} = \mb{Q}_{\X\X}^{-1}\sigma^{2}\mb{Q}_{\X\X}\mb{Q}_{\X\X}^{-1} = \sigma^2\mb{Q}_{\X\X}^{-1}.
\]</span> We already have an estimator for <span class="math inline">\(\mb{Q}_{\X\X}\)</span>, but we need one for <span class="math inline">\(\sigma^2\)</span>. We can easily use the SSR, <span class="math display">\[
\widehat{\sigma}^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n} \widehat{e}_{i}^{2},
\]</span> where we use <span class="math inline">\(n-k-1\)</span> in the denominator instead of <span class="math inline">\(n\)</span> to correct for the residuals being slightly less variable than the true errors (because OLS mechanically attempts to make the residuals small). For consitent variance estimation, <span class="math inline">\(n-k -1\)</span> or <span class="math inline">\(n\)</span> can both be used, since either way <span class="math inline">\(\widehat{\sigma}^2 \inprob \sigma^2\)</span>. This means that, under homoskedasticity, we have <span class="math display">\[
\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}} = \widehat{\sigma}^{2}\left(\Xmat'\Xmat\right)^{{-1}},
\]</span> which is the standard variance estimator used by <code>lm()</code> in R or <code>reg</code> in Stata.</p>
<p>Now that we have two estimators, <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}\)</span> and <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)</span>, how do they compare? Notice that the HC variance estimator and the homoskedasticity variance estimator will both be consistent when homoskedasticity holds. But like the “heteroskedasticity-consistent” label implies, only the HC variance estimator will be consistent when homoskedasticity fails to hold. So <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}\)</span> has the advantage of being consistent regardless of this assumption. There is a price to be paid, however. When homoskedasticity is correct, <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)</span> incorporates that assumption into the estimator where the HC variance estimator has to estimate it. This means that the HC estimator will have higher variance (the variance estimator will be more variable!) when homoskedasticity actually does hold.</p>
<!-- TODO: add discussion of King and Roberts -->
<p>Now that we have established the asymptotic normality of the OLS estimator and developed a consistent estimator of its variance, we can proceed with all of the statistical inference tools we discussed in Part I of this guide. Define the estimated <strong>heteroskedasticity-consistent standard errors</strong> as <span class="math display">\[
\widehat{\se}(\widehat{\beta}_{j}) = \sqrt{\frac{[\widehat{\mb{V}}_{\bfbeta}]_{jj}}{n}},
\]</span> where <span class="math inline">\([\widehat{\mb{V}}_{\bfbeta}]_{jj}\)</span> is the <span class="math inline">\(j\)</span>th diagonal entry of the HC variance estimator. Note that we divide by <span class="math inline">\(\sqrt{n}\)</span> here because <span class="math inline">\(\widehat{\mb{V}}_{\bfbeta}\)</span> is a consistent estimator of the stabilized estimator <span class="math inline">\(\sqrt{n}(\bhat - \bfbeta)\)</span> not the estimator itself.</p>
<p>Hypothesis tests and confidence intervals for individual coefficients are almost exactly the same as with the general case presented in Part I. For a two-sided test of <span class="math inline">\(H_0: \beta_j = b\)</span> versus <span class="math inline">\(H_1: \beta_j \neq b\)</span>, we can build the t-statistic and conclude that, under the null, <span class="math display">\[
\frac{\widehat{\beta}_j - b}{\widehat{\se}(\widehat{\beta}_{j})} \indist \N(0, 1).
\]</span> Typically, statistical software will helpfully provide the t-statistic for the null of no (partial) linear relationship between <span class="math inline">\(X_{ij}\)</span> and <span class="math inline">\(Y_i\)</span>, <span class="math display">\[
t = \frac{\widehat{\beta}_{j}}{\widehat{\se}(\widehat{\beta}_{j})},
\]</span> which measures how large the estimated coefficient is in standard errors. With a level of <span class="math inline">\(\alpha = 0.05\)</span>, asymptotic normality would imply that we reject this null when <span class="math inline">\(t &gt; 1.96\)</span>. We can form asymptotically-valid confidence intervals with <span class="math display">\[
\left[\widehat{\beta}_{j} - z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j}),\;\widehat{\beta}_{j} + z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j})\right].
\]</span> For reasons that we will discuss below, standard software typically relies on the <span class="math inline">\(t\)</span> distribution instead of the normal for both hypothesis testing and confidence intervals, but this difference is of little consequence in large samples.</p>
</section>
<section id="inference-for-multiple-parameters" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="inference-for-multiple-parameters"><span class="header-section-number">7.3</span> Inference for multiple parameters</h2>
<p>With multiple coefficients, we might have hypotheses that involve more than one coefficient. As an example, let’s focus on a regression with an interaction between two covariates, <span class="math display">\[
Y_i = \beta_0 + X_i\beta_2 + Z_i\beta_2 + X_iZ_i\beta_3 + e_i.
\]</span> Suppose that we wanted to test the hypothesis that <span class="math inline">\(X_i\)</span> has no effect on the best linear predictor for <span class="math inline">\(Y_i\)</span>. That would be <span class="math display">\[
H_{0}: \beta_{1} = 0 \text{ and } \beta_{3} = 0\quad\text{vs}\quad H_{1}: \beta_{1} \neq 0 \text{ or } \beta_{3} \neq 0,
\]</span> where we usually write the null more compactly as <span class="math inline">\(H_0: \beta_1 = \beta_3 = 0\)</span>.</p>
<p>To test this null hypothesis, we need a test statistic that discriminates the two hypotheses: it should be large when the alternative is true and small when the null is true. With a single coefficient we usually test the null hypothesis of <span class="math inline">\(H_0: \beta_j = b_0\)</span> with the <span class="math inline">\(t\)</span>-statistic, <span class="math display">\[
t = \frac{\widehat{\beta}_{j} - b_{0}}{\widehat{\se}(\widehat{\beta}_{j})},
\]</span> and we usually take the absolute value, <span class="math inline">\(|t|\)</span>, as our measure of how far our estimate is from the null. But notice that we could also use the square of the <span class="math inline">\(t\)</span> statistic, which is <span id="eq-squared-t"><span class="math display">\[
t^{2} = \frac{\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{\V[\widehat{\beta}_{j}]} = \frac{n\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{[\mb{V}_{\bfbeta}]_[jj]}
\tag{7.2}\]</span></span></p>
<p>So here’s another way to differentiate the null from the alternative: the squared distance between them divided by the variance of the estimate.</p>
<p>Can we generalize this idea to hypotheses about multiple parameters? It is straightforward to, say, add the sum of squared distances for each component of the null hypothesis. For our interaction example, that would be <span class="math display">\[
\widehat{\beta}_1^2 + \widehat{\beta}_3^2,
\]</span> but remember that some of the estimated coefficients are noisier than others so we should account for the uncertainty, just like we did for the <span class="math inline">\(t\)</span>-statistic.</p>
<p>With multiple parameters and multiple coefficients, the variances will now require the use of matrix algebra. We can write any hypothesis about linear functions of the coefficients as <span class="math inline">\(H_{0}: \mb{L}\bfbeta = \mb{c}\)</span>. For example, in the interaction case, we have <span class="math display">\[
\mb{L} =
\begin{pmatrix}
  0 &amp; 1 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 1 \\
\end{pmatrix}
\qquad
\mb{c} =
\begin{pmatrix}
  0 \\
  0
\end{pmatrix}
\]</span> Thus, <span class="math inline">\(\mb{L}\bfbeta = \mb{0}\)</span> is equivalent to <span class="math inline">\(\beta_1 = 0\)</span> and <span class="math inline">\(\beta_3 = 0\)</span>. Notice that with different <span class="math inline">\(\mb{L}\)</span> matrices we could represent more complicated hypotheses like <span class="math inline">\(2\beta_1 - \beta_2 = 34\)</span>, though we mostly stick to simpler functions. Let <span class="math inline">\(\widehat{\bs{\theta}} = \mb{L}\bhat\)</span> be the OLS estimate of the function of the coefficients. By the delta method (discussed in <a href="03_asymptotics.html#sec-delta-method"><span>Section&nbsp;3.9</span></a>), we have <span class="math display">\[
\sqrt{n}\left(\mb{L}\bhat - \mb{L}\bfbeta\right) \indist \N(0, \mb{L}'\mb{V}_{\bfbeta}\mb{L}).
\]</span> Using this, we can now create our generalization of the squared <span class="math inline">\(t\)</span> statistic in <a href="#eq-squared-t">Equation&nbsp;<span>7.2</span></a>. In particular, we will take the distances <span class="math inline">\(\mb{L}\bhat - \mb{c}\)</span> weighted by the variance-covariance matrix <span class="math inline">\(\mb{L}'\mb{V}_{\bfbeta}\mb{L}\)</span>, <span class="math display">\[
W = n(\mb{L}\bhat - \mb{c})'(\mb{L}'\mb{V}_{\bfbeta}\mb{L})^{-1}(\mb{L}\bhat - \mb{c}),
\]</span> which is called the <strong>Wald test statistic</strong>. This statistic generalizes the ideas of the t-statistic to multiple parameters. With the t-statistic, we recenter to have mean 0 and divide by the standard error to get a variance of 1. If we ignore the middle variance weighting, we have <span class="math inline">\((\mb{L}\bhat - \mb{c})'(\mb{L}\bhat - \mb{c})\)</span> which is just the sum of the squared deviations of the estimates from the null. Including the <span class="math inline">\((\mb{L}'\mb{V}_{\bfbeta}\mb{L})^{-1}\)</span> weight has the effect of rescaling the distribution of <span class="math inline">\(\mb{L}\bhat - \mb{c}\)</span> to make it rotationally symmetric around 0 (so the resulting dimensions are uncorrelated) with each dimension having equal variance of 1. In this way, the Wald statistic transforms the random vectors to be mean-centered and have variance 1 (just the t-statistic), but also to have the resulting random variables in the vector be uncorrelated.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Why transform the data in this way? <a href="#fig-wald">Figure&nbsp;<span>7.1</span></a> shows the contour plot of a hypothetical joint distribution of two coefficients from an OLS regression. We might want to know how far different points in the distribution are from the mean, which in this case is <span class="math inline">\((1, 2)\)</span>. Without taking into consideration the joint distribution, the circle is obviously closer to the mean that the triangle. However, looking at where the two points are on the distribution, the circle is at a lower contour than the triangle, meaning it is more extreme than the triangle for this particular distribution. The Wald statistic, then, takes into consideration how much of a “climb” it is for <span class="math inline">\(\mb{L}\bhat\)</span> to get to <span class="math inline">\(\mb{c}\)</span> given the the distribution of <span class="math inline">\(\mb{L}\bhat\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-wald" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="08_ols_properties_files/figure-html/fig-wald-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7.1: Hypothetical joint distribution of two slope coefficients. The circle is closer to the center of the distribution by the standard Euclidean distance, but the triangle is closer once you consider the joint distribution.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>If <span class="math inline">\(\mb{L}\)</span> only has one row, then our Wald statistic is the same as the squared <span class="math inline">\(t\)</span> statistic, <span class="math inline">\(W = t^2\)</span>. This will help us think about the asymptotic distribution of <span class="math inline">\(W\)</span>. Notice that as <span class="math inline">\(n\to\infty\)</span>, we know that by the asymptotic normality of <span class="math inline">\(\bhat\)</span>, <span class="math display">\[
t = \frac{\widehat{\beta}_{j} - \beta_{j}}{\widehat{\se}[\widehat{\beta}_{j}]} \indist \N(0,1)
\]</span> so <span class="math inline">\(t^2\)</span> will converge in distribution to a <span class="math inline">\(\chi^2_1\)</span> (since a <span class="math inline">\(\chi^2_1\)</span> is just one standard normal squared). After recentering ad rescaling by the covariance matrix, <span class="math inline">\(W\)</span> converges to the sum of <span class="math inline">\(q\)</span> squared independent normals, where <span class="math inline">\(q\)</span> is the number of rows of <span class="math inline">\(\mb{L}\)</span>, or equivalently, the number of restrictions implied by the null hypothesis. Thus, under the null hypothesis of <span class="math inline">\(\mb{L}\bhat = \mb{c}\)</span>, we have <span class="math inline">\(W \indist \chi^2_{q}\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Chi-squared critical values
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can obtain critical values for the <span class="math inline">\(\chi^2_q\)</span> distribution using the <code>qchisq()</code> function in R. For example, if we wanted to obtain the critical value <span class="math inline">\(w\)</span> that such that <span class="math inline">\(\P(W &gt; w_{\alpha}) = \alpha\)</span> for our two parameter interaction example, we could use:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="at">p =</span> <span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.991465</code></pre>
</div>
</div>
</div>
</div>
<p>To use the Wald statistic in a test, we need to define the rejection region. Because we are squaring each distance in <span class="math inline">\(W \geq 0\)</span> and larger values of <span class="math inline">\(W\)</span> indicate more disagreement with the null in either direction. Thus, for an <span class="math inline">\(\alpha\)</span>-level test of the joint null, we only need a one-sided rejection region of the form <span class="math inline">\(\P(W &gt; w_{\alpha}) = \alpha\)</span>. Obtaining these values is straightforward (see the above callout tip). For <span class="math inline">\(q = 2\)</span> and a <span class="math inline">\(\alpha = 0.05\)</span>, the critical value is roughly 6.</p>
<p>The Wald statistic is not a common test provided by standard statistical software functions like <code>lm()</code> in R, though it is fairly straightforward to implement “by hand.” Alternatively, packages like <a href="https://cran.r-project.org/web/packages/aod/index.html"><code>{aod}</code></a> or <a href="http://jepusto.github.io/clubSandwich/"><code>{clubSandwich}</code></a> have implementations of the test. What is reported by most software implementations of OLS (like <code>lm()</code> in R) is the F-statistic, which is <span class="math display">\[
F = \frac{W}{q},
\]</span> which also typically uses the the homoskedastic variance estimator <span class="math inline">\(\mb{V}^{\texttt{lm}}_{\bfbeta}\)</span> in <span class="math inline">\(W\)</span>. The p-values reported for such tests use the <span class="math inline">\(F_{q,n-k-1}\)</span> distribution because this is the exact distribution of the <span class="math inline">\(F\)</span> statistic when the errors are (a) homoskedastic and (b) normally distributed. When these assumptions do not hold, the <span class="math inline">\(F\)</span> distribution is not really statistically justified, but it is slightly more conservative than the <span class="math inline">\(\chi^2_q\)</span> distribution and the inference will converge as <span class="math inline">\(n\to\infty\)</span>. So it might be justified as an <em>ad hoc</em> small sample adjustment to the Wald test. For example, if we used the <span class="math inline">\(F_{q,n-k-1}\)</span> with the interaction example where <span class="math inline">\(q=2\)</span> and say we have a sample size of <span class="math inline">\(n = 100\)</span>. In that case the critical value for the F test with <span class="math inline">\(\alpha = 0.05\)</span> is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qf</span>(<span class="fl">0.95</span>, <span class="at">df1 =</span> <span class="dv">2</span>, <span class="at">df2 =</span> <span class="dv">100</span> <span class="sc">-</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.091191</code></pre>
</div>
</div>
<p>which equates to a critical value of 6.182 on the scale of the Wald statistic (multiplying it by <span class="math inline">\(q = 2\)</span>). Compared to the earlier critical value of 5.991 based on the <span class="math inline">\(\chi^2_2\)</span> distribution, we can see that the inferences will be very similar even in moderately-sized datasets.</p>
<p>Finally, note that the F-statistic reported by <code>lm()</code> in R is the test of all the coefficients except the intercept being 0. In modern quantitative social sciences, this is almost never a substantively interesting test.</p>
</section>
<section id="finite-sample-properties-with-a-linear-cef" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="finite-sample-properties-with-a-linear-cef"><span class="header-section-number">7.4</span> Finite-sample properties with a linear CEF</h2>
<p>All of the results above have been large-sample properties and we have not addressed finite-sample properties like the sampling variance or unbiasedness. Actually, under the linear projection assumption above, OLS is generally biased without stronger assumptions. In this section, we introduce the stronger assumption that will allow us to establish stronger properties for OLS. As usual, however, remember that these stronger assumption can be wrong.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumption: Linear Regression Model
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>The variables <span class="math inline">\((Y_{i}, \X_{i})\)</span> satisfy the linear CEF assumption. <span class="math display">\[
\begin{aligned}
  Y_{i} &amp;= \X_{i}'\bfbeta + e_{i} \\
  \E[e_{i}\mid \X_{i}] &amp; = 0.
\end{aligned}
\]</span></p></li>
<li><p>The design matrix is invertible <span class="math inline">\(\E[\X_{i}\X_{i}'] &gt; 0\)</span> (positive definite).</p></li>
</ol>
</div>
</div>
<p>We discussed the concept of a linear CEF extensively in <a href="06_linear_model.html"><span>Chapter&nbsp;5</span></a>, but recall that the CEF might be linear mechanically if the model is <strong>saturated</strong>, or when there are as many coefficients in the model as there are unique values of <span class="math inline">\(\X_i\)</span>. When a model is not saturated, the linear CEF assumption is just that: an assumption. What can this assumption do? It can actually establish quite a few nice statistical properties in finite samples.</p>
<p>One note before we proceed. When focusing on the finite sample inference for OLS, it is customary to focus on its properties <strong>conditional on the observed covariates</strong>, such as <span class="math inline">\(\E[\bhat \mid \Xmat]\)</span> or <span class="math inline">\(\V[\bhat \mid \Xmat]\)</span>. The historical reason for this was because these independent variable were often chosen by the researcher and not really random at all. Thus, in some older texts, you’ll sometimes see <span class="math inline">\(\Xmat\)</span> treated as “fixed” and might even omit any explicit conditioning statements.</p>
<div id="thm-ols-unbiased" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.3 </strong></span>Under the linear regression model assumption, OLS is unbiased for the population regression coefficients, <span class="math display">\[
\E[\bhat \mid \Xmat] = \bfbeta,
\]</span> and its conditional sampling variance issue <span class="math display">\[
\mb{\V}_{\bhat} = \V[\bhat \mid \Xmat] = \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1},
\]</span> where <span class="math inline">\(\sigma^2_{i} = \E[e_{i}^{2} \mid \Xmat]\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove the conditional unbiasedness, recall that we can write the OLS estimator as <span class="math display">\[
\bhat = \bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\mb{e},
\]</span> and so taking (conditional) expectations, we have, <span class="math display">\[
\E[\bhat \mid \Xmat] = \bfbeta + \E[(\Xmat'\Xmat)^{-1}\Xmat'\mb{e} \mid \Xmat] = \bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\E[\mb{e} \mid \Xmat] = \bfbeta,
\]</span> because under the linear CEF assumption <span class="math inline">\(\E[\mb{e}\mid \Xmat] = 0\)</span>.</p>
<p>For the conditional sampling variance, we can use the same decomposition we have, <span class="math display">\[
\V[\bhat \mid \Xmat] = \V[\bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\mb{e} \mid \Xmat] = (\Xmat'\Xmat)^{-1}\Xmat'\V[\mb{e} \mid \Xmat]\Xmat(\Xmat'\Xmat)^{-1}.
\]</span> Since <span class="math inline">\(\E[\mb{e}\mid \Xmat] = 0\)</span>, we know that <span class="math inline">\(\V[\mb{e}\mid \Xmat] = \E[\mb{ee}' \mid \Xmat]\)</span>, which is a matrix with diagonal entries <span class="math inline">\(\E[e_{i}^{2} \mid \Xmat] = \sigma^2_i\)</span> and off-diagonal entries <span class="math inline">\(\E[e_{i}e_{j} \Xmat] = \E[e_{i}\mid \Xmat]\E[e_{j}\mid\Xmat] = 0\)</span>, where the first equality follows from the independence of the errors across units. Thus, <span class="math inline">\(\V[\mb{e} \mid \Xmat]\)</span> is a diagonal matrix with <span class="math inline">\(\sigma^2_i\)</span> along the diagonal, which means <span class="math display">\[
\Xmat'\V[\mb{e} \mid \Xmat]\Xmat = \sum_{i=1}^n \sigma^2_i \X_i\X_i',
\]</span> establishing the conditional sampling variance.</p>
</div>
<p>Thus, for any realization of the covariates, <span class="math inline">\(\Xmat\)</span>, OLS is unbiased for the true regression coefficients <span class="math inline">\(\bfbeta\)</span>. By the law of iterated expectation, we also know that it is unconditionally unbiased<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> as well since <span class="math display">\[
\E[\bhat] = \E[\E[\bhat \mid \Xmat]] = \bfbeta.
\]</span> The difference between these two statements usually isn’t incredibly meaningful.</p>
<p>There are a lot of variances flying around, so it’s helpful to review them. Above, we derived the asymptotic variance of <span class="math inline">\(\mb{Z}_{n} = \sqrt{n}(\bhat - \bfbeta)\)</span>, <span class="math display">\[
\mb{V}_{\bfbeta} = \left( \E[\X_i\X_i'] \right)^{-1}\E[e_i^2\X_i\X_i']\left( \E[\X_i\X_i'] \right)^{-1},
\]</span> which implies that the approximate variance of <span class="math inline">\(\bhat\)</span> will be <span class="math inline">\(\mb{V}_{\bfbeta} / n\)</span> because <span class="math display">\[
\bhat = \frac{Z_n}{\sqrt{n}} + \bfbeta \quad\implies\quad \bhat \overset{a}{\sim} \N(\bfbeta, n^{-1}\mb{V}_{\bfbeta}),
\]</span> where <span class="math inline">\(\overset{a}{\sim}\)</span> means approximately asymptotically distributed as. Under the linear CEF, the conditional sampling variance of <span class="math inline">\(\bhat\)</span> has a similar form and will be similar to the<br>
<span class="math display">\[
\mb{V}_{\bhat} = \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \approx \mb{V}_{\bfbeta} / n
\]</span> In practice, these two derivations lead to basically the same variance estimator. Recall the heteroskedastic-consistent variance estimator is <span class="math display">\[
\widehat{\mb{V}}_{\bfbeta} = \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i' \right)  \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1},
\]</span> is basically a valid plug-in estimator for the asymptotic variance and <span class="math display">\[
\widehat{\mb{V}}_{\bhat} = n^{-1}\widehat{\mb{V}}_{\bfbeta}.
\]</span> Thus, in practice, the asymptotic results and the finite-sample results under a linear CEF both justify the same variance estimator.</p>
<section id="linear-cef-model-under-homoskedasticity" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="linear-cef-model-under-homoskedasticity"><span class="header-section-number">7.4.1</span> Linear CEF model under homoskedasticity</h3>
<p>If we are willing to make a homoskedasticity assumption on the errors, we can derive even stronger results for OLS. This makese sense: stronger assumptions typically lead to stronger conclusions, but those conclusions may not be robust to violations of the assumption. But homoskedasticity is such a historically important assumption that statistical software implementations of OLS like <code>lm()</code> in R assume it.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumption: Homoskedasticity with a linear CEF
</div>
</div>
<div class="callout-body-container callout-body">
<p>In addition the linear CEF assumption, we further assume that <span class="math display">\[
\E[e_i^2 \mid \X_i] = \E[e_i^2] = \sigma^2,
\]</span> or that variance of the errors does not depend on the covariates.</p>
</div>
</div>
<div id="thm-homoskedasticity" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.4 </strong></span>Under a linear CEF model with homoskedastic errors, the conditional sampling variance is <span class="math display">\[
\mb{V}^{\texttt{lm}}_{\bhat} = \V[\bhat \mid \Xmat] = \sigma^2 \left( \Xmat'\Xmat \right)^{-1},
\]</span> and the variance estimator <span class="math display">\[
\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} = \widehat{\sigma}^2 \left( \Xmat'\Xmat \right)^{-1} \quad\text{where,}\quad \widehat{\sigma}^2 = \frac{1}{n - k - 1} \sum_{i=1}^n \widehat{e}_i^2
\]</span> is unbiased, <span class="math inline">\(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat] = \mb{V}^{\texttt{lm}}_{\bhat}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Under homoskedasticity <span class="math inline">\(\sigma^2_i = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. Recall that <span class="math inline">\(\sum_{i=1}^n \X_i\X_i' = \Xmat'\Xmat\)</span> Thus, the conditional sampling variance from <a href="#thm-ols-unbiased">Theorem&nbsp;<span>7.3</span></a>, <span class="math display">\[
\begin{aligned}
\V[\bhat \mid \Xmat] &amp;= \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2 \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \\ &amp;= \sigma^2\left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \\&amp;= \sigma^2\left( \Xmat'\Xmat \right)^{-1}\left( \Xmat'\Xmat \right) \left( \Xmat'\Xmat \right)^{-1} \\&amp;= \sigma^2\left( \Xmat'\Xmat \right)^{-1} = \mb{V}^{\texttt{lm}}_{\bhat}.
\end{aligned}
\]</span></p>
<p>For the unbiasedness, we just need to show that <span class="math inline">\(\E[\widehat{\sigma}^{2} \mid \Xmat] = \sigma^2\)</span>. Recall that we defined <span class="math inline">\(\mb{M}_{\Xmat}\)</span> as the residual-maker because <span class="math inline">\(\mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}}\)</span>. We can use this to connect the residuals to the errors, <span class="math display">\[
\mb{M}_{\Xmat}\mb{e} = \mb{M}_{\Xmat}\mb{Y} - \mb{M}_{\Xmat}\Xmat\bfbeta = \mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}},
\]</span> so <span class="math display">\[
\V[\widehat{\mb{e}} \mid \Xmat] = \mb{M}_{\Xmat}\V[\mb{e} \mid \Xmat] = \mb{M}_{\Xmat}\sigma^2,
\]</span> where the first equality is because <span class="math inline">\(\mb{M}_{\Xmat} = \mb{I}_{n} - \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\)</span> is constant conditional on <span class="math inline">\(\Xmat\)</span>. Notice that the diagonal entries of this matrix are the variances of particular residuals <span class="math inline">\(\widehat{e}_i\)</span> and that the diagonal entries of the annihilator matrix are <span class="math inline">\(1 - h_{ii}\)</span> (since the <span class="math inline">\(h_{ii}\)</span> are the diagonal entries of <span class="math inline">\(\mb{P}_{\Xmat}\)</span>). Thus, we have <span class="math display">\[
\V[\widehat{e}_i \mid \Xmat] = \E[\widehat{e}_{i}^{2} \mid \Xmat] = (1 - h_{ii})\sigma^{2}.
\]</span> In the last chapter we established one property of these leverage values in <a href="07_least_squares.html#sec-leverage"><span>Section&nbsp;6.9.1</span></a> is that <span class="math inline">\(\sum_{i=1}^n h_{ii} = k+ 1\)</span>, so <span class="math inline">\(\sum_{i=1}^n 1- h_{ii} = n - k - 1\)</span> and we have <span class="math display">\[
\begin{aligned}
  \E[\widehat{\sigma}^{2} \mid \Xmat] &amp;= \frac{1}{n-k-1} \sum_{i=1}^{n} \E[\widehat{e}_{i}^{2} \mid \Xmat] \\
                                      &amp;= \frac{\sigma^{2}}{n-k-1} \sum_{i=1}^{n} 1 - h_{ii} \\
                                      &amp;= \sigma^{2}
\end{aligned}
\]</span> This establishes <span class="math inline">\(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat] = \mb{V}^{\texttt{lm}}_{\bhat}\)</span>.</p>
</div>
<p>Thus, under the linear CEF model and homoskedasticity of the errors, we have an unbiased variance estimator that is a simple function of the sum of squared residuals and the design matrix. In most statistical software packages, estimated standard errors are derived from <span class="math inline">\(\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}\)</span>.</p>
<p>The final result that we can derive for the linear CEF under homoskedasticity is an optimality result. We might ask ourselves if there is another estimator for <span class="math inline">\(\bfbeta\)</span> that would outperform OLS in the sense of having lower sampling variance. Turns out there is no linear estimator for <span class="math inline">\(\bfbeta\)</span> that has lower conditional variance, meaning that OLS is the <strong>best linear unbiased estimator</strong>, often jovially shortened to BLUE. This result is famously known as the Gauss-Markov Theorem.</p>
<div id="thm-gauss-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.5 </strong></span>Let <span class="math inline">\(\widetilde{\bfbeta} = \mb{AY}\)</span> be a linear and unbiased estimator for <span class="math inline">\(\bfbeta\)</span>. Under the linear CEF model with homoskedastic errors, <span class="math display">\[
\V[\widetilde{\bfbeta}\mid \Xmat] \geq \V[\bhat \mid \Xmat].
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note that if <span class="math inline">\(\widetilde{\bfbeta}\)</span> is unbiased then <span class="math inline">\(\E[\widetilde{\bfbeta} \mid \Xmat] = \bfbeta\)</span> and so <span class="math display">\[
\bfbeta = \E[\mb{AY} \mid \Xmat] = \mb{A}\E[\mb{Y} \mid \Xmat] = \mb{A}\Xmat\bfbeta,
\]</span> which implies that <span class="math inline">\(\mb{A}\Xmat = \mb{I}_n\)</span>. Rewrite the competitor as <span class="math inline">\(\widetilde{\bfbeta} = \bhat + \mb{BY}\)</span> where, <span class="math display">\[
\mb{B} = \mb{A} - \left(\Xmat'\Xmat\right)^{-1}\Xmat'.
\]</span> and note that <span class="math inline">\(\mb{A}\Xmat = \mb{I}_n\)</span> implies that <span class="math inline">\(\mb{B}\Xmat = 0\)</span>. We now we have <span class="math display">\[
\begin{aligned}
  \widetilde{\bfbeta} &amp;= \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{Y} \\
                      &amp;= \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\Xmat\bfbeta + \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{e} \\
                      &amp;= \bfbeta + \mb{B}\Xmat\bfbeta + \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{e} \\
  &amp;= \bfbeta + \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\mb{e}
\end{aligned}
\]</span> The variance of the competitor is thus, <span class="math display">\[
\begin{aligned}
  \V[\widetilde{\bfbeta} \mid \Xmat]
  &amp;= \left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\V[\mb{e}\mid \Xmat]\left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)' \\
  &amp;= \sigma^{2}\left( \left(\Xmat'\Xmat\right)^{-1}\Xmat' + \mb{B}\right)\left( \Xmat\left(\Xmat'\Xmat\right)^{-1} + \mb{B}'\right) \\
  &amp;= \sigma^{2}\left(\left(\Xmat'\Xmat\right)^{-1}\Xmat'\Xmat\left(\Xmat'\Xmat\right)^{-1} + \left(\Xmat'\Xmat\right)^{-1}\Xmat'\mb{B}' + \mb{B}\Xmat\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\\
  &amp;= \sigma^{2}\left(\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\\
  &amp;\geq \sigma^{2}\left(\Xmat'\Xmat\right)^{-1} \\
  &amp;= \V[\bhat \mid \Xmat]
\end{aligned}
\]</span> The first equality comes from the properties of covariance matrics; the second equality is due to homoskedasticity; the fourth is due <span class="math inline">\(\mb{B}\Xmat = 0\)</span> which implies that <span class="math inline">\(\Xmat'\mb{B}' = 0\)</span> as well. The fifth inequality holds because matrix products of the form <span class="math inline">\(\mb{BB}'\)</span> are positive definite if <span class="math inline">\(\mb{B}\)</span> is of full rank (which we have assumed it is).</p>
</div>
<p>In this proof, we saw that the variance of the competing estimator had variance <span class="math inline">\(\sigma^2\left(\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\)</span> which we argued was “greater than 0” in the matrix sense, which is also called positive definite. What does this mean practically? Remember that any positive definite matrix must have strictly positive diagonal entries and that the diagonal entries of <span class="math inline">\(\V[\bhat \mid \Xmat]\)</span> and <span class="math inline">\(V[\widetilde{\bfbeta}\mid \Xmat]\)</span> are the variances of the individuals parameters, <span class="math inline">\(\V[\widehat{\beta}_{j} \mid \Xmat]\)</span> and <span class="math inline">\(\V[\widetilde{\beta}_{j} \mid \Xmat]\)</span>. This means that the variances of the individual parameters will be greater for <span class="math inline">\(\widetilde{\bfbeta}\)</span> than for <span class="math inline">\(\bhat\)</span>.</p>
<p>The Gauss-Markov is often cited as a key advantage of OLS over other methods, but it’s important to recognize its limitations. It requires both the linearity assumption and the homoskedastic errors assumption, both of which can be false in many applications.</p>
<p>Finally, note that while we have shown this result for linear estimators, <span class="citation" data-cites="Hansen22">Hansen (<a href="references.html#ref-Hansen22" role="doc-biblioref">2022</a>)</span> proves a more general version of this that result that applies to any unbiased estimator.</p>
</section>
</section>
<section id="the-normal-linear-model" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="the-normal-linear-model"><span class="header-section-number">7.5</span> The normal linear model</h2>
<p>Finally, we add the strongest and thus least loved of the classical linear regression assumption: (conditional) normality of the errors. The historical reason to use this assumption was that finite-sample inference hits a roadblock without some knowledge of the sampling distribution of <span class="math inline">\(\bhat\)</span>. Under the linear CEF model, we saw that it was unbiased and under homoskedasticity we were able to produce an unbiased estimator of the conditional variance. But in order to do hypothesis testing or confidence intervals, we need to be able to make probability statements about the estimator and for that we need to know its exact distribution. Obviously, when the sample size is large, we can rely on the CLT and know that it is approximately normal. But in small samples, what do we do? Historically, we decided to just assume (conditional) normality of the errors to proceed with some knowledge that we were wrong but hopefully not too wrong.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The normal linear regression model
</div>
</div>
<div class="callout-body-container callout-body">
<p>In addition to the linear CEF assumption, we assume that <span class="math display">\[
e_i \mid \Xmat \sim \N(0, \sigma^2).
\]</span></p>
</div>
</div>
<p>A couple of things to point out:</p>
<ul>
<li>The assumption here is not that <span class="math inline">\((Y_{i}, \X_{i})\)</span> are jointly normal (though this would be sufficient for the assumption to hold), but rather that <span class="math inline">\(Y_i\)</span> is normally distributed conditional on <span class="math inline">\(\X_i\)</span>.</li>
<li>Notice that the normal regression model has the homoskedasticity assumption baked in.</li>
</ul>
<div id="thm-normal-ols" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.6 </strong></span>Under the normal linear regression model, we have <span class="math display">\[
\begin{aligned}
  \bhat \mid \Xmat &amp;\sim \N\left(\bfbeta, \sigma^{2}\left(\Xmat'\Xmat\right)^{-1}\right) \\
  \frac{\widehat{\beta}_{j} - \beta_{j}}{[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}]_{jj}/\sqrt{n}} &amp;\sim t_{n-k-1} \\
  W/q &amp;\sim F_{q, n-k-1}.
\end{aligned}
\]</span></p>
</div>
<p>This theorem says that in the normal linear regression model the coefficients are normally distributed, the t-statistics are <span class="math inline">\(t\)</span>-distributed, and a transformation of the Wald statistic follows an <span class="math inline">\(F\)</span> distributed. Note that all of these are <strong>exact</strong> results and do not rely on any large-sample approximation. Under the assumption of conditional normality of the errors, they are as valid for <span class="math inline">\(n = 5\)</span> as they are for <span class="math inline">\(n = 500,000\)</span>.</p>
<p>Almost no one believes any errors are normally distributed, so why even present these results? Unfortunately, most statistical software implementations of OLS make implicitly make this assumption when calculating p-values for tests or constructing confidence intervals. That is, the p-value associated with the <span class="math inline">\(t\)</span>-statistic that <code>lm()</code> outputs in R is based on the <span class="math inline">\(t_{n-k-1}\)</span> distribution and the critical values use to construct confidence intervals with <code>confint()</code> uses that distribution as well. When normality does not hold, there is no principled reason to use the <span class="math inline">\(t\)</span> or the <span class="math inline">\(F\)</span> distributions in this way. But we might hold our nose and use this <em>ad hoc</em> procedure under two rationalizations:</p>
<ul>
<li><span class="math inline">\(\bhat\)</span> is asymptotically normal, but in smaller finite samples this approximation might be poor. The <span class="math inline">\(t\)</span> distribution will make inference more conservative in these cases (wider confidence intervals, smaller test rejection regions) and this might help to offset the poor approximation of the normal in small samples.</li>
<li>As <span class="math inline">\(n\to\infty\)</span>, the <span class="math inline">\(t_{n-k-1}\)</span> will converge to a standard normal distribution, so the <em>ad hoc</em> adjustment will not matter much for medium to large samples.</li>
</ul>
<p>This isn’t a very convincing argument since it’s not clear that the <span class="math inline">\(t\)</span> approximation will be any better than the normal in finite samples. But it’s the best we can do to console ourselves as we go find more data.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Hansen22" class="csl-entry" role="doc-biblioentry">
Hansen, Bruce E. 2022. <span>“A <span>Modern Gauss</span>–<span>Markov Theorem</span>.”</span> <em>Econometrica</em> 90 (3): 1283–94. <a href="https://doi.org/10.3982/ECTA19255">https://doi.org/10.3982/ECTA19255</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The form of the Wald statistic is that of a weighted inner product, <span class="math inline">\(\mb{x}'\mb{Ay}\)</span>, where <span class="math inline">\(\mb{A}\)</span> is a symmetric positive-definite weighting matrix.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We are basically ignoring some edge cases when it comes to discrete covariates here. In particular, we are assuming that <span class="math inline">\(\Xmat'\Xmat\)</span> is nonsingular with probability one, though this can fail if we have a binary covariate since there is some chance (however small) that the entire column will be all ones or all zeros. This would lead to a singular matrix <span class="math inline">\(\Xmat'\Xmat\)</span>. Practically this is not a big deal but it does mean that we have to basically ignore this issue theoretically or just focus on conditional unbiasedness.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./07_least_squares.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>