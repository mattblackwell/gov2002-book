<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A User’s Guide to Statistical Inference and Regression - 2&nbsp; Model-based inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./asymptotics.html" rel="next">
<link href="./design.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    macros: {
      RR: "{\\bf R}",
      bs: ["\\boldsymbol{#1}", 1],
      mb: ["\\mathbf{#1}", 1],
      E: "\\mathbb{E}",
      V: "\\mathbb{V}",
      P: "\\mathbb{P}",
      var: "\\text{var}",
      cov: "\\text{cov}",
      N: "\\mathcal{N}",
      Bern: "\\text{Bern}",
      Bin: "\\text{Bin}",
      Pois: "\\text{Pois}",
      Unif: "\\text{Unif}",
      se: "\\textsf{se}",
      U: "\\mb{U}",
      Xbar: "\\overline{X}",
      Ybar: "\\overline{Y}",
      real: "\\mathbb{R}",
      bbL: "\\mathbb{L}",
      u: "\\mb{u}",
      v: "\\mb{v}",
      M: "\\mb{M}",
      X: "\\mb{X}",
      Xmat: "\\mathbb{X}",
      bfx: "\\mb{x}",
      y: "\\mb{y}",
      bfbeta: "\\bs{\\beta}",
      e: "\\bs{\\epsilon}",
      bhat: "\\widehat{\\bs{\\beta}}",
      XX: "\\Xmat'\\Xmat",
      XXinv: "\\left(\\Xmat'\\Xmat\\right)^{-1}",
      hatsig: "\\widehat{\\sigma}^2",
      red: ["\\textcolor{red!60}{#1}", 1],
      indianred: ["\\textcolor{indianred}{#1}", 1],
      blue: ["\\textcolor{blue!60}{#1}", 1],
      dblue: ["\\textcolor{dodgerblue}{#1}", 1],
      indep: "\\perp\\!\\!\\!\\perp",
      inprob: "\\overset{p}{\\to}",
      indist: "\\overset{d}{\\to}",
      argmax: ["\\operatorname\{arg\,max\}"],
      argmin: ["\\operatorname\{arg\,min\}"]      
    }
  }
};
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./design.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./estimation.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mattblackwell/gov2002-book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./users-guide.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Design-based Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis_tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols_properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#probability-vs-inference-the-big-picture" id="toc-probability-vs-inference-the-big-picture" class="nav-link" data-scroll-target="#probability-vs-inference-the-big-picture"><span class="header-section-number">2.2</span> Probability vs inference: the big picture</a></li>
  <li><a href="#question-1-population" id="toc-question-1-population" class="nav-link" data-scroll-target="#question-1-population"><span class="header-section-number">2.3</span> Question 1: Population</a></li>
  <li><a href="#question-2-statistical-model" id="toc-question-2-statistical-model" class="nav-link" data-scroll-target="#question-2-statistical-model"><span class="header-section-number">2.4</span> Question 2: Statistical model</a></li>
  <li><a href="#question-3-quantities-of-interest" id="toc-question-3-quantities-of-interest" class="nav-link" data-scroll-target="#question-3-quantities-of-interest"><span class="header-section-number">2.5</span> Question 3: Quantities of interest</a></li>
  <li><a href="#question-4-estimator" id="toc-question-4-estimator" class="nav-link" data-scroll-target="#question-4-estimator"><span class="header-section-number">2.6</span> Question 4: Estimator</a></li>
  <li><a href="#how-to-find-estimators" id="toc-how-to-find-estimators" class="nav-link" data-scroll-target="#how-to-find-estimators"><span class="header-section-number">2.7</span> How to find estimators</a>
  <ul class="collapse">
  <li><a href="#parametric-models-and-maximum-likelihood" id="toc-parametric-models-and-maximum-likelihood" class="nav-link" data-scroll-target="#parametric-models-and-maximum-likelihood"><span class="header-section-number">2.7.1</span> Parametric models and maximum likelihood</a></li>
  <li><a href="#plug-in-estimators" id="toc-plug-in-estimators" class="nav-link" data-scroll-target="#plug-in-estimators"><span class="header-section-number">2.7.2</span> Plug-in estimators</a></li>
  </ul></li>
  <li><a href="#the-three-distributions-population-empirical-and-sampling" id="toc-the-three-distributions-population-empirical-and-sampling" class="nav-link" data-scroll-target="#the-three-distributions-population-empirical-and-sampling"><span class="header-section-number">2.8</span> The three distributions: population, empirical, and sampling</a></li>
  <li><a href="#finite-sample-properties-of-estimators" id="toc-finite-sample-properties-of-estimators" class="nav-link" data-scroll-target="#finite-sample-properties-of-estimators"><span class="header-section-number">2.9</span> Finite-sample properties of estimators</a>
  <ul class="collapse">
  <li><a href="#bias" id="toc-bias" class="nav-link" data-scroll-target="#bias"><span class="header-section-number">2.9.1</span> Bias</a></li>
  </ul></li>
  <li><a href="#question-5-uncertainty" id="toc-question-5-uncertainty" class="nav-link" data-scroll-target="#question-5-uncertainty"><span class="header-section-number">2.10</span> Question 5: Uncertainty</a>
  <ul class="collapse">
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error"><span class="header-section-number">2.10.1</span> Mean squared error</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.11</span> Summary</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mattblackwell/gov2002-book/edit/main/estimation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./design.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./estimation.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-model-based" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Suppose you have been tasked with estimating the fraction of a population that supports increasing legal immigration limits. You have a sample of data on some individual respondents’ support for the policy, but you don’t know exactly how this data was sampled. How should you use the information you know (the data) to make a best guess about the information you don’t know (the fraction of the population)?</p>
<p>The design-based approach that we discussed in the previous chapter is a coherent and internally consistent framework for estimating quantities and quantifying uncertainty, but this crisp conceptual clarity comes from having exact knowledge of the sampling design. Going back to our immigration example, a reasonable approach is to use the fraction of the sample that supports the policy as the best guess about the fraction of the population. As we saw in the last chapter, a simple random sample would justify this approach.</p>
<p>But how does one perform estimation and inference when lacking complete information about the sampling design? How do we proceed with inference if outcomes are random due to nonresponse or measurement error? What if we would like to make inferences about a population not covered in the sampling frame? In these cases, inference requires additional information that can be incorporated via a <strong>statistical model</strong>. A model-based approach to statistical inference views the data <span class="math inline">\(X_1,\ldots, X_n\)</span> as a set of random variables that follow some probability distribution. The measurements in the actual sample, <span class="math inline">\(x_1, \ldots, x_n\)</span>, are realizations of these random variables. The probability distribution of <span class="math inline">\(X_1,\ldots, X_n\)</span> is the model for the data and all inferences are based on it. Models can be very specific – for example, a researcher might assume the data are normally distributed – or can be very general – for example, the distribution of the data has finite mean and variance.</p>
<p>The focus of this chapter (and most of introductory statistics) will be on statistical models that assume units are <strong>independent and identically distributed</strong> or, more succinctly, <strong>iid</strong>. This assumption means that each unit gives us new information about the same underlying data-generating process. Because this assumption is often motivated by a probability sample from a population, some authors also refer to this as a <strong>random sampling</strong> assumption. The “sample” refers to the idea that our data is a subset of some larger <strong>population</strong>. The “random” modifier means that the subset was chosen by an uncertain process that did not favor one type of person versus another.</p>
<p>Why focus on iid/random samples even though many data sets are at least partially non-random or represent the entire population rather than a subset? Consider the famous story of a drunkard’s search for a two-dollar bill lost in downtown Boston:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<blockquote class="blockquote">
<p>“I lost a $2 bill down on Atlantic Avenue,” said the man.</p>
<p>“What’s that?” asked the puzzled officer. “You lost a $2 bill on Atlantic Avenue? Then why are you hunting around here in Copley Square?”</p>
<p>“Because,” said the man as he turned away and continued his hunt on his hands and knees, “the light’s better up here.”</p>
</blockquote>
<p>Like the poor drunkard, we focus on searching an area (random samples) that are easier to search because there is more light or, more accurately, easier math. Unlike this apocryphal tale, our search will help us better understand the darkness of non-random samples because the core ideas and intuitions from random sampling form the basis for the theoretical extensions into more exotic settings.</p>
<p>This chapter has two goals. First, we will introduce the entire model-based framework of estimation and estimators. We will discuss different ways to compare the properties of estimators. Most of these properties will be similar to those of the design-based framework, except the properties will be with respect to the model rather than the sampling design. (The core questions of quantitative research largely remain the same, but we replace specifying the sampling design with the specification of a probabilistic model for our data.) Second, we will establish key properties for a general class of estimators that can be written as a sample mean. These results are useful in their own right since these estimators are ubiquitous, but the derivations also provide examples of how we establish such results. Building comfort with these proofs helps us understand the arguments about novel estimators that we inevitably see over the course of our careers.</p>
</section>
<section id="probability-vs-inference-the-big-picture" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="probability-vs-inference-the-big-picture"><span class="header-section-number">2.2</span> Probability vs inference: the big picture</h2>
<p>Probability is the mathematical study of uncertain events and is the basis of the mathematical study of estimation. In probability, we assume we know the truth of the world (how many blue and red balls are in the urn) and calculate the probability of possible events (getting more than five red balls when drawing ten from the urn). Estimation works in reverse. Someone hands you five balls, 2 red and 3 blue, and your task is to guess the contents of the urn from which they came. With estimation, we use our observed data to make an <strong>inference</strong> about the data-generating process.</p>
<p><img src="assets/img/two-direction.png" class="img-fluid"></p>
<p>An estimator is a rule for converting our data into a best guess about some unknown quantity, such as the percent of balls in the urn, or, to use our example from the introduction, the fraction of the public supporting increasing legal immigration limits. For example, an estimator could be a rule that the proportion of red balls that you draw from the urn is a good guess for the proportion of red balls that you would find if you looked inside the urn.</p>
<p>We prefer to use <strong>good</strong> estimators rather than <strong>bad</strong> estimators. But what makes an estimator good or bad? In our red ball example, an estimator that always returns the value 3 is probably bad. Still, it will be helpful for us to formally define and explore properties of estimators that will allow us to compare them and choose the good over the bad. We begin with an example that highlights two estimators that at first glance may seem similar.</p>
<div id="exm-rct" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Randomized control trial)</strong></span> Suppose we are conducting a randomized experiment on framing effects. All respondents receive factual information about current immigration levels. Those in the treatment group (<span class="math inline">\(D_i = 1\)</span>) receive additional information about the positive benefits of immigration, while those in the control group (<span class="math inline">\(D_i = 0\)</span>) receive no additional framing. The outcome is a binary outcome, whether the respondent supports increasing legal immigration limits (<span class="math inline">\(Y_i = 1\)</span>) or not (<span class="math inline">\(Y_i = 0\)</span>). The observed data consists of <span class="math inline">\(n\)</span> pairs of random variables, the outcome, and the treatment assignment: <span class="math inline">\(\{(Y_1, D_1), \ldots, (Y_n, D_n)\}\)</span>.</p>
<p>Define the two sample means/proportions in each group as <span class="math display">\[
\Ybar_1 = \frac{1}{n_1} \sum_{i: D_i = 1} Y_i, \qquad\qquad \Ybar_0 = \frac{1}{n_0} \sum_{i: D_i = 0} Y_i,
\]</span> where <span class="math inline">\(n_1 = \sum_{i=1}^n D_i\)</span> is the number of treated units and <span class="math inline">\(n_0 = n - n_1\)</span> is the number of control units.</p>
<p>A standard estimator for the treatment effect in a study such as this would be the difference in means, <span class="math inline">\(\Ybar_1 - \Ybar_0\)</span>. But this is only one of many possible estimators. We could also estimate the effect by taking this difference in means separately by party identification and then averaging those party-specific effects by the size of those groups. This estimator is commonly called a <strong>poststratification</strong> estimator. Which of these two estimators we should prefer is at first glance unclear.</p>
</div>
<p>We now turn to the same key questions that we used to motivate design-based inference, but adapt these to consider model-based inference.</p>
</section>
<section id="question-1-population" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="question-1-population"><span class="header-section-number">2.3</span> Question 1: Population</h2>
<p>The main advantage and disadvantage of relying on models is that they are abstract and theoretical, which means the connection between a model and the population it helps explain is less direct than with the design-based framework. Nevertheless, we need to clearly articulate our population of study – that is, who or what we want to learn about – since it is crucial for evaluating the types of modeling assumptions that will be sustainable.</p>
<p>As in the design-based setting, there is often a clear and distinct population such as “all registered voters” or “all Boston residents.” In other cases, the population may be more abstract. For example, a large multi-field literature has studied how the size of minority populations affects the views of the local majority population. Researchers in this space may be interested in making claims beyond the particular geographic region or minority/majority group, instead implicitly or explicitly considering a “superpopulation” of such cases that their model might explain. While there is nothing theoretically wrong with this approach, these ideas are often neglected in practice and the “scope conditions” of a particular model go unarticulated. The best quantitative work will be clear about what units or processes it is trying to learn about so that readers can evaluate how well the modeling assumptions fit that task.</p>
</section>
<section id="question-2-statistical-model" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="question-2-statistical-model"><span class="header-section-number">2.4</span> Question 2: Statistical model</h2>
<p>Let’s begin by building a bare-bones probability model for how our data came to be. As an example, suppose we have a data set with a series of numbers representing the ages, political party affiliations, and policy opinions of 1000 survey respondents. But we know that row 58 of our data could have produced a different set of numbers if another respondent had been selected as row 58 or if the original respondent gave a different opinion about immigration because they happened to see an immigration news story just before responding. To reason about this type of uncertainty precisely, we write <span class="math inline">\(X_i\)</span> as the random variable representing the value that row <span class="math inline">\(i\)</span> of some variable will take, before we see the data. The distribution of this random variable would tell us what types of data we should expect to see.</p>
<p>Why represent the data with random variables when we already know the value of the data itself? Why pretend we haven’t seen the data? The study of estimation from a frequentist perspective (which is the perspective of this book) focuses on the properties of estimators across <strong>repeated samples</strong>. In the example of the policy survey, this is akin to drawing a 1000 person sample repeatedly, each time including possibly different respondents in the sample. The random variable <span class="math inline">\(X_i\)</span> represents our uncertainty about what value, say, age will take for respondent <span class="math inline">\(i\)</span> in any of these samples, and the set <span class="math inline">\(\{X_{1}, \ldots, X_{n}\}\)</span> represents our uncertainty about the entire column of ages for all <span class="math inline">\(n\)</span> respondents. At the most general, the model-based approach says that these <span class="math inline">\(n\)</span> random variables follow some joint distribution, <span class="math inline">\(F_{X_{1},\ldots,X_{n}}\)</span>, <span class="math display">\[
\{X_{1}, \ldots, X_{n}\} \sim F_{X_{1},\ldots,X_{n}}
\]</span> The joint distribution <span class="math inline">\(F\)</span> here represents the probability model for the data. We have made no assumptions about it so far, so it could be any joint probability distribution over <span class="math inline">\(n\)</span> random variables. Note that this level of generality is difficult to work with in practice because there is essentially one draw from this joint distribution (the <span class="math inline">\(n\)</span> measurements in the data). The core question of modeling is about what restrictions a researcher puts on this joint distribution to make learning about it more tractable.</p>
<p>We focus on a relatively simple setting where we assume the data <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span> are <strong>independent and identically distributed</strong> (iid) draws from a distribution with cumulative distribution function (cdf) <span class="math inline">\(F\)</span>. They are independent in that information about any subset of random variable is not informative about any other subset of random variables, or, more formally, <span class="math display">\[
F_{X_{1},\ldots,X_{n}}(x_{1}, \ldots, x_{n}) = F_{X_{1}}(x_{1})\cdots F_{X_{n}}(x_{n}) = \prod_{i=1}^n F(x_i)
\]</span> where <span class="math inline">\(F_{X_{1},\ldots,X_{n}}(x_{1}, \ldots, x_{n})\)</span> is the joint cdf of the random variable and <span class="math inline">\(F_{X_{j}}(x_{j})\)</span> is the marginal cdf of the <span class="math inline">\(j\)</span>th random variable. They are “identically distributed” in the sense that each of the random variables <span class="math inline">\(X_i\)</span> have the same marginal distribution, <span class="math inline">\(F\)</span>.</p>
<p>Note that we are being purposely vague about this cdf—it simply represents the unknown distribution of the data, otherwise known as the <strong>data generating process</strong> (DGP). Sometimes <span class="math inline">\(F\)</span> is also referred to as the <strong>population distribution</strong> or even just <strong>population</strong>, which has its roots in viewing the data as a random sample from some larger population.[^model] As a shorthand, we often say that the collection of random variables <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span> is a <strong>random sample</strong> from population <span class="math inline">\(F\)</span> if <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span> is iid with distribution <span class="math inline">\(F\)</span>. The <strong>sample size</strong> <span class="math inline">\(n\)</span> is the number of units in the sample.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You might wonder why we reference the distribution of <span class="math inline">\(X_i\)</span> with the cdf, <span class="math inline">\(F\)</span>. Mathematical statistics tends to do this to avoid having to deal with discrete and continuous random variables separately. Every random variable – whether discrete or continuous – has a cdf, and the cdf contains all information about the distribution of a random variable.</p>
</div>
</div>
<p>Two metaphors help build intuition behind viewing the data as an iid draw from <span class="math inline">\(F\)</span>:</p>
<ol type="1">
<li><strong>Random sampling</strong>. Suppose we have a population of size <span class="math inline">\(N\)</span> that is much larger than our sample size <span class="math inline">\(n\)</span>, and we take a random sample of size <span class="math inline">\(n\)</span> from this population with replacement. The distribution of the data in the random sample will be iid draws from the population distribution of the variables we are sampling. For example, suppose the population proportion of Democratic party identifiers among U.S. citizens is 0.33. If we randomly sample <span class="math inline">\(n = 100\)</span> U.S. citizens, each data point <span class="math inline">\(X_i\)</span> will be distributed Bernoulli with a probability of success (i.e., Democratic Party identifier) of 0.33.</li>
</ol>
<p>Note that the last chapter explored simple random samples <em>without replacement</em>, which is a more common type of sampling – since generally people are selected into a survey once and they do not go back into the pool of potential survey takers. Sampling without replacement creates dependence across units, which would violate the iid assumption. However, if the population size <span class="math inline">\(N\)</span> is very large relative to the sample size <span class="math inline">\(n\)</span>, this dependence will be very small, and the iid assumption will be relatively innocuous.</p>
<ol start="2" type="1">
<li><strong>Groundhog Day</strong>. Random sampling does not always make sense as a justification for iid data, especially when the units are not samples at all but rather countries, states, or subnational units. (In these cases, the population can be the same as the sample – for example, using all 50 states to draw conclusions on the efficacy of state policy.) In this case, we have to appeal to a thought experiment where <span class="math inline">\(F\)</span> represents the fundamental uncertainty in the data-generating process. The metaphor here is that if we could re-run history many times, such as what happens to the protagonist played by Bill Murray in the 1993 American comedy movie <em>Groundhog Day</em> when he is magically forced to relive February 2 over and over again. Under this fiction, data and outcomes would change slightly due to the inherently stochastic nature of the world. In the movie, for example, Murray’s character starts off the same day in exactly the same way, but he begins to change his actions and the outcomes at the end of each day change in subtle ways. The iid assumption, then, is that each of the units in our data has the same DGP producing this data or the same distribution of outcomes under the <em>Groundhog Day</em> scenario. The set of all these infinite possible draws from the DGP is sometimes referred to as the <strong>superpopulation</strong>.</li>
</ol>
<p>Note that there are other situations where the iid assumption is not appropriate, which we discuss in later chapters. But much of the innovation and growth in statistics over the last 50 years has been in figuring out how to make statistical inferences when iid does not hold. The solutions are often specific to the type of iid violation (e.g., spatial, time-series, network, clustered). As a rule of thumb, however, if the iid assumption may not be valid, any uncertainty statements will likely be overconfident. For example, confidence intervals, which we will cover in later chapters, are too small.</p>
<p>Finally, we introduced the data as a scalar random variable, but often our data has multiple variables. In that case, we easily modify <span class="math inline">\(X_i\)</span> to be a random vector (that is, a vector of random variables) and then <span class="math inline">\(F\)</span> becomes the joint distribution of that random vector. Nothing substantive changes about the above discussion.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Survey sampling is one of the most popular ways of obtaining samples from a population, but modern sampling practices rarely produce a “clean” set of <span class="math inline">\(n\)</span> iid responses. There are several reasons for this:</p>
<ul>
<li><p>Modern random sampling techniques generally do not select every unit with the same probability. We might <em>oversample</em> certain groups for which we want more precise estimates, leading those groups to have a higher likelihood of being in the sample.</p></li>
<li><p>Response rates to surveys have been in steep decline and can often dip below 10%. Such non-random selection into the observed sample might lead to problems.</p></li>
<li><p>Internet polling is less costly than other forms of polling, but obtaining a list of population email addresses (or other digital contact information) to randomly sample is basically impossible. Large survey firms instead recruit large groups of panelists with known demographic information from which they can randomly sample in a way that matches population demographic information. Because the initial opt-in panel is not randomly sampled from the population, this procedure does not produce a true “random sample,” but, under certain assumptions, we can treat it like it is.</p></li>
</ul>
<p>As discussed in the last chapter, there are ways to handle all of these issues (mostly through the use of survey weights), but it is important to realize that using a modern survey “as if” it was a simple random sample might lead to poor performance and incorrect inferences.</p>
</div>
</div>
</section>
<section id="question-3-quantities-of-interest" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="question-3-quantities-of-interest"><span class="header-section-number">2.5</span> Question 3: Quantities of interest</h2>
<p>In model-based inference, our goal is to learn about the data-generating process. Each data point <span class="math inline">\(X_i\)</span> represents a draw from a distribution, captured by the cdf <span class="math inline">\(F\)</span>, and we would like to know more about this distribution. We might be interested in estimating the cdf at a general level or only some feature of the distribution, like a mean or conditional expectation function. We call these numerical features the <strong>quantities of interest</strong>. (Similarly, in the design-based inference framework we discussed in the previous chapter, the quantity of interest was a numerical summary of the finite population.)</p>
<p>The following are examples of frequently used quantities of interest:</p>
<div id="exm-prop" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Population mean)</strong></span> We may be interested in where the typical member of a population falls on some questions. Suppose we wanted to know the proportion of US citizens who support increasing legal immigration For citizen <span class="math inline">\(i\)</span>, denote support as <span class="math inline">\(Y_i = 1\)</span>. Our quantity of interest is then the mean of this random variable, <span class="math inline">\(\mu = \E[Y_i] = \P(Y_{i} = 1)\)</span>. This is the same as the probability of randomly drawing someone from the population who supports increasing legal immigration.</p>
</div>
<div id="exm-var" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Population variance)</strong></span> We may also be interested in variation in the population. For example, feeling thermometer scores are a common way to assess how survey respondents feel about a particular person or group. These ask each respondent to say how warmly he or she feels toward a group on a scale from 0 (cool) to 100 (warm), which we will denote <span class="math inline">\(Y_i\)</span>. We might be interested in how polarized views are toward a group in the population, and one measure of polarization could be the variance, or spread, of the distribution of <span class="math inline">\(Y_i\)</span> around the mean. In this case, <span class="math inline">\(\sigma^2 = \V[Y_i] = \E[(Y_i - \E[Y_i])^2]\)</span> would be our quantity of interest.</p>
</div>
<div id="exm-rct-ii" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (RCT continued)</strong></span> <a href="#exm-rct" class="quarto-xref">Example&nbsp;<span>2.1</span></a> discussed a typical estimator for an experimental study with a binary treatment. The goal of that experiment is to learn about the difference between two conditional probabilities (or expectations): 1) the average support for increasing legal immigration in the treatment group, <span class="math inline">\(\mu_1 = \E[Y_i \mid D_i = 1]\)</span>, and 2) the same average in the control group, <span class="math inline">\(\mu_0 = \E[Y_i \mid D_i = 0]\)</span>. This difference, <span class="math inline">\(\mu_1 - \mu_0\)</span>, is a function of unknown features of these two conditional distributions.</p>
</div>
<p>Each of these is a function of the (possibly joint) distribution of the data, <span class="math inline">\(F\)</span>. In each of these, we are not necessarily interested in the entire distribution, just summaries of it (central tendency, spread). Of course, there are situations where we are also interested in the complete distribution. To speak about estimation in general, we will let <span class="math inline">\(\theta\)</span> represent some generic quantity of interest. <strong>Point estimation</strong> describes how we obtain a single “best guess” about <span class="math inline">\(\theta\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some refer to quantities of interest as <strong>parameters</strong> or <strong>estimands</strong> (that is, the target of estimation).</p>
</div>
</div>
</section>
<section id="question-4-estimator" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="question-4-estimator"><span class="header-section-number">2.6</span> Question 4: Estimator</h2>
<p>Having a target in mind, we can estimate it with our data. To do so, we first need a rule or algorithm or function that takes as inputs the data and returns a best guess about the quantity of interest. One of the most popular and useful algorithm would be to sum all the data points and divide by the number of points: <span class="math display">\[
\frac{X_1 + X_2 + \cdots + X_n}{n}.
\]</span> This, the much-celebrated sample mean, provides a rule for how produce a single-number summary of the data. To go one pedantic step further and define it as a function of the data more explicitly: <span class="math display">\[
\textsf{mean}(X_1, X_2, \ldots, X_n) = \frac{X_1 + X_2 + \cdots + X_n}{n}.
\]</span> We can use this model to provide a definition for an arbitrary estimator for an arbitrary quantity of interest.</p>
<div id="def-estimator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1</strong></span> An <strong>estimator</strong> <span class="math inline">\(\widehat{\theta}_n = \theta(X_1, \ldots, X_n)\)</span> for some parameter <span class="math inline">\(\theta\)</span>, is a function of the data intended as a guess about <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is widespread, though not universal, to use the “hat” notation to define an estimator and its estimand. For example, <span class="math inline">\(\widehat{\theta}\)</span> (or “theta hat”) indicates that this estimator is targeting the parameter <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="exm-mean-est" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Estimators for the population mean)</strong></span> Suppose our goal is to estimate the population mean of <span class="math inline">\(F\)</span>, which we will represent as <span class="math inline">\(\mu = \E[X_i]\)</span>. We could choose from several estimators, all with different properties. <span class="math display">\[
\widehat{\theta}_{n,1} = \frac{1}{n} \sum_{i=1}^n X_i, \quad \widehat{\theta}_{n,2} = X_1, \quad \widehat{\theta}_{n,3} = \text{max}(X_1,\ldots,X_n), \quad \widehat{\theta}_{n,4} = 3
\]</span> The first is just the sample mean, which is an intuitive and natural estimator for the population mean. The second just uses the first observation. While this seems silly, this is a valid statistic since it is a function of the data! The third takes the maximum value in the sample, and the fourth always returns three, regardless of the data. These are also valid statistics.</p>
</div>
<p>When we view the data <span class="math inline">\(\{X_{1}, \ldots, X_{n}\}\)</span> as a collection of random variables, then any function of them is also a random variable. Thus, we can view <span class="math inline">\(\widehat{\theta}_n\)</span> as a random variable that has a distribution induced by the randomness of the sample. Drawing two different samples of respondents will lead to two different estimates. For example, here we illustrate two samples of size <span class="math inline">\(n =5\)</span> from the population distribution of a binary variable:</p>
<p><img src="assets/img/sampling-distribution.png" class="img-fluid"></p>
<p>We can see that the mean of the variable depends on what exact values end up in our sample. We refer to the distribution of <span class="math inline">\(\widehat{\theta}_n\)</span> across repeated samples as its <strong>sampling distribution</strong>. The sampling distribution of an estimator will be the basis for all of the formal statistical properties of an estimator.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>One important distinction of jargon is between an estimator and an estimate. The estimator is a function of the data, whereas the <strong>estimate</strong> is the <em>realized value</em> of the estimator once we see the data (that is, the data are realized). The estimator is a random variable that has uncertainty over what value it will take, and we represent the estimator as a function of random variables, <span class="math inline">\(\widehat{\theta}_n = \theta(X_1, \ldots, X_n)\)</span>. An estimate is a single number, such as 0.38, that we calculated in R with our data (our draw from <span class="math inline">\(F\)</span>). Formally, the estimate is <span class="math inline">\(\theta(x_1, \ldots, x_n)\)</span> when the data is <span class="math inline">\(\{X_1, \ldots, X_n\} = \{x_1, \ldots, x_n\}\)</span>, whereas we represent the estimator as a function of random variables, <span class="math inline">\(\widehat{\theta}_n = \theta(X_1, \ldots, X_n)\)</span>.</p>
</div>
</div>
</section>
<section id="how-to-find-estimators" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="how-to-find-estimators"><span class="header-section-number">2.7</span> How to find estimators</h2>
<p>Where do estimators come from? That may seem like a question reserved for statisticians or methodologists or others responsible for “developing new methods.” But knowing how estimators are derived is valuable even if we never plan to do it ourselves. Knowing where an estimator comes from provides strong insights into its strengths and weaknesses. We will briefly introduce estimators based on parametric models, before turning to the main focus of this book, plug-in estimators.</p>
<section id="parametric-models-and-maximum-likelihood" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="parametric-models-and-maximum-likelihood"><span class="header-section-number">2.7.1</span> Parametric models and maximum likelihood</h3>
<p>The first method for generating estimators relies on <strong>parametric models</strong>, in which the researcher specifies the exact distribution (up to some unknown parameters) of the DGP. Let <span class="math inline">\(\theta\)</span> be the parameters of this distribution, where <span class="math inline">\(\{X_1, \ldots, X_n\}\)</span> be iid draws from <span class="math inline">\(F_{\theta}\)</span>. We should also formally state the set of possible values the parameters can take, which we call the <strong>parameter space</strong>, denoted by <span class="math inline">\(\Theta\)</span>. Because we assume we know the distribution of the data, we can write the probability density function, or pdf, as <span class="math inline">\(f(X_i \mid \theta)\)</span> and define the likelihood function as the product of these pdfs over the units as a function of the parameters: <span class="math display">\[
L(\theta) = \prod_{i=1}^n f(X_i \mid \theta).
\]</span> We can then define the <strong>maximum likelihood</strong> estimator (MLE) for <span class="math inline">\(\theta\)</span> as the values of the parameter that, well, maximize the likelihood: <span class="math display">\[
\widehat{\theta}_{mle} = \argmax_{\theta \in \Theta} \; L(\theta)
\]</span> Sometimes we can use calculus to derive a closed-form expression for the MLE. At other times we use iterative techniques that search the parameter space for the maximum.</p>
<p>Maximum likelihood estimators have nice properties, especially in large samples. Unfortunately, they also require the correct knowledge of the parametric model, which is often difficult to justify. Do we really know if we should model a given event count variable as Poisson or Negative Binomial? The attractive properties of MLE are only as good as the ability to specify the parametric model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
No free lunch
</div>
</div>
<div class="callout-body-container callout-body">
<p>Building up intuition about the <strong>assumptions-precision tradeoff</strong> is essential. Researchers can usually get more precise estimates if they make stronger and potentially more fragile assumptions. Conversely, they will almost always get less accurate estimates when weakening the assumptions.</p>
</div>
</div>
</section>
<section id="plug-in-estimators" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="plug-in-estimators"><span class="header-section-number">2.7.2</span> Plug-in estimators</h3>
<p>The second broad class of estimators is <strong>semiparametric</strong> in that we specify some finite-dimensional parameters of the DGP but leave the rest of the distribution unspecified. For example, we might define a population mean, <span class="math inline">\(\mu = \E[X_i]\)</span> and a population variance, <span class="math inline">\(\sigma^2 = \V[X_i]\)</span> but leave the shape of the distribution unrestricted. This ensures that our estimators will be less dependent on correctly specifying distributions about which we have little intuition or knowledge.</p>
<p>The primary method for constructing estimators in this setting is to use the <strong>plug-in estimator</strong>, or the estimator that replaces any population mean with a sample mean. Obviously, in the case of estimating the population mean, <span class="math inline">\(\mu\)</span>, we will use the <strong>sample mean</strong> as the estimate: <span class="math display">\[
\Xbar_n = \frac{1}{n} \sum_{i=1}^n X_i \quad \text{estimates} \quad \E[X_i] = \int_{\mathcal{X}} x f(x)dx
\]</span> In plain language, we are replacing the unknown population distribution <span class="math inline">\(f(x)\)</span> in the population mean with a discrete uniform distribution over our data points, with <span class="math inline">\(1/n\)</span> probability assigned to each unit. Why do this? It encodes that, if we have a random sample, our best guess about the population distribution of <span class="math inline">\(X_i\)</span> is the sample distribution in our observed data. If this intuition fails, you can hold onto an analog principle: sample means of random variables are natural estimators of population means.</p>
<p>What about estimating something more complicated, like the expected value of a function of the data, <span class="math inline">\(\theta = \E[r(X_i)]\)</span>? The key is to see that <span class="math inline">\(r(X_i)\)</span> is also a random variable. Let this random variable be <span class="math inline">\(Y_i = r(X_i)\)</span>. Now we can see that <span class="math inline">\(\theta\)</span> is just the population expectation of this random variable. Using the plug-in estimator gives us: <span class="math display">\[
\widehat{\theta} = \frac{1}{n} \sum_{i=1}^n Y_i = \frac{1}{n} \sum_{i=1}^n r(X_i).
\]</span></p>
<p>These facts enable us to describe a more general plug-in estimator. To estimate some quantity of interest that is a function of population means, we can generate a plug-in estimator by replacing any population mean with a sample mean. Formally, let <span class="math inline">\(\alpha = g\left(\E[r(X_i)]\right)\)</span> be a parameter that is defined as a function of the population mean of a (possibly vector-valued) function of the data. We can then estimate this parameter by plugging in the sample mean for the population mean to get the <strong>plug-in estimator</strong>, <span class="math display">\[
\widehat{\alpha} = g\left( \frac{1}{n} \sum_{i=1}^n r(X_i) \right) \quad \text{estimates} \quad \alpha = g\left(\E[r(X_i)]\right)
\]</span> This approach to plug-in estimation with sample means is very general and will allow us to derive estimators in various settings.</p>
<div id="exm-var-est" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Estimating population variance)</strong></span> The population variance of a random variable is <span class="math inline">\(\sigma^2 = \E[(X_i - \E[X_i])^2]\)</span>. To derive a plug-in estimator for this quantity, we replace the inner <span class="math inline">\(\E[X_i]\)</span> with <span class="math inline">\(\Xbar_n\)</span> and the outer expectation with another sample mean: <span class="math display">\[
\widehat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \Xbar_n)^2.
\]</span> This plug-in estimator differs from the standard sample variance, which divides by <span class="math inline">\(n - 1\)</span> rather than <span class="math inline">\(n\)</span>. This minor difference does not matter in moderate to large samples.</p>
</div>
<div id="exm-cov-est" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Estimating population covariance)</strong></span> Suppose we have two variables, <span class="math inline">\((X_i, Y_i)\)</span>. A natural quantity of interest here is the population covariance between these variables, <span class="math display">\[
\sigma_{xy} = \text{Cov}[X_i,Y_i] = \E[(X_i - \E[X_i])(Y_i-\E[Y_i])],
\]</span> which has the plug-in estimator, <span class="math display">\[
\widehat{\sigma}_{xy} = \frac{1}{n} \sum_{i=1}^n (X_i - \Xbar_n)(Y_i - \Ybar_n).
\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Given the connection between the population mean and the sample mean, you may see the <span class="math inline">\(\E_n[\cdot]\)</span> operator used as a shorthand for the sample average: <span class="math display">\[
\E_n[r(X_i)] \equiv \frac{1}{n} \sum_{i=1}^n r(X_i).
\]</span></p>
</div>
</div>
<p>Finally, plug-in estimation goes beyond just replacing population means with sample means. We can derive estimators of the population quantiles like the median with sample versions of those quantities. These approaches are unified in replacing the unknown population cdf, <span class="math inline">\(F\)</span>, with the empirical cdf, <span class="math display">\[
\widehat{F}_n(x) = \frac{\sum_{i=1}^n \mathbb{I}(X_i \leq x)}{n},
\]</span> where <span class="math inline">\(\mathbb{I}(A)\)</span> is an <em>indicator function</em> that takes the value 1 if the event <span class="math inline">\(A\)</span> occurs and 0 otherwise. For a more complete and technical treatment of these ideas, see Wasserman (2004) Chapter 7.</p>
</section>
</section>
<section id="the-three-distributions-population-empirical-and-sampling" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="the-three-distributions-population-empirical-and-sampling"><span class="header-section-number">2.8</span> The three distributions: population, empirical, and sampling</h2>
<p>Once we start to wade into estimation, there are several distributions to keep track of, and things can quickly become confusing. Three specific distributions are all related and easy to confuse, but keeping them distinct is crucial.</p>
<p>The <strong>population distribution</strong> is the distribution of the random variable, <span class="math inline">\(X_i\)</span>, which we have labeled <span class="math inline">\(F\)</span> and is our target of inference. The <strong>empirical distribution</strong> is the distribution of the actual realizations of the random variables in our samples, <span class="math inline">\(X_1, \ldots, X_n\)</span> (that is, the values that we eventually observe in our data frame). Because this is a random sample from the population distribution and can serve as an estimator of <span class="math inline">\(F\)</span>, we sometimes call this <span class="math inline">\(\widehat{F}_n\)</span>.</p>
<p><!-- TODO: Insert Sampling distribution figure here --></p>
<p>Separately from both is the <strong>sampling distribution of an estimator</strong>, which is the probability distribution of <span class="math inline">\(\widehat{\theta}_n\)</span>. This represents the uncertainty around our estimate before we see the data. Remember that our estimator is itself a random variable because it is a function of random variables: the data itself. That is, we defined the estimator as <span class="math inline">\(\widehat{\theta}_n = \theta(X_1, \ldots, X_n)\)</span>.</p>
<div id="exm-three-dist" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Likert responses)</strong></span> Suppose <span class="math inline">\(X_i\)</span> is the answer to the question “How much do you agree with the following statement: Immigrants are a net positive for the United States,” where <span class="math inline">\(X_i = 0\)</span> is “strongly disagree,” <span class="math inline">\(X_i = 1\)</span> is “disagree,” <span class="math inline">\(X_i = 2\)</span> is “neither agree nor disagree,” <span class="math inline">\(X_i = 3\)</span> is “agree,” and <span class="math inline">\(X_i = 4\)</span> is “strongly agree.”</p>
<p>The population distribution describes the probability of randomly selecting a person with each one of these values, <span class="math inline">\(\P(X_i = x)\)</span>. The empirical distribution would be the fraction of our observed data taking each value. And the sampling distribution of the sample mean, <span class="math inline">\(\Xbar_n\)</span>, would be the distribution of the sample mean recalculated across repeated samples from the population.</p>
<p>Suppose the population distribution of <span class="math inline">\(X_i\)</span> followed a binomial distribution with five trials and probability of success in each trial of <span class="math inline">\(p = 0.4\)</span>. We could generate one sample with <span class="math inline">\(n = 10\)</span> and thus one empirical distribution using <code>rbinom()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>my_samp <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">prob =</span> <span class="fl">0.4</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>my_samp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1 2 1 3 3 0 2 3 2 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(my_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>my_samp
0 1 2 3 
1 3 3 3 </code></pre>
</div>
</div>
<p>We obtain one draw from the sampling distribution of <span class="math inline">\(\Xbar_n\)</span> by taking the mean of this sample:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(my_samp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.8</code></pre>
</div>
</div>
<p>If we had a different sample,however, we would obtain a different empirical distribution and thus get a different estimate of the sample mean:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>my_samp2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">prob =</span> <span class="fl">0.4</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(my_samp2) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.6</code></pre>
</div>
</div>
<p>The sampling distribution is the distribution of these sample means across repeated sampling.</p>
</div>
</section>
<section id="finite-sample-properties-of-estimators" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="finite-sample-properties-of-estimators"><span class="header-section-number">2.9</span> Finite-sample properties of estimators</h2>
<p>As discussed in our introduction to estimators, their usefulness depends on how well they help us learn about the quantity of interest. If we get an estimate <span class="math inline">\(\widehat{\theta} = 1.6\)</span>, we would like to know that this is “close” to the true parameter <span class="math inline">\(\theta\)</span>. The sampling distribution is key to answering these questions. Intuitively, we would like the sampling distribution of <span class="math inline">\(\widehat{\theta}_n\)</span> to be as tightly clustered around the true <span class="math inline">\(\theta\)</span> as possible. Here, though, we run into a problem: the sampling distribution depends on the population distribution since it is about repeated samples of the data from that distribution filtered through the function <span class="math inline">\(\theta()\)</span>. Since <span class="math inline">\(F\)</span> is unknown, this implies that the sampling distribution will also usually be unknown.</p>
<p>Even though we cannot precisely pin down the entire sampling distribution, we can use assumptions to derive specific properties of the sampling distribution that are useful in comparing estimators. Note that the properties here will be very similar to the</p>
<section id="bias" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="bias"><span class="header-section-number">2.9.1</span> Bias</h3>
<p>The first property of the sampling distribution concerns its central tendency. In particular, we define the <strong>bias</strong> (or <strong>estimation bias</strong>) of estimator <span class="math inline">\(\widehat{\theta}\)</span> for parameter <span class="math inline">\(\theta\)</span> as <span class="math display">\[
\text{bias}[\widehat{\theta}] = \E[\widehat{\theta}] - \theta,
\]</span> which is the difference between the mean of the estimator (across repeated samples) and the true parameter. All else equal, we would like the estimation bias to be as small as possible. The smallest possible bias, obviously, is 0, and we define an <strong>unbiased estimator</strong> as one with <span class="math inline">\(\text{bias}[\widehat{\theta}] = 0\)</span> or equivalently, <span class="math inline">\(\E[\widehat{\theta}] = \theta\)</span>.</p>
<p>However, all else is not always equal, and unbiasedness is not a property to which we should become overly attached. Many biased estimators have other attractive properties, and many popular modern estimators are biased.</p>
<div id="exm-mean-unbiased" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Unbiasedness of the sample mean)</strong></span> The sample mean is unbiased for the population mean when the data is iid and <span class="math inline">\(\E|X| &lt; \infty\)</span>. In particular, we apply the rules of expectations: <span class="math display">\[\begin{aligned}
\E\left[ \Xbar_n \right] &amp;= \E\left[\frac{1}{n} \sum_{i=1}^n X_i\right] &amp; (\text{definition of } \Xbar_n) \\
&amp;= \frac{1}{n} \sum_{i=1}^n \E[X_i] &amp; (\text{linearity of } \E)\\
&amp;= \frac{1}{n} \sum_{i=1}^n \mu &amp; (X_i \text{ identically distributed})\\
&amp;= \mu.
\end{aligned}\]</span> Notice that we only used the “identically distributed” part of iid. Independence is not needed.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Properties like unbiasedness might only hold for a subset of DGPs. For example, we just showed that the sample mean is unbiased but only when the population mean is finite. There are probability distributions like the Cauchy that are not finite and where the expected value diverges. Thus, here we are dealing with a restricted class of DGPs that rules out such distributions. This is sometimes formalized by defining a class <span class="math inline">\(\mathcal{F}\)</span> of distributions; unbiasedness might hold in that class if it is unbiased for all <span class="math inline">\(F \in \mathcal{F}\)</span>.</p>
</div>
</div>
</section>
</section>
<section id="question-5-uncertainty" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="question-5-uncertainty"><span class="header-section-number">2.10</span> Question 5: Uncertainty</h2>
<p>The spread of the sampling distribution is also important. We define the <strong>sampling variance</strong> as the variance of an estimator’s sampling distribution, <span class="math inline">\(\V[\widehat{\theta}]\)</span>, which measures how spread out the estimator is around its mean. For an unbiased estimator, lower sampling variance implies the distribution of <span class="math inline">\(\widehat{\theta}\)</span> is more concentrated around the true value of the parameter.</p>
<div id="exm-mean-var" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 (Sampling variance of the sample mean)</strong></span> We can prove that the sampling variance of the sample mean of iid data for all <span class="math inline">\(F\)</span> such that <span class="math inline">\(\V[X_i]\)</span> is finite (more precisely, <span class="math inline">\(\E[X_i^2] &lt; \infty\)</span>)</p>
<p><span class="math display">\[\begin{aligned}
  \V\left[ \Xbar_n \right] &amp;= \V\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] &amp; (\text{definition of } \Xbar_n) \\
                           &amp;= \frac{1}{n^2} \V\left[ \sum_{i=1}^n X_i \right] &amp; (\text{property of } \V) \\
                           &amp;= \frac{1}{n^2} \sum_{i=1}^n \V[X_i] &amp; (\text{independence}) \\
                           &amp;= \frac{1}{n^2} \sum_{i=1}^n \sigma^2 &amp; (X_i \text{ identically distributed}) \\
                           &amp;= \frac{\sigma^2}{n}
\end{aligned}\]</span></p>
</div>
<p>As we discussed before, an alternative measure of spread for any distribution is the standard deviation, which is on the same scale as the original random variable. The standard deviation of the sampling distribution of <span class="math inline">\(\widehat{\theta}\)</span> is known as the <strong>standard error</strong> of <span class="math inline">\(\widehat{\theta}\)</span>: <span class="math inline">\(\se(\widehat{\theta}) = \sqrt{\V[\widehat{\theta}]}\)</span>.</p>
<p>Given the above derivation, the standard error of the sample mean under iid sampling is <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
<section id="mean-squared-error" class="level3" data-number="2.10.1">
<h3 data-number="2.10.1" class="anchored" data-anchor-id="mean-squared-error"><span class="header-section-number">2.10.1</span> Mean squared error</h3>
<p>Bias and sampling variance measure two properties of “good” estimators because they capture the fact that we want the estimator to be as close as possible to the true value. One summary measure of the quality of an estimator is the <strong>mean squared error</strong> or <strong>MSE</strong>, which is<br>
<span class="math display">\[
\text{MSE} = \E[(\widehat{\theta}_n-\theta)^2].
\]</span> We would ideally have this be as small as possible!</p>
<p>The MSE also relates to the bias and the sampling variance (provided it is finite) via the following decomposition result: <span id="eq-mse-decomposition"><span class="math display">\[
\text{MSE} = \text{bias}[\widehat{\theta}_n]^2 + \V[\widehat{\theta}_n]
\tag{2.1}\]</span></span> This decomposition implies that, for unbiased estimators, MSE is the sampling variance. It also highlights why we might accept some bias for significant reductions in variance for lower overall MSE.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="estimation_files/figure-html/mse-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Two sampling distributions</figcaption>
</figure>
</div>
</div>
</div>
<p>This figure shows the sampling distributions of two estimators: (1) <span class="math inline">\(\widehat{\theta}_a\)</span>, which is unbiased (centered on the true value <span class="math inline">\(\theta\)</span>) but with a high sampling variance, and (2) <span class="math inline">\(\widehat{\theta}_b\)</span>, which is slightly biased but with much lower sampling variance. Even though <span class="math inline">\(\widehat{\theta}_b\)</span> is biased, the probability of drawing a value close to the truth is higher than for <span class="math inline">\(\widehat{\theta}_a\)</span>. The MSE helps capture this balancing between bias and variance, and, indeed, in this case, <span class="math inline">\(MSE[\widehat{\theta}_b] &lt; MSE[\widehat{\theta}_a]\)</span>.</p>
</section>
</section>
<section id="summary" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.11</span> Summary</h2>
<p>In this chapter, we introduced <strong>model-based inference</strong>, in which we posit a probability model for the data-generating process. These models can be <strong>parametric</strong> in the sense that we specify the probability distribution of the data up to some parameters. They can also be <strong>semiparametric</strong> where we only specify certain features of the distribution such as a finite mean and variance. This chapter mostly focused on the latter, where we assumed the observed data were <strong>independent and identically distributed</strong> draws from a population distribution with finite mean and variance.</p>
<p>An <strong>estimator</strong> is a function of the data meant as a guess for some quantity of interest in the population. Because it is a function of random variables (the data), estimators are also random variables and have distributions, called <strong>sampling distributions</strong>, across repeated draws from the population. If this distribution is centered on the true value of the quantity of interest, we call the estimator <strong>unbiased</strong>. The variance of the sampling distribution, called the <strong>sampling variance</strong>, tells us how variable we should expect the estimator to be across draws from the population. The mean-squared error is an overall measure of the accuracy of an estimator that combines notions of bias and sampling variance.</p>
<p>We showed in this chapter that the sample mean is unbiased for the population mean and that the sampling variance of the sample mean is the ratio of the population variance to the sample size. We also saw that <strong>plug-in estimators</strong> are a powerful way of constructing estimators as functions of sample means. That said, the focus of this chapter was finite-sample properties (that is, properties that are true no matter the sample size). In the next chapter, we will derive even more powerful results using large-sample approximations.</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>1924 May 24, Boston Herald, Whiting’s Column: Tammany Has Learned That This Is No Time for Political Bosses, Quote Page 2, Column 1, Boston, Massachusetts. (GenealogyBank)<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./design.html" class="pagination-link" aria-label="Design-based Inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Design-based Inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./asymptotics.html" class="pagination-link" aria-label="Asymptotics">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mattblackwell/gov2002-book/edit/main/estimation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>