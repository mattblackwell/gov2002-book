<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A User’s Guide to Statistical Inference and Regression - 6&nbsp; Linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./05_confidence_intervals.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_estimation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hypothesis_tests.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis Tests</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_confidence_intervals.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Confidence intervals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_linear_model.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-do-we-need-models" id="toc-why-do-we-need-models" class="nav-link active" data-scroll-target="#why-do-we-need-models"><span class="toc-section-number">6.1</span>  Why do we need models?</a></li>
  <li><a href="#population-linear-regression" id="toc-population-linear-regression" class="nav-link" data-scroll-target="#population-linear-regression"><span class="toc-section-number">6.2</span>  Population linear regression</a>
  <ul class="collapse">
  <li><a href="#projection-error" id="toc-projection-error" class="nav-link" data-scroll-target="#projection-error"><span class="toc-section-number">6.2.1</span>  Projection error</a></li>
  </ul></li>
  <li><a href="#linear-cefs-without-assumptions" id="toc-linear-cefs-without-assumptions" class="nav-link" data-scroll-target="#linear-cefs-without-assumptions"><span class="toc-section-number">6.3</span>  Linear CEFs without assumptions</a></li>
  <li><a href="#partitioned-regression" id="toc-partitioned-regression" class="nav-link" data-scroll-target="#partitioned-regression"><span class="toc-section-number">6.4</span>  Partitioned regression</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias"><span class="toc-section-number">6.5</span>  Omitted variable bias</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Unif}{\text{Unif}}
\newcommand{\se}{\textsf{se}}
\newcommand{\au}{\underline{a}}
\newcommand{\du}{\underline{d}}
\newcommand{\Au}{\underline{A}}
\newcommand{\Du}{\underline{D}}
\newcommand{\xu}{\underline{x}}
\newcommand{\Xu}{\underline{X}}
\newcommand{\Yu}{\underline{Y}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\boldsymbol}
\newcommand{\U}{\mb{U}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\bbL}{\mathbb{L}}
\renewcommand{\u}{\mb{u}}
\renewcommand{\v}{\mb{v}}
\newcommand{\M}{\mb{M}}
\newcommand{\X}{\mb{X}}
\newcommand{\Xmat}{\mathbb{X}}
\newcommand{\bfx}{\mb{x}}
\newcommand{\y}{\mb{y}}
\renewcommand{\b}{\mb{\beta}}
\newcommand{\e}{\bs{\epsilon}}
\newcommand{\bhat}{\widehat{\mb{\beta}}}
\newcommand{\XX}{\Xmat'\Xmat}
\newcommand{\XXinv}{\left(\XX\right)^{-1}}
\newcommand{\hatsig}{\hat{\sigma}^2}
\newcommand{\red}[1]{\textcolor{red!60}{#1}}
\newcommand{\indianred}[1]{\textcolor{indianred}{#1}}
\newcommand{\blue}[1]{\textcolor{blue!60}{#1}}
\newcommand{\dblue}[1]{\textcolor{dodgerblue}{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\inprob}{\overset{p}{\to}}
\newcommand{\indist}{\overset{d}{\to}}
\newcommand{\eframe}{\end{frame}}
\newcommand{\bframe}{\begin{frame}}
\newcommand{\R}{\textsf{\textbf{R}}}
\newcommand{\Rst}{\textsf{\textbf{RStudio}}}
\newcommand{\rfun}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\rpack}[1]{\textbf{#1}}
\newcommand{\rexpr}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\filename}[1]{\texttt{\color{blue}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Linear regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Regression refers to a collection of tools for assessing the relationship between a <strong>outcome variable</strong>, <span class="math inline">\(Y_i\)</span>, and a set of <strong>covariates</strong>, <span class="math inline">\(\X_i\)</span>. In particular, these tools focus on showing how the conditional mean of <span class="math inline">\(Y_i\)</span> varies as a function <span class="math inline">\(\X_i\)</span>. For example, we may want to know how wait voting poll wait times vary as a function of some socioeconomic features of the precinct like income and racial composition. This is usually accomplished by estimating the <strong>regression function</strong> or <strong>conditional expectation function</strong> (CEF) of the outcome given the covariates, <span class="math display">\[
\mu(\bfx) = \E[Y_i \mid \X_i = \bfx].
\]</span> Why is estimation and inference for this regression function special? Why can’t we just use the approaches we have seen for the mean, variance, covariance, and so on? The basic problem with the CEF is that there may be many, many values <span class="math inline">\(\bfx\)</span> that can occur and so many, many different conditional expectations that we will need to estimate. In fact, if any variable in <span class="math inline">\(\X_i\)</span> is continuous, then there will be an infinite number of possible values of <span class="math inline">\(\bfx\)</span> that we need to estimate. Because this problem gets worse as we add covariates to <span class="math inline">\(\X_i\)</span>, this is sometimes referred to as the <strong>curse of dimensionality</strong>. How can we resolve this with our measely finite data?</p>
<p>In this chapter, we are going to explore two different ways of “solving” the curse of dimensionality: assuming it away and changing the quantity of interest to something easier to estimate.</p>
<p>Regression is so ubiquitous in so many scientific fields that it has a lot of acquired notational baggage. In particular, the labels of the <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\X_i\)</span> varies greatly:</p>
<ul>
<li>The outcome can also be called: the response variable, the dependent variable, the labels (in machine learning), the left-hand side variable, or the regressand.</li>
<li>The covariates are also called: the explanatory variables, the independent variables, the predictors, the regressors, inputs, or features.</li>
</ul>
<section id="why-do-we-need-models" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="why-do-we-need-models"><span class="header-section-number">6.1</span> Why do we need models?</h2>
<p>At first glance, the connection between the CEF and parametric models might be hazy. For example, imagine we are interested in estimating the average poll wait times (<span class="math inline">\(Y_i\)</span>) for Black voters (<span class="math inline">\(X_i = 1\)</span>) versus non-Black voters (<span class="math inline">\(X_i=0\)</span>). In that case, there are two parameters to estimate, <span class="math display">\[
\mu(1) = \E[Y_i \mid X_i = 1] \quad \text{and}\quad \mu(0) = \E[Y_i \mid X_i = 0],
\]</span> which we could estimate by using the plug-in estimators that replace the population averages with their sample counterparts, <span class="math display">\[
\widehat{\mu}(1) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 1)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 1)} \qquad \widehat{\mu}(0) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 0)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 0)}.
\]</span> These are just the sample averages of the wait times for Black and non-Black voters, respectively. And because the race variable here is discrete, this basically mimics the situation of estimating a single mean, just within subpopulations defined by race in this case. The same logic would apply if we had <span class="math inline">\(k\)</span> racial categories: we would have <span class="math inline">\(k\)</span> conditional expectations to estimate and <span class="math inline">\(k\)</span> (conditional) sample means.</p>
<p>Now imagine that we want to know how the average poll wait time varies as a function of income, so that <span class="math inline">\(X_i\)</span> is (essentially) continuous. Now we have a different conditional expectation for every possible dollar amount from 0 to Bill Gates’s income. Imagine we pick particular income, $42,238, and so we are interested in the conditional expectation <span class="math inline">\(\mu(42,238)= \E[Y_{i}\mid X_{i} = 42,238]\)</span>. We could use the same plug-in estimator in the discrete case, <span class="math display">\[
\widehat{\mu}(42,238) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 42,238)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 42,238)}.
\]</span> What is the problem with this estimator? In all likelihood there are 0 units in any particular dataset that have that exact income, meaning this estimator is undefined (we would be dividing by zero).</p>
<p>One solution to this problem is to use <strong>subclassification</strong> and turn the continuous variable into a discrete one and proceed with the discrete approach above. We might group incomes into $25,000 bins and then calculate the average wait times of anyone between, say, $25,000 and $50,000 income. When we make this estimator switch for pragmatic purposes, we need to connect it back to the DGP of interest somehow. We could <strong>assume</strong> that the CEF of interest only depends on these binned means, which would mean we have:<br>
<span class="math display">\[
\mu(x) =
\begin{cases}
  \E[Y_{i} \mid 0 \leq X_{i} &lt; 25,000]  &amp;\text{if }  0 \leq x &lt; 25,000 \\
  \E[Y_{i} \mid 25,000 \leq X_{i} &lt; 50,000]  &amp;\text{if }  25,000 \leq x &lt; 50,000\\
  \E[Y_{i} \mid 50,000 \leq X_{i} &lt; 100,000]  &amp;\text{if }  50,000 \leq x &lt; 100,000\\
  \vdots \\
  \E[Y_{i} \mid 200,000 \leq X_{i}]  &amp;\text{if }  200,000 \leq x\\
\end{cases}
\]</span> This assumes, perhaps incorrectly, that the average wait time does not vary within the bins. <a href="#fig-cef-binned">Figure&nbsp;<span>6.1</span></a> shows a hypothetical joint distribution between income and wait times with the true CEF, <span class="math inline">\(\mu(x)\)</span> shown in red. The figure also shows the bins created by subclassification and the implied CEF if we assume bin-constant means in blue. We can see that blue function approximates the true CEF but deviates from it especially close to the bin edges. The trade off is that once we make the assumption, we only have to estimate one mean for every bin, rather than an infinite number of means for each possible income.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-cef-binned" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06_linear_model_files/figure-html/fig-cef-binned-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6.1: Hypothetical joint distribution of income and poll wait times (contour plot), conditional expectation function (red), and the conditional expectation of the binned income (blue).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Similarly, we could <strong>assume</strong> that the CEF follows a simple functional form like a line, <span class="math display">\[
\mu(x) = \E[Y_{i}\mid X_{i} = x] = \beta_{0} + \beta_{1} x.
\]</span> This reduces our infinite number of unknowns (the conditional mean at every possible income) to just two unknowns, the slope and intercept. As we will see, we can use the standard ordinary least squares to estimate these parameters. Notice again, though, that if the true CEF is nonlinear this assumption is incorrect, any estimate based off this assumption might be biased or even inconsistent.</p>
<p>We call the binning and linear assumptions on <span class="math inline">\(\mu(x)\)</span> <strong>functional form</strong> assumptions because restrict the class of functions that <span class="math inline">\(\mu(x)\)</span> can take. While powerful, these types of assumptions can muddy the roles of defining the quantity of interest and estimation. If our estimator <span class="math inline">\(\widehat{\mu}(x)\)</span> performs poorly, it will be difficult to tell if this is because the estimator is flawed in some way or because our functional form assumptions are incorrect.</p>
<p>To help clarify these issues, we will pursue a different approach: understanding what linear regression can estimate well under minimal assumptions and then investigating how well this estimand approximates the true CEF.</p>
</section>
<section id="population-linear-regression" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="population-linear-regression"><span class="header-section-number">6.2</span> Population linear regression</h2>
<p>Let’s set aside the idea of the conditional expectation function and instead focus on finding the <strong>linear</strong> function of <span class="math inline">\(\X_i\)</span> that best predicts the outcome. Remember that a linear function can be written <span class="math display">\[
\bfx'\b = x_{1}\beta_{1} + x_{2}\beta_{2} + \cdots + X_{k}\beta_{k}.
\]</span> We will define the <strong>best linear predictor</strong> (BLP) to be <span class="math display">\[
\bbL[Y_{i} \mid \X_{i}] =\X_{i}'\b, \quad \text{where}\quad \b = \argmin_{\mb{b} \in \real^k}\; \E\left[ \left( Y_{i} - \mb{X}_{i}'\b \right)^2 \right]
\]</span> The BLP function <span class="math inline">\(\bbL[Y \mid \X]\)</span> is the linear function of the covariates that minimizes the mean squared prediction errors, where we are averaging over the joint distribution of the data. If we have a single covariate, this would be the population line of best fit. Again, this function is a feature of the joint distribution of the data—the DGP—and so represents something that we would like to learn about with our sample. It is an alternative to the CEF for summarizing the relationship between the outcome and the covariate, though we will see that they will sometimes be equal to each other.</p>
<!-- TODO: make this #assumption -->
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Best linear projection assumptions
</div>
</div>
<div class="callout-body-container callout-body">
<p>Without some assumptions on the joint distribution of the data, The following “regularity conditions” will ensure the existence of the BLP:</p>
<ol type="1">
<li><span class="math inline">\(\E[Y^2] &lt; \infty\)</span> (outcome has finite mean/variance)</li>
<li><span class="math inline">\(\E\Vert \mb{X} \Vert^2 &lt; \infty\)</span> (<span class="math inline">\(\mb{X}\)</span> has finite means/variances/covariances)</li>
<li><span class="math inline">\(\mb{Q}_{\mb{XX}} = \E[\mb{XX}']\)</span> is positive definite (columns of <span class="math inline">\(\X\)</span> are linearly independent)<br>
</li>
</ol>
</div>
</div>
<p>Under these assumption, it is possible to derive a closed-form expression for the <strong>population coefficients</strong> <span class="math inline">\(\b\)</span> using matrix calculus. To set up the optimization problem, we will find the first order condition my taking the derivative of the expectation of the squared errors. First, let’s take derivative of the squared prediction errors using the chain rule: <span class="math display">\[
\begin{aligned}
  \frac{\partial}{\partial \mb{b}^{'}}\left(Y_{i} - \X_{i}'\mb{b}\right)^{2}
  &amp;= 2\left(Y_{i} - \X_{i}'\mb{b}\right)\frac{\partial}{\partial \mb{b}'}(Y_{i} - \X_{i}'\mb{b})  \\
  &amp;= -2\left(Y_{i} - \X_{i}'\mb{b}\right)\X_{i} \\
  &amp;= -2\left(\X_{i}Y_{i} - \X_{i}\X_{i}'\mb{b}\right)
\end{aligned}
\]</span> We can now plug this into the expectation to get the first-order condition and solve for <span class="math inline">\(\b\)</span>, <span class="math display">\[
\begin{aligned}
  0 &amp;= -2\E[\X_{i}Y_{i} - \X_{i}\X_{i}'\mb{b}] \\
  \E[\X_{i}\X_{i}'] \b &amp;= \E[\X_{i}Y_{i}],
\end{aligned}
\]</span> which implies the population coefficients are <span class="math display">\[
\b = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] = \mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y}
\]</span> We now have an expression for the coefficients for the population best linear predictor in terms of the joint distribution <span class="math inline">\((Y_{i}, \X_{i})\)</span>. There are a couple facts that might be useful for reasoning about this expression. Recall that <span class="math inline">\(\mb{Q}_{\mb{XX}} = \E[\X_{i}\X_{i}']\)</span> is a <span class="math inline">\(k\times k\)</span> matrix and <span class="math inline">\(\mb{Q}_{\X Y} = \E[\X_{i}Y_{i}]\)</span> is a <span class="math inline">\(k\times 1\)</span> column vector, which implies that <span class="math inline">\(\b\)</span> is also a <span class="math inline">\(k \times 1\)</span> column vector.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Intuitively, what is happening in the expression for the population regression coefficients? It is helpful to separate out the intercept or constant term so that we have <span class="math display">\[
Y_{i} = \beta_{0} + \X'\b + e_{i},
\]</span> so <span class="math inline">\(\b\)</span> refers to just the coefficients on each of the covariates. In this case, we can write the coefficients in more interpretable way: <span class="math display">\[
\b = \V[\X]^{-1}\text{Cov}(\X, Y), \qquad \beta_0 = \mu_Y - \mb{\mu}'_{\mb{X}}\b
\]</span></p>
<p>Thus, the population coefficients take the covariance between the outcome and the covariates and “divide” it by information about variances and covariances of the covariates. The intercept recenters the regression so that projection errors are mean zero.</p>
</div>
</div>
<p>With an expression for the population linear regression coefficients, we can write the linear projection as <span class="math display">\[
\bbL[Y_{i} \mid \X_{i}] = \X_{i}'\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] = \X_{i}'\mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y}
\]</span></p>
<section id="projection-error" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="projection-error"><span class="header-section-number">6.2.1</span> Projection error</h3>
<p>The <strong>projection error</strong> is the difference between the actual value of <span class="math inline">\(Y_i\)</span> and the projection, <span class="math display">\[
e_{i} = Y_{i} - \bbL[Y_{i} \mid \X_{i}] =  Y_i - \X_{i}'\b,
\]</span> where it is hopefully clear that we have made no assumptions about this error yet. It is simply the prediction error if we used the linear projection to predict the outcome. Rewriting this definition, we can see that we can always write the outcome as the linear projection plus the projection error, <span class="math display">\[
Y_{i} = \X_{i}'\b + e_{i}.
\]</span> Notice that this looks suspiciously like we have made a linearity assumption on the CEF or on the relationship between the outcome and the covariates. But we haven’t, we have just used the definition of the projection error to write a statement that is tautological: <span class="math display">\[
Y_{i} = \X_{i}'\b + e_{i} = \X_{i}'\b + Y_{i} - \X_{i}'\b = Y_{i}.
\]</span> The key difference between this representation and the usual linear model assumption is what properties <span class="math inline">\(e_{i}\)</span> possesses.</p>
<p>One key property of the projection errors is that when the covariate vector includes an “intercept” or constant term, the projection errors are uncorrelated with the covariates. To see this, we first note that <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span> since <span class="math display">\[
\begin{aligned}
  \E[\X_{i}e_{i}] &amp;= \E[\X_{{i}}(Y_{i} - \X_{i}'\b)] \\
                  &amp;= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\b \\
                  &amp;= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] \\
  &amp;= \E[\X_{i}Y_{i}] - \E[\X_{i}Y_{i}] = 0
\end{aligned}
\]</span> Thus, for every <span class="math inline">\(X_{ij}\)</span> in <span class="math inline">\(\X_{i}\)</span>, we have <span class="math inline">\(\E[X_{ij}e_{i}] =0\)</span>. If one of the entries in <span class="math inline">\(\X_i\)</span> is a constant 1, then this also implies that <span class="math inline">\(\E[e_{i}] =0\)</span>. Together, these facts imply that the projection error is uncorrelated with each <span class="math inline">\(X_{ij}\)</span>, since <span class="math display">\[
\cov(X_{ij}, e_{i}) = \E[X_{ij}e_{i}] - \E[X_{ij}]\E[e_{i}] = 0 - 0 = 0
\]</span> Notice that we still have made no assumptions about these projection errors except some mild regularity conditions on the joint distribution of the outcome and covariates. Thus, in very general settings, we can write the linear projection model <span class="math inline">\(Y_i = \X_i'\b + e_i\)</span> where <span class="math inline">\(\b = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]\)</span> and conclude that <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span> by definition not by assumption.</p>
<p>The projection error is uncorrelated with the covariates, so does this mean that the CEF is linear? Unfortunately, no because recall that while independence implies uncorrelated, the reverse does not hold. So when we look at the CEF, we have <span class="math display">\[
\E[Y_{i} \mid \X_{i}] = \X_{i}'\b + \E[e_{i} \mid \X_{i}],
\]</span> and the last term <span class="math inline">\(\E[e_{i} \mid \X_{i}]\)</span> would only be 0 if the errors were independent of the covariates so <span class="math inline">\(\E[e_{i} \mid \X_{i}] = \E[e_{i}] = 0\)</span>. But nowhere in the linear projection model did we assume this. So while we can (almost) always write the outcome as <span class="math inline">\(Y_i = \X_i'\b + e_i\)</span> and have those projections error be uncorrelated with the covariates, it will require additional assumptions to ensure that the true CEF is in fact linear <span class="math inline">\(\E[Y_{i} \mid \X_{i}] = \X_{i}'\b\)</span>.</p>
<p>Let’s take a step back. What have we shown here? In a nutshell, we have shown that under very general conditions, a population linear regression exists and we can write the coefficients of that population linear regression as a function of expectations of the joint distribution of the data. Why do we care about this? It turns out that the ordinary least squares estimator, the workhorse regression estimator, targets this quantity of interest in large samples, regardless of whether the true CEF is linear or not. This means that even when a linear CEF assumption is incorrect, OLS still targets a perfectly valid quantity of interest: the coefficients from this population linear projection.</p>
</section>
</section>
<section id="linear-cefs-without-assumptions" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="linear-cefs-without-assumptions"><span class="header-section-number">6.3</span> Linear CEFs without assumptions</h2>
<p>What is the relationship between the best linear predictor (which we just saw exists very generally) and the CEF? To draw the connection, remember a key property of the conditional expectation: it is the function of <span class="math inline">\(\X_i\)</span> that best predicts <span class="math inline">\(Y_{i}\)</span>. The population regression was the best <strong>linear</strong> predictor, but the CEF is the best predictor among all functions of <span class="math inline">\(\X_{i}\)</span>, linear or nonlinear, that are nicely behaved. In particular, if we label <span class="math inline">\(L_2\)</span> be the set of all functions of the covariates <span class="math inline">\(g()\)</span> such that they are finite squared expectation, <span class="math inline">\(\E[g(\X_{i})^{2}] &lt; \infty\)</span>, then we can show that the CEF has the lowest squared prediction error in this class of functions: <span class="math display">\[
\mu(\X) = \E[Y_{i} \mid \X_{i}] = \argmin_{g(\X_i) \in L_2}\; \E\left[(Y_{i} - f(\X_{i}))^{2}\right],
\]</span></p>
<p>So we have established that the CEF is the best predictor and the population linear regression <span class="math inline">\(\bbL[Y_{i}\mid \X_{i}]\)</span> is the best linear predictor. These two facts allow us to connect the CEF and the population regression.</p>
<div id="thm-cef-blp" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1 </strong></span>If <span class="math inline">\(\mu(\X_{i})\)</span> is a linear function of <span class="math inline">\(\X_i\)</span>, then <span class="math inline">\(\mu(\X_{i}) = \bbL[Y_{i}\mid \X_{i}] = \X_i'\b\)</span>.</p>
</div>
<p>This theorem says that if the true CEF is linear, then it is equal to the population regression. The proof of this is straightforward: the CEF is the best predictor, so if it is linear, it must also be the best linear predictor.</p>
<p>In general, we are usually in the business of learning about the CEF so we are unlikely to know if it truly is linear or not. In some situations, however, we can show that the CEF is linear without any additional assumptions. These will be situations when the covariates take on a finite number of possible values. Suppose that we are interested in the CEF of poll wait times for Black (<span class="math inline">\(X_i = 1\)</span>) vs non-Black (<span class="math inline">\(X_i = 0\)</span>) voters. In this case, there are two possible values of the CEF, <span class="math inline">\(\mu(1) = \E[Y_{i}\mid X_{i}= 1]\)</span>, the average wait time for Black voters, and <span class="math inline">\(\mu(0) = \E[Y_{i}\mid X_{i} = 0]\)</span>, the average wait time for non-Black voters. Notice that we can write the CEF as <span class="math display">\[
\mu(x) = x \mu(1) + (1 - x) \mu(0) = \mu(0) + x\left(\mu(1) - \mu(0)\right)= \beta_0 + x\beta_1,
\]</span> which is clearly a linear function of <span class="math inline">\(x\)</span>. Based on this derivation, we can see that the coefficients of this linear CEF have a clear interpretation:</p>
<ul>
<li><span class="math inline">\(\beta_0 = \mu(0)\)</span>: the expected wait time for a Black voter.</li>
<li><span class="math inline">\(\beta_1 = \mu(1) - \mu(0)\)</span>: the difference in average wait times between Black and non-Black voters. Notice that it matters how <span class="math inline">\(X_{i}\)</span> is defined here since the intercept will always be the average outcome when <span class="math inline">\(X_i = 0\)</span> and the slope will always be the difference in means between the <span class="math inline">\(X_i = 1\)</span> group and the <span class="math inline">\(X_i = 0\)</span> group.</li>
</ul>
<p>What about a categorical coviarate with more than two levels? For instance, we might be interested in wait times by party identification, where <span class="math inline">\(X_i = 1\)</span> indicates Democratic voters, <span class="math inline">\(X_i = 2\)</span> indicates Republican voters, and <span class="math inline">\(X_i = 3\)</span> indicates independent voters. How can we possible write the CEF of wait times as a linear function of this variable? That would assume that the difference between Democrats and Republicans is the same as for Independents and Republicans. With more than two levels, a categorical variable is better represented as a vector of binary variables, <span class="math inline">\(\X_i = (X_{i1}, X_{i2})\)</span>, where <span class="math display">\[
\begin{aligned}
  X_{{i1}} &amp;= \begin{cases}
                1&amp;\text{if Republican} \\
                   0 &amp; \text{if not Republican}
              \end{cases} \\
X_{{i2}} &amp;= \begin{cases}
                1&amp;\text{if independent} \\
                   0 &amp; \text{if not independent}
              \end{cases} \\
\end{aligned}
\]</span> Clearly, these two indicator variables encode the same information as the original three-level variable, <span class="math inline">\(X_{i}\)</span>. If I know the values of <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span>, I know exactly what party to which <span class="math inline">\(i\)</span> belongs. Thus, the CEFs with repect to <span class="math inline">\(X_i\)</span> and the pair of indicator variables, <span class="math inline">\(\X_i\)</span>, is exactly the same, but the latter admits a very nice linear representation, <span class="math display">\[
\E[Y_i \mid X_{i1}, X_{i2}] = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2},
\]</span> where</p>
<ul>
<li><span class="math inline">\(\beta_0 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\)</span> is the average wait time for the group who does not get an indicator variable (Democrats in this case).</li>
<li><span class="math inline">\(\beta_1 = \E[Y_{i} \mid X_{i1} = 1, X_{i2} = 0] - \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\)</span> is the difference in means between Republican voters and Democratic voters, or the difference between the first indicator group and the baseline group.</li>
<li><span class="math inline">\(\beta_2 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 1] - \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\)</span> is the difference in means between independent voters and Democratic voters, or the difference between the second indicator group and the baseline group.</li>
</ul>
<p>This approach generalizes easily to categorical variables with an arbitrary number of levels.</p>
<p>What have we shown? The CEF will be linear without additional assumptions when there is a categorical covariate. We can show that this continues to hold even when we have multiple categorical variables. Suppose now that we have two binary covariates: <span class="math inline">\(X_{i1}=1\)</span> indicating a Black voter and <span class="math inline">\(X_{i2} = 1\)</span> indicating an urban voter. With these two binary variables, there are 4 possible values of the CEF: <span class="math display">\[
\mu(x_1, x_2) = \begin{cases}
\mu_{00} &amp; \text{if } x_1 = 0 \text{ and } x_2 = 0 \text{ (non-Black, rural)} \\
  \mu_{10} &amp; \text{if }  x_1 = 1 \text{ and } x_2 = 0 \text{ (Black, rural)}\\
  \mu_{01} &amp; \text{if }  x_1 = 0 \text{ and } x_2 = 1 \text{ (non-Black, urban)}\\
\mu_{11} &amp; \text{if }  x_1 = 1 \text{ and } x_2 = 1 \text{ (Black, urban)}
\end{cases}
\]</span> We can write this as <span class="math display">\[
\mu(x_{1}, x_{2}) = (1 - x_{1})(1 - x_{2})\mu_{00} + x_{1}(1 -x_{2})\mu_{10} + (1-x_{1})x_{2}\mu_{01} + x_{1}x_{2}\mu_{11},
\]</span> which we can rewrite as <span class="math display">\[
\mu(x_1, x_2) = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3,
\]</span> where</p>
<ul>
<li><span class="math inline">\(\beta_0 = \mu_{00}\)</span>: average wait times for rural non-Black voters.</li>
<li><span class="math inline">\(\beta_1 = \mu_{10} - \mu_{00}\)</span>: difference in means for rural Black vs rural non-Black voters..</li>
<li><span class="math inline">\(\beta_2 = \mu_{01} - \mu_{00}\)</span>: difference in means for urban non-Black vs rural non-Black voters.</li>
<li><span class="math inline">\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\)</span>: difference in urban racial difference vs rural racial difference.</li>
</ul>
<p>Thus, we can write the CEF with two binary covriataes as linear when the linear specification includes and multiplicative interaction between them (<span class="math inline">\(x_1x_2\)</span>). This holds for all pairs of binary covariates and we can generalize the interpretation of the coefficients in the CEF as</p>
<ul>
<li><span class="math inline">\(\beta_0 = \mu_{00}\)</span>: average outcome when both variables are 0.</li>
<li><span class="math inline">\(\beta_1 = \mu_{10} - \mu_{00}\)</span>: difference in average outcomes for the first covariate when second covariate is 0.</li>
<li><span class="math inline">\(\beta_2 = \mu_{01} - \mu_{00}\)</span>: difference in average outcomes for the second covariate when the first covariate is 0.</li>
<li><span class="math inline">\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\)</span>: change in the “effect” of the first (second) covariate when the second (first) covariate goes from 0 to 1.</li>
</ul>
<p>This result also generalizes to an arbitrary number of binary covariates. If we have <span class="math inline">\(p\)</span> binary covariates, then the CEF will be linear with all two-way interactions, <span class="math inline">\(x_1x_2\)</span>, all three-way interactions, <span class="math inline">\(x_1x_2x_3\)</span>, up to the <span class="math inline">\(p\)</span>-way interaction <span class="math inline">\(x_1\times\cdots\times x_p\)</span>. Furthermore, we can generalize to arbitrary numbers of categorical variables by expanding each categorical variable into a series of binary variables and then including all interactions between the resulting binary variables.</p>
<p>We have established that when we have a set of categorical covariates, the true CEF will be linear and we have seen the various ways to represent that CEF. Notice that when we actually use, for example, ordinary least squares, we are free to choose how to include our variables. That means that we could run a regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span> without an interaction term. This model will only be correct if <span class="math inline">\(\beta_3\)</span> is actually equal to 0 and so the interaction term is irrelevant. Because of this ability to choose our models, it’s helpful to have a language for models that capture the linear CEF appropriately. We call a model <strong>saturated</strong> if there as many coefficients as there are unique values of the CEF. A saturated model by its nature can always be written as a linear function without assumptions. The above examples show how to construct saturated models in various situations.</p>
</section>
<section id="partitioned-regression" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="partitioned-regression"><span class="header-section-number">6.4</span> Partitioned regression</h2>
<p>When we have a regression with two groups of covariates, <span class="math inline">\(\mb{V}_{i} = (\X_{i}, \mb{Z}_{i})\)</span>, we might want a formula for the coefficients on just <span class="math inline">\(\X_i\)</span>. Given the matrix algebra involved in the definition of the population regression coefficients, it isn’t clear how we might acheive this. It turns out we can obtain a formula for those coefficients from a set of two population regressions.</p>
<p>In particular, let <span class="math inline">\(\mb{\delta}' = (\b', \gamma')\)</span> be the coefficients the coefficients on the complete set of covariates, with <span class="math inline">\(\b\)</span> corresponding to <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(\mb{\gamma}\)</span> corresponding to <span class="math inline">\(\mb{Z}_i\)</span>. In other words, the linear projection in this case can be written as <span class="math display">\[
\bbL(Y_{i} \mid \mb{V}_{i})= \mb{V}_{i}'\mb{\delta} =  \X_{i}'\b + \mb{Z}_{i}'\mb{\gamma}.
\]</span> To obtain an expression for <span class="math inline">\(\b\)</span>, we can first think of a population regression of the covariates of interest <span class="math inline">\(\X_i\)</span> on the other covariates <span class="math inline">\(\mb{Z}_i\)</span>, <span class="math inline">\(\bbL(\X_{i} \mid \mb{Z}_{i})\)</span>. This looks somewhat odd because we have a vector as the dependent variable, but it represents the idea of running a separate regression of each variable in <span class="math inline">\(\X_i\)</span> on <span class="math inline">\(\mb{Z}_i\)</span>. So we end up with a matrix of coefficients instead of a vector (one column for each variable in <span class="math inline">\(\X_i\)</span>), even though the expression for the linear projection does not change much: <span class="math display">\[
\bbL(\X_{i} \mid \mb{Z}_{i}) = \mb{Z}_{i}'\left(\E[\mb{Z}_{i}\mb{Z}_{i}']\right)^{{-1}} \E[\mb{Z}_{i}'\X_{i}].
\]</span> These represent the best linear guess about the covariates of interest given the other covariates. Let’s define the prediction errors <span class="math inline">\(\widetilde{\X}_i = \X_i - \bbL(\X_{i} \mid \mb{Z}_{i})\)</span>. Because these are linear projection errors, we know from the above results that they are uncorrelated with <span class="math inline">\(\mb{Z}_i\)</span>, which geometrically means they are <strong>orthogonal</strong> to those covariates. Thus, we sometimes call the transformation <span class="math inline">\(A - \bbL(A \mid B)\)</span> <strong>orthogonalizing</strong> <span class="math inline">\(A\)</span> with respect to <span class="math inline">\(B\)</span>. The vector <span class="math inline">\(\widetilde{X}_i\)</span> captures the variation in <span class="math inline">\(\X_i\)</span> that is not capture by a linear function of <span class="math inline">\(\mb{Z}_i\)</span>. An amazing property of these errors is that when we project <span class="math inline">\(Y_i\)</span> onto just <span class="math inline">\(\widetilde{\X}_i\)</span>, we obtain the coefficients on <span class="math inline">\(\X_i\)</span> from the projection of <span class="math inline">\(Y_i\)</span> on both <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(\mb{Z}_i\)</span>: <span class="math display">\[
\b = \left( \E[\widetilde{\X}_{i}\widetilde{\X}_{i}'] \right)^{-1}\E[\widetilde{\X}_{i}'Y_{i}].
\]</span> The same result applies if we orthogonalize <span class="math inline">\(Y_i\)</span> with respect to <span class="math inline">\(\mb{Z}_i\)</span> as well,<br>
<span class="math display">\[
\b = \left( \E[\widetilde{\X}_{i}\widetilde{\X}_{i}'] \right)^{-1}\E[\widetilde{\X}_{i}'\widetilde{Y}_{i}],
\]</span> where <span class="math inline">\(\widetilde{Y}_i = Y_i - \bbL(Y_{i} \mid \mb{Z}_{i})\)</span>.</p>
<p>Why do care about this ability to partition the regression coefficients? One reason is that it allows us to derive an interpretable expression for each coefficient in the BLP. Let’s change our partition slightly so that <span class="math inline">\(\X_i = (X_{{ik}}, \X_{i,-k})\)</span>, where <span class="math inline">\(\X_{i,-k}\)</span> is the vector of covariates omitting the <span class="math inline">\(k\)</span>th entry (we assume that <span class="math inline">\(\X_{i,-k}\)</span> includes the constant term 1). Based on the above, we can define <span class="math inline">\(\widetilde{X}_{ik} = X_{ik} - \bbL(X_{ik}\mid \X_{i,-k})\)</span> as the <span class="math inline">\(k\)</span>th variable othorgonalized with respect to rest of the variables. Then the above partition results implies that <span class="math display">\[
\beta_k = \frac{\cov(Y_i, \widetilde{X}_{ik})}{\V[\widetilde{X}_{ik}]}.
\]</span> Thus, the population regression coefficient in the BLP is the covariance of the outcome and the orthogonalized variable divided by the variance of the orthogonalized value. More generally, this implies that we could use a scatterplot of the outcome and the orthogonalized covariate to explore the conditional relationship between these two variable in a more flexible or exploratory manner.</p>
</section>
<section id="omitted-variable-bias" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="omitted-variable-bias"><span class="header-section-number">6.5</span> Omitted variable bias</h2>
<p>In many situations, we might need to choose to include a variable in a regression or not, so it can be helpful to understand how this choice might affect the population coefficients on the other variables in a the regression. In particular, suppose we are considering whether to include some variable <span class="math inline">\(Z_i\)</span> in our regression where <span class="math inline">\(\X_i\)</span> represents the other covariates. Then we are deciding between the two projections <span class="math display">\[
\bbL[Y_i \mid \X_i, Z_i] = \X_i'\b + Z_i\gamma, \qquad \bbL[Y \mid \X] = \X_i'\mb{\delta},
\]</span> where we often refer to <span class="math inline">\(\bbL[Y_i \mid \X_i, Z_i]\)</span> as the long regression and <span class="math inline">\(\bbL[Y_i \mid \X_i]\)</span> as the short regression.</p>
<p>Can we relate the coefficients on <span class="math inline">\(\X_i\)</span> from these two regressions? It turns out that we can using two property that linear projections share with conditional expectations: the <strong>law of iterated projections</strong>, <span class="math display">\[
\bbL[Y_i \mid \X_i] = \bbL\left\{ \bbL[Y_i \mid \X_i, Z_i] \mid \X_i \right\},
\]</span> and that linear projection is linear, so that <span class="math inline">\(\bbL[A + B \mid C] = \bbL[A\mid C] + \bbL[B \mid C]\)</span>. Together, these imply <span class="math display">\[  
\begin{aligned}
\bbL[Y_i \mid \X_i] &amp;= \bbL[\X_i'\b + Z_i\gamma \mid \X_i] \\
&amp;= \bbL[\X_i\mid \X_i]'\b + \bbL[Z_i \mid \X_i]\gamma \\
&amp;= \X'_i\b + \bbL[Z_i \mid \X_i]\gamma
\end{aligned}
\]</span> We can write the regression/projection of <span class="math inline">\(Z_i\)</span> on <span class="math inline">\(\X_i\)</span> as <span class="math inline">\(\bbL[Z_i \mid \X_i]= \X_i'\mb{\pi}\)</span> and so <span class="math display">\[
\bbL[Y_i \mid \X_i] = \X_i'(\b + \mb{\pi}\gamma),
\]</span> which implies that the short coefficients can be written as <span class="math inline">\(\mb{\delta} = \b + \mb{\pi}\gamma\)</span>.</p>
<p>We can easily rewrite this to show that the difference between the coefficients in these two projections is <span class="math inline">\(\mb{\delta} - \b= \mb{\pi}\gamma\)</span> or the product of the coefficient on the “excluded” <span class="math inline">\(Z_i\)</span> and the coefficient of the included <span class="math inline">\(\X_i\)</span> on the excluded. This difference is usually referred to as the <strong>omitted variable bias</strong> of omitting <span class="math inline">\(Z_i\)</span> under the idea that <span class="math inline">\(\b\)</span> is the true target of inference. But the result is much broader than this since it just tells us how to relate the coefficients of two nested projection.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05_confidence_intervals.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Confidence intervals</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>