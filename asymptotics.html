<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Asymptotics – A User's Guide to Statistical Inference and Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./hypothesis_tests.html" rel="next">
<link href="./estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    macros: {
      RR: "{\\bf R}",
      bs: ["\\boldsymbol{#1}", 1],
      mb: ["\\mathbf{#1}", 1],
      E: "\\mathbb{E}",
      V: "\\mathbb{V}",
      P: "\\mathbb{P}",
      var: "\\text{var}",
      cov: "\\text{cov}",
      N: "\\mathcal{N}",
      Bern: "\\text{Bern}",
      Bin: "\\text{Bin}",
      Pois: "\\text{Pois}",
      Unif: "\\text{Unif}",
      se: "\\textsf{se}",
      U: "\\mb{U}",
      Xbar: "\\overline{X}",
      Ybar: "\\overline{Y}",
      real: "\\mathbb{R}",
      bbL: "\\mathbb{L}",
      u: "\\mb{u}",
      v: "\\mb{v}",
      M: "\\mb{M}",
      X: "\\mb{X}",
      Xmat: "\\mathbb{X}",
      bfx: "\\mb{x}",
      y: "\\mb{y}",
      bfbeta: "\\bs{\\beta}",
      e: "\\bs{\\epsilon}",
      bhat: "\\widehat{\\bs{\\beta}}",
      XX: "\\Xmat'\\Xmat",
      XXinv: "\\left(\\Xmat'\\Xmat\\right)^{-1}",
      hatsig: "\\widehat{\\sigma}^2",
      red: ["\\textcolor{red!60}{#1}", 1],
      indianred: ["\\textcolor{indianred}{#1}", 1],
      blue: ["\\textcolor{blue!60}{#1}", 1],
      dblue: ["\\textcolor{dodgerblue}{#1}", 1],
      indep: "\\perp\\!\\!\\!\\perp",
      inprob: "\\overset{p}{\\to}",
      indist: "\\overset{d}{\\to}",
      argmax: ["\\operatorname\{arg\,max\}"],
      argmin: ["\\operatorname\{arg\,min\}"]      
    }
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./design.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./asymptotics.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mattblackwell/gov2002-book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./users-guide.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Design-based Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis_tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Regression</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./least_squares.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols_properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3.1</span> Introduction</a></li>
  <li><a href="#convergence-of-deterministic-sequences" id="toc-convergence-of-deterministic-sequences" class="nav-link" data-scroll-target="#convergence-of-deterministic-sequences"><span class="header-section-number">3.2</span> Convergence of deterministic sequences</a></li>
  <li><a href="#convergence-in-probability-and-consistency" id="toc-convergence-in-probability-and-consistency" class="nav-link" data-scroll-target="#convergence-in-probability-and-consistency"><span class="header-section-number">3.3</span> Convergence in probability and consistency</a></li>
  <li><a href="#useful-inequalities" id="toc-useful-inequalities" class="nav-link" data-scroll-target="#useful-inequalities"><span class="header-section-number">3.4</span> Useful inequalities</a></li>
  <li><a href="#the-law-of-large-numbers" id="toc-the-law-of-large-numbers" class="nav-link" data-scroll-target="#the-law-of-large-numbers"><span class="header-section-number">3.5</span> The law of large numbers</a></li>
  <li><a href="#consistency-of-estimators" id="toc-consistency-of-estimators" class="nav-link" data-scroll-target="#consistency-of-estimators"><span class="header-section-number">3.6</span> Consistency of estimators</a></li>
  <li><a href="#convergence-in-distribution-and-the-central-limit-theorem" id="toc-convergence-in-distribution-and-the-central-limit-theorem" class="nav-link" data-scroll-target="#convergence-in-distribution-and-the-central-limit-theorem"><span class="header-section-number">3.7</span> Convergence in distribution and the central limit theorem</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="header-section-number">3.8</span> Confidence intervals</a>
  <ul class="collapse">
  <li><a href="#deriving-confidence-intervals" id="toc-deriving-confidence-intervals" class="nav-link" data-scroll-target="#deriving-confidence-intervals"><span class="header-section-number">3.8.1</span> Deriving confidence intervals</a></li>
  <li><a href="#interpreting-confidence-intervals" id="toc-interpreting-confidence-intervals" class="nav-link" data-scroll-target="#interpreting-confidence-intervals"><span class="header-section-number">3.8.2</span> Interpreting confidence intervals</a></li>
  </ul></li>
  <li><a href="#sec-delta-method" id="toc-sec-delta-method" class="nav-link" data-scroll-target="#sec-delta-method"><span class="header-section-number">3.9</span> Delta method</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3.10</span> Summary</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mattblackwell/gov2002-book/edit/main/asymptotics.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./design.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./asymptotics.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-asymptotics" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>Suppose we are still interested in estimating the proportion of citizens who prefer increasing legal immigration. Based on the last chapter, a good strategy would be to use the sample proportion of immigration supporters in a random sample of citizens. You would have good reason to be confident with this estimator, with its finite-sample properties like unbiasedness and a sampling variance. We call these “finite-sample” properties since they hold at any sample size—they are as true for random samples of size for <span class="math inline">\(n = 10\)</span> as they are for random samples of size <span class="math inline">\(n = 1,000,000\)</span>.</p>
<p>Finite-sample results, though, are of limited value because they only tell us about the center and spread of the sampling distribution of <span class="math inline">\(\Xbar_n\)</span>. Suppose we found that <span class="math inline">\(\Xbar_n = 0.47\)</span> or 47% of respondents in a single survey supported increasing immigration. We might want to know how plausible it would be for the true population proportion – which is distinct from the sample proportion – to be 50% or greater. Questions like this are critical for a decision maker and, to answer this, we need to know the (approximate) distribution of <span class="math inline">\(\Xbar_n\)</span> in addition to its mean and variance. We can often derive the exact distribution of an estimator if we are willing to make certain, sometimes strong assumptions about the underlying data (for example, if the population is normal, then the sample means will also be normal). Still, this approach is brittle: if our parametric assumption is false, we are back to square one.</p>
<p>In this chapter, we take a different approach by asking what happens to the sampling distribution of estimators as the sample size gets very large, which we refer to as <strong>asymptotic theory</strong>. While asymptotics will often simplify derivations, an essential point is that everything we do with asymptotics will be an approximation. No one ever has infinite data, but we hope that the approximations will be closer to the truth as our samples get larger.</p>
<p>Asymptotic results are key to modern statistical methods because many methods of quantifying uncertainty about estimates rely on asymptotic approximations. We will rely on the asymptotic results we derive in this chapter to estimate standard errors, construct confidence intervals, and perform hypothesis tests, all without assuming a fully parametric model.</p>
</section>
<section id="convergence-of-deterministic-sequences" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="convergence-of-deterministic-sequences"><span class="header-section-number">3.2</span> Convergence of deterministic sequences</h2>
<p>A helpful place to begin is by reviewing the basic idea of convergence in deterministic sequences from calculus:</p>
<div id="def-limit" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1</strong></span> A sequence <span class="math inline">\(\{a_n: n = 1, 2, \ldots\}\)</span> has the <strong>limit</strong> <span class="math inline">\(a\)</span> written <span class="math inline">\(a_n \rightarrow a\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span> or <span class="math inline">\(\lim_{n\rightarrow \infty} a_n = a\)</span> if for all <span class="math inline">\(\epsilon &gt; 0\)</span> there is some <span class="math inline">\(n_{\epsilon} &lt; \infty\)</span> such that for all <span class="math inline">\(n \geq n_{\epsilon}\)</span>, <span class="math inline">\(|a_n - a| \leq \epsilon\)</span>.</p>
</div>
<p>We say that <span class="math inline">\(a_n\)</span> <strong>converges</strong> to <span class="math inline">\(a\)</span> if <span class="math inline">\(\lim_{n\rightarrow\infty} a_n = a\)</span>. Basically, a sequence converges to a number if the sequence gets closer and closer to that number as the sequence goes on.</p>
<div id="exm-limit" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1</strong></span> One important sequence that arises often in statistics is <span class="math inline">\(1/n\)</span> as <span class="math inline">\(n\to\infty\)</span>. It may seem clear that this sequence converges to 0, but showing this using the formal definition of convergence is helpful.</p>
<p>Let us pick a specific value of <span class="math inline">\(\epsilon = 0.3\)</span>. Now we need to find an integer <span class="math inline">\(n_{\epsilon}\)</span> so that <span class="math inline">\(|1/n - 0| = 1/n \leq \epsilon\)</span> for all <span class="math inline">\(n \geq n_{\epsilon}\)</span>. Clearly, if <span class="math inline">\(\epsilon = 0.3\)</span>, then <span class="math inline">\(n_{\epsilon} = 4\)</span> would satisfy this condition since <span class="math inline">\(1/4 \leq 0.3\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="asymptotics_files/figure-html/sequence-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>More generally, for any <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(n \geq 1/\epsilon\)</span> implies <span class="math inline">\(1/n \leq \epsilon\)</span>. Thus, setting <span class="math inline">\(n_{\epsilon} = 1/\epsilon\)</span> ensures that the definition holds for all values of <span class="math inline">\(\epsilon\)</span> and that <span class="math inline">\(\lim_{n\to\infty} 1/n = 0\)</span>.</p>
</div>
<p>We will mostly not use such formal definitions to establish a limit but, rather, rely on the properties of limits. For example, convergence and limits follow basic arithmetic operations. Suppose that we have two sequences with limits <span class="math inline">\(\lim_{n\to\infty} a_n = a\)</span> and <span class="math inline">\(\lim_{n\to\infty} b_n = b\)</span>. Then the properties of limits imply:</p>
<ul>
<li><span class="math inline">\(\lim_{n\to\infty} (a_n + b_n) = a + b\)</span></li>
<li><span class="math inline">\(\lim_{n\to\infty} a_nb_n = ab\)</span></li>
<li><span class="math inline">\(\lim_{n\to\infty} ca_n = c\cdot a\)</span></li>
<li><span class="math inline">\(\lim_{n\to\infty} (a_n/b_n) = a/b\)</span> if <span class="math inline">\(b \neq 0\)</span></li>
<li><span class="math inline">\(\lim_{n\to\infty} a_n^{k} = a^{k}\)</span></li>
</ul>
<p>These rules plus the result in <a href="#exm-limit" class="quarto-xref">Example&nbsp;<span>3.1</span></a> allow us to prove other useful facts such as <span class="math display">\[
\lim_{n\to\infty} \frac{2}{n} = 2 \cdot 0 = 0 \qquad  \lim_{n\to\infty} \frac{1}{n^{2}} = 0.
\]</span></p>
<p>Can we apply a similar definition of convergence to sequences of random variables (like estimators)? Possibly. Some examples clarify why this might be difficult.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Suppose we have a sequence of <span class="math inline">\(a_n = a\)</span> for all <span class="math inline">\(n\)</span> (that is, a constant sequence). Then obviously <span class="math inline">\(\lim_{n\rightarrow\infty} a_n = a\)</span>. Now let’s say we have a sequence of random variables, <span class="math inline">\(X_1, X_2, \ldots\)</span>, that are all independent with a standard normal distribution, <span class="math inline">\(N(0,1)\)</span>. From the analogy to the deterministic case, saying that <span class="math inline">\(X_n\)</span> converges to <span class="math inline">\(X \sim N(0, 1)\)</span> would be tempting, but note that because they are all different random variables, <span class="math inline">\(\P(X_n = X) = 0\)</span>. Thus, we must be careful about saying how one variable converges to another variable.</p>
<p>Another example highlights subtle problems with a sequence of random variables converging to a single value. Suppose we have a sequence of random variables <span class="math inline">\(X_1, X_2, \ldots\)</span> where <span class="math inline">\(X_n \sim N(0, 1/n)\)</span>. Clearly, the distribution of <span class="math inline">\(X_n\)</span> will concentrate around 0 for large values of <span class="math inline">\(n\)</span>, so saying that <span class="math inline">\(X_n\)</span> converges to 0 is tempting. But notice that <span class="math inline">\(\P(X_n = 0) = 0\)</span> because of the nature of continuous random variables.</p>
</section>
<section id="convergence-in-probability-and-consistency" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="convergence-in-probability-and-consistency"><span class="header-section-number">3.3</span> Convergence in probability and consistency</h2>
<p>A sequence of random variables can converge in several different ways. The first type of convergence deals with sequences converging to a single value.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div id="def-inprob" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2</strong></span> A sequence of random variables, <span class="math inline">\(X_1, X_2, \ldots\)</span>, is said to <strong>converge in probability</strong> to a value <span class="math inline">\(b\)</span> if for every <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math display">\[
\P(|X_n - b| &gt; \varepsilon) \rightarrow 0,
\]</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. We write this <span class="math inline">\(X_n \inprob b\)</span>.</p>
</div>
<p>What’s happening in this definition? The even <span class="math inline">\(|X_n - b| &gt; \varepsilon\)</span> says that a draw of <span class="math inline">\(X_n\)</span> is more than <span class="math inline">\(\varepsilon\)</span> away from <span class="math inline">\(b\)</span> (above or below). So convergence in probability says that the probability of being some distance away from the limit value goes to zero as the <span class="math inline">\(n\)</span> goes to <span class="math inline">\(\infty\)</span>. With deterministic sequences, we said that <span class="math inline">\(a_n\)</span> converges to <span class="math inline">\(a\)</span> as it gets closer and closer to <span class="math inline">\(a\)</span> as <span class="math inline">\(n\)</span> gets bigger. For convergence in probability, the sequence of random variables converges to <span class="math inline">\(b\)</span> if the probability that random variables are far away from <span class="math inline">\(b\)</span> gets smaller and smaller as <span class="math inline">\(n\)</span> gets big.</p>
<div id="exm-inprob" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2</strong></span> Let’s illustrate the definition of convergence in probability by constructing a sequence of random variables, <span class="math display">\[
X_n \sim N(0, 1/n).
\]</span></p>
<p>We can see intuitively that this sequence will be centered at zero with a shrinking variance. Below, we will see that this is enough to establish convergence in probability of <span class="math inline">\(X_n\)</span> to 0, but we can also show this in terms of its definition. To do so, we need to show that <span class="math display">\[
\P(|X_n| &gt; \varepsilon) \to 0.
\]</span></p>
<p>Let <span class="math inline">\(\Phi(\cdot)\)</span> be the cdf for the standard normal. For any <span class="math inline">\(n\)</span>, the cdf for <span class="math inline">\(X_n\)</span> is <span class="math inline">\(\P(X_{n} &lt; x) = \Phi(\sqrt{n}x)\)</span>. Thus, <span class="math display">\[
\begin{aligned}
\P(|X_n| &gt; \varepsilon) &amp;= \P(X_n &lt; -\varepsilon) + \P(X_n &gt; \varepsilon) \\ &amp;= \Phi(-\sqrt{n}\varepsilon) + (1 - \Phi(\sqrt{n}\varepsilon)) \to 0.
\end{aligned}
\]</span> The last limit is due to <span class="math inline">\(\sqrt{n}\varepsilon \to \infty\)</span> and thus, by the properties of the cdf, <span class="math inline">\(\Phi(-\sqrt{n}\varepsilon) \to 0\)</span> and <span class="math inline">\(\Phi(\sqrt{n}\varepsilon) \to 1\)</span>. Clearly this holds for any <span class="math inline">\(\varepsilon\)</span>, so <span class="math inline">\(X_n \inprob 0\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes convergence in probability is written as <span class="math inline">\(\text{plim}(Z_n) = b\)</span> when <span class="math inline">\(Z_n \inprob b\)</span>, <span class="math inline">\(\text{plim}\)</span> stands for “probability limit.”</p>
</div>
</div>
<p>Convergence in probability is crucial for evaluating estimators. While we said that unbiasedness was not the be-all and end-all of properties of estimators, the following property is an essential and fundamental property of good estimators.</p>
<div id="def-consistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3</strong></span> An estimator is <strong>consistent</strong> if <span class="math inline">\(\widehat{\theta}_n \inprob \theta\)</span>.</p>
</div>
<p>Consistency of an estimator implies that the sampling distribution of the estimator “collapses” on the true value as the sample size gets large. An estimator is <strong>inconsistent</strong> if it converges in probability to any other value. As the sample size gets large, the probability that an inconsistent estimator will be close to the truth will approach 0. Generally speaking, consistency is a very desirable property of an estimator.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Estimators can be inconsistent yet still converge in probability to an understandable quantity. For example, we will discuss in later chapters that regression coefficients estimated by ordinary least squares (OLS) are consistent for the conditional expectation if the conditional expectation is linear. If that function is non-linear, however, then OLS will be consistent for the best linear approximation to that function. While not ideal, it does mean that this estimator is at least consistent for an interpretable quantity.</p>
</div>
</div>
<p>We can also define convergence in probability for a sequence of random vectors, <span class="math inline">\(\X_1, \X_2, \ldots\)</span>, where <span class="math inline">\(\X_i = (X_{i1}, \ldots, X_{ik})\)</span> is a random vector of length <span class="math inline">\(k\)</span>. This sequence converges in probability to a vector <span class="math inline">\(\mb{b} = (b_1, \ldots, b_k)\)</span> if and only if each random variable in the vector converges to the corresponding element in <span class="math inline">\(\mb{b}\)</span>, or that <span class="math inline">\(X_{nj} \inprob b_j\)</span> for all <span class="math inline">\(j = 1, \ldots, k\)</span>.</p>
</section>
<section id="useful-inequalities" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="useful-inequalities"><span class="header-section-number">3.4</span> Useful inequalities</h2>
<p>At first glance, establishing an estimator’s consistency will be difficult. How can we know if a distribution will collapse to a specific value without knowing the shape or family of the distribution? It turns out that there are certain relationships between the mean and variance of a random variable and certain probability statements that hold for all distributions (that have finite variance, at least). These relationships are key to establishing results that do not depend on a specific distribution.</p>
<div id="thm-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Markov Inequality)</strong></span> For any r.v. <span class="math inline">\(X\)</span> and any <span class="math inline">\(\delta &gt;0\)</span>, <span class="math display">\[
\P(|X| \geq \delta) \leq \frac{\E[|X|]}{\delta}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note that we can let <span class="math inline">\(Y = |X|/\delta\)</span> and rewrite the statement as <span class="math inline">\(\P(Y \geq 1) \leq \E[Y]\)</span> (since <span class="math inline">\(\E[|X|]/\delta = \E[|X|/\delta]\)</span> by the properties of expectation), which is what we will show. But also note that <span class="math display">\[
\mathbb{I}(Y \geq 1) \leq Y.
\]</span> Why does this hold? The two possible values of the indicator function show why. If <span class="math inline">\(Y\)</span> is less than 1, then the indicator function will be 0, but recall that <span class="math inline">\(Y\)</span> is nonnegative, so we know that it must be at least as big as 0 so that inequality holds. If <span class="math inline">\(Y \geq 1\)</span>, then the indicator function will take the value one, but we just said that <span class="math inline">\(Y \geq 1\)</span>, so the inequality holds. If we take the expectation of both sides of this inequality, we obtain the result (remember, the expectation of an indicator function is the probability of the event).</p>
</div>
<p>In words, Markov’s inequality says that the probability of a random variable being large in magnitude cannot be high if the average is not large in magnitude. Blitzstein and Hwang (2019) provide an excellent intuition behind this result using income as an example. Let <span class="math inline">\(X\)</span> be the income of a randomly selected individual in a population and set <span class="math inline">\(\delta = 2\E[X]\)</span> so that the inequality becomes <span class="math inline">\(\P(X &gt; 2\E[X]) &lt; 1/2\)</span> (assuming that all income is nonnegative). Here, the inequality says that the share of the population with an income twice the average must be less than 0.5 since if more than half the population were making twice the average income, then the average would have to be higher.</p>
<p>It’s pretty astounding how general this result is since it holds for all random variables. Of course, its generality comes at the expense of not being very informative. If <span class="math inline">\(\E[|X|] = 5\)</span>, for instance, the inequality tells us that <span class="math inline">\(\P(|X| \geq 1) \leq 5\)</span>, which is not very helpful since we already know that probabilities are less than 1! We can get tighter bounds if we are willing to make some assumptions about <span class="math inline">\(X\)</span>.</p>
<div id="thm-chebyshev" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Chebyshev Inequality)</strong></span> Suppose that <span class="math inline">\(X\)</span> is r.v. for which <span class="math inline">\(\V[X] &lt; \infty\)</span>. Then, for every real number <span class="math inline">\(\delta &gt; 0\)</span>, <span class="math display">\[
\P(|X-\E[X]| \geq \delta) \leq \frac{\V[X]}{\delta^2}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove this, we only need to square both sides of the inequality inside the probability statement and apply Markov’s inequality: <span class="math display">\[
\P\left( |X - \E[X]| \geq \delta \right) = \P((X-\E[X])^2 \geq \delta^2) \leq \frac{\E[(X - \E[X])^2]}{\delta^2} = \frac{\V[X]}{\delta^2},
\]</span> with the last equality holding by the definition of variance.</p>
</div>
<p>Chebyshev’s inequality is a straightforward extension of the Markov result: the probability of a random variable being far from its mean (that is, <span class="math inline">\(|X-\E[X]|\)</span> being large) is limited by the variance of the random variable. If we let <span class="math inline">\(\delta = c\sigma\)</span>, where <span class="math inline">\(\sigma\)</span> is the standard deviation of <span class="math inline">\(X\)</span>, we can use this result to bound the normalized deviation from the mean: <span class="math display">\[
\P\left(\frac{|X - \E[X]|}{\sigma} &gt; c \right) \leq \frac{1}{c^2}.
\]</span> This statement says the probability of being two standard deviations away from the mean must be less than 1/4 = 0.25. Notice that this bound can be fairly wide. If <span class="math inline">\(X\)</span> has a normal distribution, we know that about 5% of draws will be greater than 2 SDs away from the mean, much lower than the 25% bound implied by Chebyshev’s inequality.</p>
</section>
<section id="the-law-of-large-numbers" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="the-law-of-large-numbers"><span class="header-section-number">3.5</span> The law of large numbers</h2>
<p>We can now use these inequalities to show how estimators can be consistent for their target quantities of interest without making parametric assumptions. Why are these inequalities helpful? Remember that convergence in probability was about the probability of an estimator being far away from a value going to zero. Chebyshev’s inequality shows that we can bound these exact probabilities.</p>
<p>The most famous consistency result has a special name.</p>
<div id="thm-lln" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (Weak Law of Large Numbers)</strong></span> Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be i.i.d. draws from a distribution with mean <span class="math inline">\(\mu = \E[X_i]\)</span> and variance <span class="math inline">\(\sigma^2 = \V[X_i] &lt; \infty\)</span>. Let <span class="math inline">\(\Xbar_n = \frac{1}{n} \sum_{i =1}^n X_i\)</span>. Then, <span class="math inline">\(\Xbar_n \inprob \mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that the sample mean is unbiased, so <span class="math inline">\(\E[\Xbar_n] = \mu\)</span> with sampling variance <span class="math inline">\(\sigma^2/n\)</span>. We can then apply Chebyshev to the sample mean to get <span class="math display">\[
\P(|\Xbar_n - \mu| \geq \delta) \leq \frac{\V[\Xbar_n]}{\delta^2} = \frac{\sigma^2}{n\delta^2}
\]</span> An <span class="math inline">\(n\rightarrow\infty\)</span>, the right-hand side goes to 0, which means that the left-hand side also must go to 0, which is the definition of <span class="math inline">\(\Xbar_n\)</span> converging in probability to <span class="math inline">\(\mu\)</span>.</p>
</div>
<p>The weak law of large numbers (WLLN) shows that, under general conditions, the sample mean gets closer to the population mean as <span class="math inline">\(n\rightarrow\infty\)</span>. This result holds even when the variance of the data is infinite, though researchers will rarely face such a situation.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The naming of the “weak” law of large numbers seems to imply the existence of a “strong” law of large numbers (SLLN), which is true. The SLLN states that the sample mean converges to the population mean with probability 1. This type of convergence, called <strong>almost sure convergence</strong>, is stronger than convergence in probability, which only says that the probability of the sample mean being close to the population mean converges to 1. While it is nice to know that this stronger form of convergence holds for the sample mean under the same assumptions, it is rare for researchers outside of theoretical probability and statistics to rely on almost sure convergence.</p>
</div>
</div>
<div id="exm-lln" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3</strong></span> Seeing how the distribution of the sample mean changes as a function of the sample size allows us to appreciate the WLLN. We can see this by taking repeated iid samples of different sizes from an exponential random variable with rate parameter 0.5 so that <span class="math inline">\(\E[X_i] = 2\)</span>. In <a href="#fig-lln-sim" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, we show the distribution of the sample mean (across repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue), and 1000 (green). The sample mean distribution “collapses” on the true population mean, 2. The probability of being far away from 2 becomes progressively smaller as the sample size increases.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-lln-sim" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lln-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-lln-sim-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lln-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Sampling distribution of the sample mean as a function of sample size.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The WLLN also holds for random vectors in addition to random variables. Let <span class="math inline">\((\X_1, \ldots, \X_n)\)</span> be an iid sample of random vectors of length <span class="math inline">\(k\)</span>, <span class="math inline">\(\mb{X}_i = (X_{i1}, \ldots, X_{ik})\)</span>. We can define the vector sample mean as just the vector of sample means for each of the entries:</p>
<p><span class="math display">\[
\overline{\mb{X}}_n = \frac{1}{n} \sum_{i=1}^n \mb{X}_i =
\begin{pmatrix}
\Xbar_{n,1} \\ \Xbar_{n,2} \\ \vdots \\ \Xbar_{n, k}
\end{pmatrix}
\]</span> Since this is just a vector of sample means, each random variable in the random vector will converge in probability to the mean of that random variable. Fortunately, this is the exact definition of convergence in probability for random vectors. We formally write this in the following theorem.</p>
<div id="thm-vector-wlln" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4</strong></span> If <span class="math inline">\(\X_i \in \mathbb{R}^k\)</span> are iid draws from a distribution with <span class="math inline">\(\E[X_{ij}] &lt; \infty\)</span> for all <span class="math inline">\(j=1,\ldots,k\)</span> then as <span class="math inline">\(n\rightarrow\infty\)</span></p>
<p><span class="math display">\[
\overline{\mb{X}}_n \inprob \E[\X] =
\begin{pmatrix}
\E[X_{i1}] \\ \E[X_{i2}] \\ \vdots \\ \E[X_{ik}]
\end{pmatrix}.
\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that many of the formal results presented so far have “moment conditions” that certain moments are finite. For the vector WLLN, we saw that applied to the mean of each variable in the vector. Some books use a shorthand for this: <span class="math inline">\(\E\Vert \X_i\Vert &lt; \infty\)</span>, where <span class="math display">\[
\Vert\X_i\Vert = \left(X_{i1}^2 + X_{i2}^2 + \ldots + X_{ik}^2\right)^{1/2}.
\]</span> This expression has slightly more compact notation, but why does it work? One can show that this function, called the <strong>Euclidean norm</strong> or <span class="math inline">\(L_2\)</span>-norm, is a <strong>convex</strong> function, so we can apply Jensen’s inequality to show that: <span class="math display">\[
\E\Vert \X_i\Vert \geq \Vert \E[\X_i] \Vert = (\E[X_{i1}]^2 + \ldots + \E[X_{ik}]^2)^{1/2}.
\]</span> So if <span class="math inline">\(\E\Vert \X_i\Vert\)</span> is finite, all the component means are finite. Otherwise, the right-hand side of the previous equation would be infinite.</p>
</div>
</div>
</section>
<section id="consistency-of-estimators" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="consistency-of-estimators"><span class="header-section-number">3.6</span> Consistency of estimators</h2>
<p>The WLLN shows that the sample mean of iid draws is consistent for the population mean, which is a massive result given that so many estimators are sample means of potentially complicated functions of the data. What about other estimators? The proof of the WLLN points to one way to determine that an estimator is consistent: if it is unbiased and the sampling variance shrinks as the sample size grows.</p>
<div id="thm-consis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.5</strong></span> For any estimator <span class="math inline">\(\widehat{\theta}_n\)</span>, if <span class="math inline">\(\text{bias}[\widehat{\theta}_n] = 0\)</span> and <span class="math inline">\(\V[\widehat{\theta}_n] \rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, then <span class="math inline">\(\widehat{\theta}_n\)</span> is consistent.</p>
</div>
<p>Thus, for unbiased estimators, if we can characterize its sampling variance, we should be able to tell if it is consistent. This result is handy since working with the probability statements used for the WLLN can sometimes be confusing.</p>
<p>What about biased estimators? Consider a situation where we calculate average household income, <span class="math inline">\(\Xbar_n\)</span>, from a random sample with mean <span class="math inline">\(\mu\)</span>, but our actual interest is in the log of average income, <span class="math inline">\(\alpha = \log(\mu)\)</span>. We can obviously use the standard plug-in estimator <span class="math inline">\(\widehat{\alpha} = \log(\Xbar_n)\)</span>, but, for nonlinear functions like logarithms we have <span class="math inline">\(\log\left(\E[Z]\right) \neq \E[\log(Z)]\)</span>, so <span class="math inline">\(\E[\widehat{\alpha}] \neq \log(\E[\Xbar_n])\)</span> and the plug-in estimator will be biased for <span class="math inline">\(\log(\mu)\)</span>. Obtaining an expression for the bias in terms of <span class="math inline">\(n\)</span> is also difficult. Is the quest doomed? Must we give up on consistency? No, and, in fact, a few key properties of consistency make working with it much easier compared to unbiasedness.</p>
<div id="thm-inprob-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.6 (Properties of convergence in probability)</strong></span> Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Z_n\)</span> be two sequences of random variables such that <span class="math inline">\(X_n \inprob a\)</span> and <span class="math inline">\(Z_n \inprob b\)</span>, and let <span class="math inline">\(g(\cdot)\)</span> be a continuous function. Then,</p>
<ol type="1">
<li><span class="math inline">\(g(X_n) \inprob g(a)\)</span> (continuous mapping theorem)</li>
<li><span class="math inline">\(X_n + Z_n \inprob a + b\)</span></li>
<li><span class="math inline">\(X_nZ_n \inprob ab\)</span></li>
<li><span class="math inline">\(X_n/Z_n \inprob a/b\)</span> if <span class="math inline">\(b &gt; 0\)</span>.</li>
</ol>
</div>
<p>We can now see that many of the nasty problems with expectations and nonlinear functions are made considerably easier with convergence in probability in the asymptotic setting. So while we know that <span class="math inline">\(\log(\Xbar_n)\)</span> is biased for <span class="math inline">\(\log(\mu)\)</span>, we know that it is consistent since <span class="math inline">\(\log(\Xbar_n) \inprob \log(\mu)\)</span> because <span class="math inline">\(\log\)</span> is a continuous function.</p>
<div id="exm-nonresponse" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4</strong></span> Suppose we implemented a survey by randomly selecting a sample from the population of size <span class="math inline">\(n\)</span>, but not everyone responds to the survey. Let the data consist of pairs of random variables, <span class="math inline">\((Y_1, R_1), \ldots, (Y_n, R_n)\)</span>, where <span class="math inline">\(Y_i\)</span> is the question of interest and <span class="math inline">\(R_i\)</span> is a binary indicator for if the respondent answered the question (<span class="math inline">\(R_i = 1\)</span>) or not (<span class="math inline">\(R_i = 0\)</span>). Our goal is to estimate the mean of the question for responders: <span class="math inline">\(\E[Y_i \mid R_i = 1]\)</span>. We can use the law of iterated expectation to obtain <span class="math display">\[
\begin{aligned}
\E[Y_iR_i] &amp;= \E[Y_i \mid R_i = 1]\P(R_i = 1) + \E[ 0 \mid R_i = 0]\P(R_i = 0) \\
\implies \E[Y_i \mid R_i = 1] &amp;= \frac{\E[Y_iR_i]}{\P(R_i = 1)}
\end{aligned}
\]</span></p>
<p>The relevant estimator for this quantity is the mean of the outcome among those who responded, which is slightly more complicated than a typical sample mean because the denominator is a random variable: <span class="math display">\[
\widehat{\theta}_n = \frac{\sum_{i=1}^n Y_iR_i}{\sum_{i=1}^n R_i}.
\]</span> Notice that this estimator is the ratio of two random variables. The numerator has mean <span class="math inline">\(n\E[Y_iR_i]\)</span> and the denominator has mean <span class="math inline">\(n\P(R_i = 1)\)</span>. It is then tempting to say that we can take the ratio of these means as the mean of <span class="math inline">\(\widehat{\theta}_n\)</span>, but expectations are not preserved in nonlinear functions like this.</p>
<p>We can establish consistency of our estimator, though, by noting that we can rewrite the estimator as a ratio of sample means <span class="math display">\[
\widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n Y_iR_i}{(1/n)\sum_{i=1}^n R_i},
\]</span> where by the WLLN the numerator <span class="math inline">\((1/n)\sum_{i=1}^n Y_iR_i \inprob \E[Y_iR_i]\)</span> and the denominator <span class="math inline">\((1/n)\sum_{i=1}^n R_i \inprob \P(R_i = 1)\)</span>. Thus, by <a href="#thm-inprob-properties" class="quarto-xref">Theorem&nbsp;<span>3.6</span></a>, we have <span class="math display">\[
\widehat{\theta}_n = \frac{(1/n)\sum_{i=1}^n Y_iR_i}{(1/n)\sum_{i=1}^n R_i} \inprob \frac{\E[Y_iR_i]}{\P[R_i = 1]} = \E[Y_i \mid R_i = 1],
\]</span> so long as the probability of responding is greater than zero. This establishes that our sample mean among responders, while biased for the conditional expectation among responders, is consistent for that quantity.</p>
</div>
<p>Keeping the difference between unbiased and consistent clear in your mind is essential. You can easily create ridiculous unbiased estimators that are inconsistent. Let’s return to our iid sample, <span class="math inline">\(X_1, \ldots, X_n\)</span>, from a population with <span class="math inline">\(E[X_i] = \mu\)</span>. There is nothing in the rule book against defining an estimator <span class="math inline">\(\widehat{\theta}_{first} = X_1\)</span> that uses the first observation as the estimate. This estimator is silly, but it is unbiased since <span class="math inline">\(\E[\widehat{\theta}_{first}] = \E[X_1] = \mu\)</span>. It is inconsistent since the sampling variance of this estimator is just the variance of the population distribution, <span class="math inline">\(\V[\widehat{\theta}_{first}] = \V[X_i] = \sigma^2\)</span>, which does not change as a function of the sample size. Generally speaking, we can regard “unbiased but inconsistent” estimators as silly and not worth our time (along with biased and inconsistent estimators).</p>
<p>Some estimators are biased but consistent that are often much more interesting. We already saw one such estimator in <a href="#exm-nonresponse" class="quarto-xref">Example&nbsp;<span>3.4</span></a>, but there are many more. Maximum likelihood estimators, for example, are (under some regularity conditions) consistent for the parameters of a parametric model but are often biased.</p>
<p>To study these estimator, we can broaden <a href="#thm-consis" class="quarto-xref">Theorem&nbsp;<span>3.5</span></a> to the class of <strong>asymptotically unbiased</strong> estimators that have bias that vanishes as the sample size grows.</p>
<div id="thm-consis-2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.7</strong></span> For any estimator <span class="math inline">\(\widehat{\theta}_n\)</span>, if <span class="math inline">\(\text{bias}[\widehat{\theta}_n] \to 0\)</span> and <span class="math inline">\(\V[\widehat{\theta}_n] \rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>, then <span class="math inline">\(\widehat{\theta}_n\)</span> is consistent.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Using Markov’s inequality, we have <span class="math display">\[
\P\left( |\widehat{\theta}_n - \theta| \geq \delta \right) = \P((\widehat{\theta}_n-\theta)^2 \geq \delta^2) \leq \frac{\E[(\widehat{\theta}_n - \theta)^2]}{\delta^2} = \frac{\text{bias}[\widehat{\theta}_n]^2 + \V[\widehat{\theta}]}{\delta^2} \to 0.
\]</span> The last inequality follows from the bias-variance decomposition of the mean squared error in <a href="estimation.html#eq-mse-decomposition" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>.</p>
</div>
<p>We can use this result to show consistency for a large range of estimators.</p>
<div id="exm-plug-in-variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 (Plug-in variance estimator)</strong></span> In the last chapter, we introduced the plug-in estimator for the population variance, <span class="math display">\[
\widehat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \Xbar_n)^2,
\]</span> which we will now show is biased but consistent. To see the bias note that we can rewrite the sum of square deviations <span class="math display">\[\sum_{i=1}^n (X_i - \Xbar_n)^2 = \sum_{i=1}^n X_i^2 - n\Xbar_n^2. \]</span> Then, the expectation of the plug-in estimator is <span class="math display">\[
\begin{aligned}
\E[\widehat{\sigma}^2] &amp; = \E\left[\frac{1}{n}\sum_{i=1}^n X_i^2\right] - \E[\Xbar_n^2] \\
&amp;= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \E[X_iX_j] \\
&amp;= \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \E[X_i^2] - \frac{1}{n^2}\sum_{i=1}^n \sum_{j\neq i} \underbrace{\E[X_i]\E[X_j]}_{\text{independence}} \\
&amp;= \E[X_i^2] - \frac{1}{n}\E[X_i^2] - \frac{1}{n^2} n(n-1)\mu^2 \\
&amp;= \frac{n-1}{n} \left(\E[X_i^2] - \mu^2\right) \\
&amp;= \frac{n-1}{n} \sigma^2 = \sigma^2 - \frac{1}{n}\sigma^2
\end{aligned}.
\]</span> Thus, we can see that the bias of the plug-in estimator is <span class="math inline">\(-(1/n)\sigma^2\)</span>, so it slightly underestimates the variance. Nicely, though, the bias shrinks as a function of the sample size, so according to <a href="#thm-consis-2" class="quarto-xref">Theorem&nbsp;<span>3.7</span></a>, it will be consistent so long as the sampling variance of <span class="math inline">\(\widehat{\sigma}^2\)</span> shrinks as a function of the sample size, which it does (though omit that proof here). Of course, simply multiplying this estimator by <span class="math inline">\(n/(n-1)\)</span> will give an unbiased and consistent estimator that is also the typical sample variance estimator.</p>
</div>
</section>
<section id="convergence-in-distribution-and-the-central-limit-theorem" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="convergence-in-distribution-and-the-central-limit-theorem"><span class="header-section-number">3.7</span> Convergence in distribution and the central limit theorem</h2>
<p>Convergence in probability and the law of large numbers are beneficial for understanding how our estimators will (or will not) collapse to their estimand as the sample size increases. But what about the shape of the sampling distribution of our estimators? For statistical inference, we would like to be able to make probability statements such as <span class="math inline">\(\P(a \leq \widehat{\theta}_n \leq b)\)</span>. These statements will be the basis of hypothesis testing and confidence intervals. But to make those types of statements, we need to know the entire distribution of <span class="math inline">\(\widehat{\theta}_n\)</span>, not just the mean and variance. Luckily, established results will allow us to approximate the sampling distribution of a vast swath of estimators when our sample sizes are large.</p>
<p>We need first to describe a weaker form of convergence to see how we will develop these approximations.</p>
<div id="def-indist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4</strong></span> Let <span class="math inline">\(X_1,X_2,\ldots\)</span>, be a sequence of r.v.s, and for <span class="math inline">\(n = 1,2, \ldots\)</span> let <span class="math inline">\(F_n(x)\)</span> be the c.d.f. of <span class="math inline">\(X_n\)</span>. Then it is said that <span class="math inline">\(X_1, X_2, \ldots\)</span> <strong>converges in distribution</strong> to r.v. <span class="math inline">\(X\)</span> with c.d.f. <span class="math inline">\(F(x)\)</span> if <span class="math display">\[
\lim_{n\rightarrow \infty} F_n(x) = F(x),
\]</span> for all values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(F(x)\)</span> is continuous. We write this as <span class="math inline">\(X_n \indist X\)</span> or sometimes <span class="math inline">\(X_n ⇝ X\)</span>.</p>
</div>
<p>Essentially, convergence in distribution means that as <span class="math inline">\(n\)</span> gets large, the distribution of <span class="math inline">\(X_n\)</span> becomes more and more similar to the distribution of <span class="math inline">\(X\)</span>, which we often call the <strong>asymptotic distribution</strong> of <span class="math inline">\(X_n\)</span> (other names include the <strong>large-sample distribution</strong>). If we know that <span class="math inline">\(X_n \indist X\)</span>, then we can use the distribution of <span class="math inline">\(X\)</span> as an approximation to the distribution of <span class="math inline">\(X_n\)</span>, and that distribution can be reasonably accurate.</p>
<div id="exm-indist" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6</strong></span> A simple example of convergence in distribution would be the sequence <span class="math display">\[
X_n \sim N\left(\frac{1}{n}, 1 + \frac{1}{n}\right),
\]</span> which, of course, has the cdf, <span class="math display">\[
\Phi\left(\frac{x - 1/n}{1+1/n}\right).
\]</span> By inspection, this converges to <span class="math inline">\(\Phi(x)\)</span>, which is the cdf for the standard normal. This implies <span class="math inline">\(X_n \indist N(0, 1)\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="asymptotics_files/figure-html/indist-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</div>
<p>One of the most remarkable results in probability and statistics is that a large class of estimators will converge in distribution to one particular family of distributions: the normal. This result is one reason we study the normal so much and why investing in building intuition about it will pay off across many domains of applied work. We call this broad class of results the “central limit theorem” (CLT), but it would probably be more accurate to refer to them as “central limit theorems” since much of statistics is devoted to showing the result in different settings. We now present the simplest CLT for the sample mean.</p>
<div id="thm-clt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.8 (Central Limit Theorem)</strong></span> Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be i.i.d. r.v.s from a distribution with mean <span class="math inline">\(\mu = \E[X_i]\)</span> and variance <span class="math inline">\(\sigma^2 = \V[X_i]\)</span>. Then if <span class="math inline">\(\E[X_i^2] &lt; \infty\)</span>, we have <span class="math display">\[
\frac{\Xbar_n - \mu}{\sqrt{\V[\Xbar_n]}} = \frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{\sigma} \indist \N(0, 1).
\]</span></p>
</div>
<p>In words: the sample mean of a random sample from a population with finite mean and variance will be approximately normally distributed in large samples. Notice how we have not made any assumptions about the distribution of the underlying random variables, <span class="math inline">\(X_i\)</span>. They could be binary, event count, continuous, or anything. The CLT is incredibly broadly applicable.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why do we state the CLT in terms of the sample mean after centering and scaling by its standard error? Suppose we don’t normalize the sample mean in this way. In that case, it isn’t easy to talk about convergence in distribution because we know from the WLLN that <span class="math inline">\(\Xbar_n \inprob \mu\)</span>, so in the limit, the distribution of <span class="math inline">\(\Xbar_n\)</span> is concentrated at point mass around that value. Normalizing by centering and rescaling ensures that the variance of the resulting quantity will not depend on <span class="math inline">\(n\)</span>, so it makes sense to talk about its distribution converging. Sometimes you will see the equivalent result as <span class="math display">\[
\sqrt{n}\left(\Xbar_n - \mu\right) \indist \N(0, \sigma^2).
\]</span></p>
</div>
</div>
<p>We can use this result to state approximations that we can use when discussing estimators such as <span class="math display">\[
\Xbar_n \overset{a}{\sim} N(\mu, \sigma^2/n),
\]</span> where we use <span class="math inline">\(\overset{a}{\sim}\)</span> to be “approximately distributed as in large samples.” This approximation allows us to say things like: “in large samples, we should expect the sample mean to between within <span class="math inline">\(2\sigma/\sqrt{n}\)</span> of the true mean in 95% of repeated samples.” These statements will be essential for hypothesis tests and confidence intervals! Estimators so often follow the CLT that we have an expression for this property.</p>
<div id="def-asymptotically-normal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.5</strong></span> An estimator <span class="math inline">\(\widehat{\theta}_n\)</span> is <strong>asymptotically normal</strong> if for some <span class="math inline">\(\theta\)</span> <span class="math display">\[
\sqrt{n}\left( \widehat{\theta}_n - \theta \right) \indist N\left(0,\V_{\theta}\right).
\]</span></p>
</div>
<div id="exm-bin-clt" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7</strong></span> To illustrate how the CLT works, we can simulate the sampling distribution of the (normalized) sample mean at different sample sizes. Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be iid samples from a Bernoulli with probability of success 0.25. We then draw repeated samples of size <span class="math inline">\(n=30\)</span> and <span class="math inline">\(n=100\)</span> and calculate <span class="math inline">\(\sqrt{n}(\Xbar_n - 0.25)/\sigma\)</span> for each random sample. <a href="#fig-clt" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> plots the density of these two sampling distributions along with a standard normal reference. We can see that even at <span class="math inline">\(n=30\)</span>, the rough shape of the density looks normal, with spikes and valleys due to the discrete nature of the data (the sample mean can only take on 31 possible values in this case). By <span class="math inline">\(n=100\)</span>, the sampling distribution is very close to the true standard normal.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-clt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-clt-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Sampling distributions of the normalized sample mean at n=30 and n=100.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>There are several properties of convergence in distribution that are helpful to us.</p>
<div id="thm-indist-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.9 (Properties of convergence in distribution)</strong></span> Let <span class="math inline">\(X_n\)</span> be a sequence of random variables <span class="math inline">\(X_1, X_2,\ldots\)</span> that converges in distribution to some rv <span class="math inline">\(X\)</span> and let <span class="math inline">\(Y_n\)</span> be a sequence of random variables <span class="math inline">\(Y_1, Y_2,\ldots\)</span> that converges in probability to some number, <span class="math inline">\(c\)</span>. Then,</p>
<ol type="1">
<li><span class="math inline">\(g(X_n) \indist g(X)\)</span> for all continuous functions <span class="math inline">\(g\)</span>.</li>
<li><span class="math inline">\(X_nY_n\)</span> converges in distribution to <span class="math inline">\(cX\)</span></li>
<li><span class="math inline">\(X_n + Y_n\)</span> converges in distribution to <span class="math inline">\(X + c\)</span></li>
<li><span class="math inline">\(X_n / Y_n\)</span> converges in distribution to <span class="math inline">\(X / c\)</span> if <span class="math inline">\(c \neq 0\)</span></li>
</ol>
</div>
<p>We refer to the last three results as <strong>Slutsky’s theorem</strong>. These results are often crucial for determining an estimator’s asymptotic distribution.</p>
<p>A critical application of Slutsky’s theorem is when we replace the (unknown) population variance in the CLT with an estimate. Recall the definition of the <strong>sample variance</strong> as <span class="math display">\[
S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \Xbar_n)^2,
\]</span> with the <strong>sample standard deviation</strong> defined as <span class="math inline">\(S_{n} = \sqrt{S_{n}^2}\)</span>. It’s easy to show that these are consistent estimators for their respective population parameters <span class="math display">\[
S_{n}^2 \inprob \sigma^2 = \V[X_i], \qquad S_{n} \inprob \sigma,
\]</span> which, by Slutsky’s theorem, implies that <span class="math display">\[
\frac{\sqrt{n}\left(\Xbar_n - \mu\right)}{S_n} \indist \N(0, 1)
\]</span> Comparing this result to the statement of CLT, we see that replacing the population variance with a consistent estimate of the variance (or standard deviation) does not affect the asymptotic distribution.</p>
<p>Like with the WLLN, the CLT holds for random vectors of sample means, where their centered and scaled versions converge to a multivariate normal distribution with a covariance matrix equal to the covariance matrix of the underlying random vectors of data, <span class="math inline">\(\X_i\)</span>.</p>
<div id="thm-multivariate-clt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.10</strong></span> If <span class="math inline">\(\mb{X}_i \in \mathbb{R}^k\)</span> are i.i.d. and <span class="math inline">\(\E\Vert \mb{X}_i \Vert^2 &lt; \infty\)</span>, then as <span class="math inline">\(n \to \infty\)</span>, <span class="math display">\[
\sqrt{n}\left( \overline{\mb{X}}_n - \mb{\mu}\right) \indist \N(0, \mb{\Sigma}),
\]</span> where <span class="math inline">\(\mb{\mu} = \E[\mb{X}_i]\)</span> and <span class="math inline">\(\mb{\Sigma} = \V[\mb{X}_i] = \E\left[(\mb{X}_i-\mb{\mu})(\mb{X}_i - \mb{\mu})'\right]\)</span>.</p>
</div>
<p>Notice that <span class="math inline">\(\mb{\mu}\)</span> is the vector of population means for all the random variables in <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(\mb{\Sigma}\)</span> is the variance-covariance matrix for that vector.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As with the notation alert with the WLLN, we are using shorthand here, <span class="math inline">\(\E\Vert \mb{X}_i \Vert^2 &lt; \infty\)</span>, which implies that <span class="math inline">\(\E[X_{ij}^2] &lt; \infty\)</span> for all <span class="math inline">\(j = 1,\ldots, k\)</span>, or equivalently, that the variances of each variable in the sample means has finite variance.</p>
</div>
</div>
</section>
<section id="confidence-intervals" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">3.8</span> Confidence intervals</h2>
<p>We now turn to an essential application of the central limit theorem: confidence intervals.</p>
<p>Suppose we have run an experiment with a treatment and control group and have presented readers with our single best guess about the treatment effect using the difference in sample means. We have also presented the estimated standard error of this estimate to give readers a sense of how variable it is. But none of these approaches answer a fairly compelling question: what range of values of the treatment effect is <strong>plausible</strong> given the data we observe?</p>
<p>A point estimate of the difference in sample means typically has 0 probability of being the exact true value, but intuitively we hope that the true treatment effect is close to our estimate. <strong>Confidence intervals</strong> make this kind of intuition more formal by instead estimating ranges of values with a fixed percentage of these ranges containing the actual unknown parameter value.</p>
<p>We begin with the basic definition of a confidence interval.</p>
<div id="def-coverage" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.6</strong></span> A <span class="math inline">\(1-\alpha\)</span> <strong>confidence interval</strong> for a real-valued parameter <span class="math inline">\(\theta\)</span> is a pair of statistics <span class="math inline">\(L= L(X_1, \ldots, X_n)\)</span> and <span class="math inline">\(U = U(X_1, \ldots, X_n)\)</span> such that <span class="math inline">\(L &lt; U\)</span> for all values of the sample and such that <span class="math display">\[
\P(L \leq \theta \leq U \mid \theta) \geq 1-\alpha, \quad \forall \theta \in \Theta.
\]</span></p>
</div>
<p>We say that a <span class="math inline">\(1-\alpha\)</span> confidence interval covers (or contains, captures, traps, etc.) the true value at least <span class="math inline">\(100(1-\alpha)\%\)</span> of the time, and we refer to <span class="math inline">\(1-\alpha\)</span> as the <strong>coverage probability</strong> or simply <strong>coverage</strong>. Typical confidence intervals include 95% percent (<span class="math inline">\(\alpha = 0.05\)</span>), 90% (<span class="math inline">\(\alpha = 0.1\)</span>), and 99% (<span class="math inline">\(\alpha = 0.01\)</span>). All else equal, larger coverage will imply larger intervals.</p>
<p>So a confidence interval is a random interval with a particular guarantee about how often it will contain the true value of the unknown population parameter (in our example, the true treatment effect). Remember what is random and what is fixed in this setup. The interval varies from sample to sample, but the true value of the parameter stays fixed even if it is unknown, and the coverage is how often we should expect the interval to contain that true value. The “repeating my sample over and over again” analogy can break down very quickly, so it is sometimes helpful to interpret it as giving guarantees across confidence intervals across different experiments. In particular, suppose that a journal publishes 100 quantitative articles annually, each producing a single 95% confidence interval for their quantity of interest. Then, if the confidence intervals are valid and each is constructed in the exact same way, we should expect 95 of those confidence intervals to contain the true value.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose we have a 95% confidence interval, <span class="math inline">\([0.1, 0.4]\)</span>. It would be tempting to make a probability statement like <span class="math inline">\(\P(0.1 \leq \theta \leq 0.4 \mid \theta) = 0.95\)</span> or that there’s a 95% chance that the parameter is in <span class="math inline">\([0.1, 0.4]\)</span>. But looking at the probability statement, everything on the left-hand side of the conditioning bar is fixed, so the probability either has to be 0 (<span class="math inline">\(\theta\)</span> is outside the interval) or 1 (<span class="math inline">\(\theta\)</span> is in the interval); the unknown parameter is a fixed value, so it is either in the interval or it is not. Another way to think about this is that the coverage probability of a confidence interval refers to its status as a pair of random variables, <span class="math inline">\((L, U)\)</span>, not any particular realization of those variables like <span class="math inline">\((0.1, 0.4)\)</span>. As an analogy, consider if we calculated the sample mean as <span class="math inline">\(0.25\)</span> and then tried to say that <span class="math inline">\(0.25\)</span> is unbiased for the population mean. This statement doesn’t make sense because unbiasedness refers not to a fixed value but how the sample mean varies from sample to sample.</p>
</div>
</div>
<p>In most cases, we will not be able to derive exact confidence intervals but rather confidence intervals that are <strong>asymptotically valid</strong>, which means that if we write the interval as a function of the sample size, <span class="math inline">\((L_n, U_n)\)</span>, they would have <strong>asymptotic coverage</strong> <span class="math display">\[
\lim_{n\to\infty} \P(L_n \leq \theta \leq U_n) \geq 1-\alpha \quad\forall\theta\in\Theta.
\]</span></p>
<p>We can show asymptotic coverage for most confidence intervals since we usually rely on large-sample approximations based on the central limit theorem.</p>
<section id="deriving-confidence-intervals" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="deriving-confidence-intervals"><span class="header-section-number">3.8.1</span> Deriving confidence intervals</h3>
<p>To derive confidence intervals, consider the standard formula for the 95% confidence interval of the sample mean, <span class="math display">\[
\left[\Xbar_n - 1.96\frac{s}{\sqrt{n}},\; \Xbar_n + 1.96\frac{s}{\sqrt{n}}\right],
\]</span> where <span class="math inline">\(s\)</span> is the sample standard deviation and <span class="math inline">\(s/\sqrt{n}\)</span> is the estimate of the standard error of the sample mean. If this is a 95% confidence interval, then the probability that it contains the true population mean <span class="math inline">\(\mu\)</span> should be 0.95, but how can we derive this? We can justify this logic using the central limit theorem, and the argument will hold for any asymptotically normal estimator.</p>
<p>Suppose we have an estimator, <span class="math inline">\(\widehat{\theta}_n\)</span> for the parameter <span class="math inline">\(\theta\)</span> with estimated standard error <span class="math inline">\(\widehat{\se}[\widehat{\theta}_n]\)</span>. If the estimator is asymptotically normal, then in large samples, we know that <span class="math display">\[
\frac{\widehat{\theta}_n - \theta}{\widehat{\se}[\widehat{\theta}_n]} \sim \N(0, 1).
\]</span> Then we use our knowledge of the standard normal to find <span class="math display">\[
\P\left( -1.96 \leq \frac{\widehat{\theta}_n - \theta}{\widehat{\se}[\widehat{\theta}_n]} \leq 1.96\right) = 0.95.
\]</span> Multiplying each part of the inequality by <span class="math inline">\(\widehat{\se}[\widehat{\theta}_n]\)</span> gives us <span class="math display">\[
\P\left( -1.96\,\widehat{\se}[\widehat{\theta}_n] \leq \widehat{\theta}_n - \theta \leq 1.96\,\widehat{\se}[\widehat{\theta}_n]\right) = 0.95,
\]</span> We then subtract all parts by the estimator to get <span class="math display">\[
\P\left(-\widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n] \leq - \theta \leq -\widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n]\right) = 0.95,
\]</span> and finally we multiply all parts by <span class="math inline">\(-1\)</span> (and flipping the inequalities) to arrive at <span class="math display">\[
\P\left(\widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n] \leq \theta \leq \widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n]\right) = 0.95.
\]</span> To connect back to the definition of the confidence interval, we have now shown that the random interval <span class="math inline">\([L, U]\)</span> where <span class="math display">\[
\begin{aligned}
  L = L(X_1, \ldots, X_n) &amp;= \widehat{\theta}_n - 1.96\,\widehat{\se}[\widehat{\theta}_n] \\
  U = U(X_1, \ldots, X_n) &amp;= \widehat{\theta}_n + 1.96\,\widehat{\se}[\widehat{\theta}_n],
\end{aligned}
\]</span> is an asymptotically valid estimator.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Replacing <span class="math inline">\(\Xbar_n\)</span> for <span class="math inline">\(\widehat{\theta}_n\)</span> and <span class="math inline">\(s/\sqrt{n}\)</span> for <span class="math inline">\(\widehat{\se}[\widehat{\theta}_n]\)</span> establishes how the standard 95% confidence interval for the sample mean above is asymptotically valid.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-std-normal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-std-normal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-std-normal-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-std-normal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Critical values for the standard normal.
</figcaption>
</figure>
</div>
</div>
</div>
<p>How can we generalize this to <span class="math inline">\(1-\alpha\)</span> confidence intervals? For a random variable that is distributed following a standard normal, <span class="math inline">\(Z\)</span>, we know that <span class="math display">\[
\P(-z_{\alpha/2} \leq Z \leq z_{\alpha/2}) = 1-\alpha
\]</span> which implies that we can obtain a <span class="math inline">\(1-\alpha\)</span> asymptotic confidence intervals by using the interval <span class="math inline">\([L, U]\)</span>, where <span class="math display">\[
L = \widehat{\theta}_{n} - z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}], \quad U = \widehat{\theta}_{n} + z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}].
\]</span> This is sometimes shortened to <span class="math inline">\(\widehat{\theta}_n \pm z_{\alpha/2} \widehat{\se}[\widehat{\theta}_{n}]\)</span>. Remember that we can obtain the values of <span class="math inline">\(z_{\alpha/2}\)</span> easily from R:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## alpha = 0.1 for 90% CI</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.1</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.644854</code></pre>
</div>
</div>
<p>As a concrete example, then, we could derive a 90% asymptotic confidence interval for the sample mean as <span class="math display">\[
\left[\Xbar_{n} - 1.64 \frac{\widehat{\sigma}}{\sqrt{n}}, \Xbar_{n} + 1.64 \frac{\widehat{\sigma}}{\sqrt{n}}\right]
\]</span></p>
</section>
<section id="interpreting-confidence-intervals" class="level3" data-number="3.8.2">
<h3 data-number="3.8.2" class="anchored" data-anchor-id="interpreting-confidence-intervals"><span class="header-section-number">3.8.2</span> Interpreting confidence intervals</h3>
<p>A very important point is that the interpretation of confidence is how the random interval performs over repeated samples. A valid 95% confidence interval is a random interval that contains the true population value in 95% of samples. Simulating repeated samples helps clarify this.</p>
<div id="exm-cis" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8</strong></span> Suppose we are taking samples of size <span class="math inline">\(n=500\)</span> of random variables where <span class="math inline">\(X_i \sim \N(1, 10)\)</span>, and we want to estimate the population mean <span class="math inline">\(\E[X] = 1\)</span>. To do so, we repeat the following steps:</p>
<ol type="1">
<li>Draw a sample of <span class="math inline">\(n=500\)</span> from <span class="math inline">\(\N(1, 10)\)</span>.</li>
<li>Calculate the 95% confidence interval sample mean <span class="math inline">\(\Xbar_n \pm 1.96\widehat{\sigma}/\sqrt{n}\)</span>.</li>
<li>Plot the intervals along the x-axis and color them blue if they contain the truth (1) and red if not.</li>
</ol>
<p><a href="#fig-ci-sim" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> shows 100 iterations of these steps. We see that, as expected, most calculated CIs do contain the true value. Five random samples produce intervals that fail to include 1, an exact coverage rate of 95%. Of course, this is just one simulation, and a different set of 100 random samples might have produced a slightly different coverage rate. The guarantee of the 95% confidence intervals is that if we were to continue to take these repeated samples, the long-run frequency of intervals covering the truth would approach 0.95.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ci-sim" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-ci-sim-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: 95% confidence intervals from 100 random samples. Intervals are blue if they contain the truth and red if they do not.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="sec-delta-method" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="sec-delta-method"><span class="header-section-number">3.9</span> Delta method</h2>
<p>Suppose that we know that an estimator follows the CLT, and so we have <span class="math display">\[
\sqrt{n}\left(\widehat{\theta}_n - \theta \right) \indist \N(0, V),
\]</span> but we actually want to estimate <span class="math inline">\(h(\theta)\)</span> so we use the plug-in estimator, <span class="math inline">\(h(\widehat{\theta}_n)\)</span>. It seems like we should be able to apply part 1 of <a href="#thm-indist-properties" class="quarto-xref">Theorem&nbsp;<span>3.9</span></a> to obtain the asymptotic distribution of <span class="math inline">\(h(\widehat{\theta}_n)\)</span>. Still, the CLT established the large-sample distribution of the centered and scaled random sequence, <span class="math inline">\(\sqrt{n}(\widehat{\theta}_n - \theta)\)</span>, not to the original estimator itself, and we would need the latter to investigate the asymptotic distribution of <span class="math inline">\(h(\widehat{\theta}_n)\)</span>. We can use a little bit of calculus to get an approximation of the distribution we need.</p>
<div id="thm-delta-method" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.11</strong></span> If <span class="math inline">\(\sqrt{n}\left(\widehat{\theta}_n - \theta\right) \indist \N(0, V)\)</span> and <span class="math inline">\(h(u)\)</span> is continuously differentiable in a neighborhood around <span class="math inline">\(\theta\)</span>, then as <span class="math inline">\(n\to\infty\)</span>, <span class="math display">\[
\sqrt{n}\left(h(\widehat{\theta}_n) - h(\theta) \right) \indist \N(0, (h'(\theta))^2 V).
\]</span></p>
</div>
<p>Understanding what is happening here provides intuition as to when this might go wrong. Why do we focus on continuously differentiable functions, <span class="math inline">\(h()\)</span>? These functions can be well-approximated with a line in a neighborhood around a given point like <span class="math inline">\(\theta\)</span>. In <a href="#fig-delta" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>, we show this at the point where the tangent line at <span class="math inline">\(\theta_0\)</span>, which has slope <span class="math inline">\(h'(\theta_0)\)</span>, is very similar to <span class="math inline">\(h(\theta)\)</span> for values close to <span class="math inline">\(\theta_0\)</span>. Because of this, we can approximate the difference between <span class="math inline">\(h(\widehat{\theta}_n)\)</span> and <span class="math inline">\(h(\theta_0)\)</span> with the what this tangent line would give us: <span class="math display">\[
\underbrace{\left(h(\widehat{\theta_n}) - h(\theta_0)\right)}_{\text{change in } y} \approx \underbrace{h'(\theta_0)}_{\text{slope}} \underbrace{\left(\widehat{\theta}_n - \theta_0\right)}_{\text{change in } x},
\]</span> and then multiplying both sides by the <span class="math inline">\(\sqrt{n}\)</span> gives <span class="math display">\[
\sqrt{n}\left(h(\widehat{\theta_n}) - h(\theta_0)\right) \approx h'(\theta_0)\sqrt{n}\left(\widehat{\theta}_n - \theta_0\right).
\]</span> The right-hand side of this approximation converges to <span class="math inline">\(h'(\theta_0)Z\)</span>, where <span class="math inline">\(Z\)</span> is a random variable with <span class="math inline">\(\N(0, V)\)</span>. The variance of this quantity will be <span class="math display">\[
\V[h'(\theta_0)Z] = (h'(\theta_0))^2\V[Z] = (h'(\theta_0))^2V,
\]</span> by the properties of variances.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-delta" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-delta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-delta-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-delta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Linear approximation to nonlinear functions.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-log" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9</strong></span> Let’s return to the iid sample <span class="math inline">\(X_1, \ldots, X_n\)</span> with mean <span class="math inline">\(\mu = \E[X_i]\)</span> and variance <span class="math inline">\(\sigma^2 = \V[X_i]\)</span>. From the CLT, we know that <span class="math inline">\(\sqrt{n}(\Xbar_n - \mu) \indist \N(0, \sigma^2)\)</span>. Suppose that we want to estimate <span class="math inline">\(\log(\mu)\)</span>, so we use the plug-in estimator <span class="math inline">\(\log(\Xbar_n)\)</span> (assuming that <span class="math inline">\(X_i &gt; 0\)</span> for all <span class="math inline">\(i\)</span> so that we can take the log). What is the asymptotic distribution of this estimator? This is a situation where <span class="math inline">\(\widehat{\theta}_n = \Xbar_n\)</span> and <span class="math inline">\(h(\mu) = \log(\mu)\)</span>. From basic calculus, we know that <span class="math display">\[
h'(\mu) = \frac{\partial \log(\mu)}{\partial \mu} = \frac{1}{\mu},
\]</span> so applying the delta method, we can determine that <span class="math display">\[
\sqrt{n}\left(\log(\Xbar_n) - \log(\mu)\right) \indist \N\left(0,\frac{\sigma^2}{\mu^2} \right).
\]</span></p>
</div>
<div id="exm-exp" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.10</strong></span> What about estimating the <span class="math inline">\(\exp(\mu)\)</span> with <span class="math inline">\(\exp(\Xbar_n)\)</span>? Recall that <span class="math display">\[
h'(\mu) = \frac{\partial \exp(\mu)}{\partial \mu} = \exp(\mu)
\]</span> so applying the delta method, we have <span class="math display">\[
\sqrt{n}\left(\exp(\Xbar_n) - \exp(\mu)\right) \indist \N(0, \exp(2\mu)\sigma^2),
\]</span> since <span class="math inline">\(\exp(\mu)^2 = \exp(2\mu)\)</span>.</p>
</div>
<p>Like all of the results in this chapter, there is a multivariate version of the delta method that is incredibly useful in practical applications. For example, suppose we want to combine two different estimators (or two different estimated parameters) to estimate another quantity. We now let <span class="math inline">\(\mb{h}(\mb{\theta}) = (h_1(\mb{\theta}), \ldots, h_m(\mb{\theta}))\)</span> map from <span class="math inline">\(\mathbb{R}^k \to \mathbb{R}^m\)</span> and be continuously differentiable (we make the function bold since it returns an <span class="math inline">\(m\)</span>-dimensional vector). It will help us to use more compact matrix notation if we introduce a <span class="math inline">\(m \times k\)</span> Jacobian matrix of all partial derivatives <span class="math display">\[
\mb{H}(\mb{\theta}) = \mb{\nabla}_{\mb{\theta}}\mb{h}(\mb{\theta}) = \begin{pmatrix}
  \frac{\partial h_1(\mb{\theta})}{\partial \theta_1} &amp; \frac{\partial h_1(\mb{\theta})}{\partial \theta_2} &amp; \cdots &amp; \frac{\partial h_1(\mb{\theta})}{\partial \theta_k} \\
  \frac{\partial h_2(\mb{\theta})}{\partial \theta_1} &amp; \frac{\partial h_2(\mb{\theta})}{\partial \theta_2} &amp; \cdots &amp; \frac{\partial h_2(\mb{\theta})}{\partial \theta_k} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \frac{\partial h_m(\mb{\theta})}{\partial \theta_1} &amp; \frac{\partial h_m(\mb{\theta})}{\partial \theta_2} &amp; \cdots &amp; \frac{\partial h_m(\mb{\theta})}{\partial \theta_k}
\end{pmatrix},
\]</span> which we can use to generate the equivalent multivariate linear approximation <span class="math display">\[
\left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0)\right) \approx \mb{H}(\mb{\theta}_0)\left(\widehat{\mb{\theta}}_n - \mb{\theta}_0\right).
\]</span> We can use this fact to derive the multivariate delta method.</p>
<div id="thm-multivariate-delta" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.12</strong></span> Suppose that <span class="math inline">\(\sqrt{n}\left(\widehat{\mb{\theta}}_n - \mb{\theta}_0 \right) \indist \N(0, \mb{\Sigma})\)</span>, then for any function <span class="math inline">\(\mb{h}\)</span> that is continuously differentiable in a neighborhood of <span class="math inline">\(\mb{\theta}_0\)</span>, we have <span class="math display">\[
\sqrt{n}\left(\mb{h}(\widehat{\mb{\theta}}_n) - \mb{h}(\mb{\theta}_0) \right) \indist \N(0, \mb{H}\mb{\Sigma}\mb{H}'),
\]</span> where <span class="math inline">\(\mb{H} = \mb{H}(\mb{\theta}_0)\)</span>.</p>
</div>
<p>This result follows from the approximation above plus rules about variances of random vectors. Recall that for any compatible matrix of constants, <span class="math inline">\(\mb{A}\)</span>, we have <span class="math inline">\(\V[\mb{A}'\mb{Z}] = \mb{A}\V[\mb{Z}]\mb{A}'\)</span>. The matrix of constants appears twice here, like the matrix version of the “squaring the constant” rule for variance.</p>
<p>The delta method is handy for generating closed-form approximations for asymptotic standard errors, but the math is often quite complex for even simple estimators. It is usually more straightforward for applied researchers to use computational tools such as the bootstrap to approximate the needed standard errors. The bootstrap has the trade-off of taking more computational time to implement compared to the delta method, but it is more easily adaptable across different estimators and domains.</p>
</section>
<section id="summary" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.10</span> Summary</h2>
<p>In this chapter, we covered asymptotic analysis, which considers how estimators behave as we feed them larger and larger samples. While we never actually have infinite data, asymptotic results provide approximations that work quite well in practice. A <strong>consistent</strong> estimator converges in probability to a desired quantity of interest. We saw several ways of establishing consistency, including the <strong>Law of Large Numbers</strong> for the sample mean, which converges in probability to the population mean. The <strong>Central Limit Theorem</strong> tells us that the sample mean will be approximately normally distributed when we have large, iid samples. We also saw how the <strong>continuous mapping theorem</strong> and <strong>Slutsky’s theorem</strong> allow us to determine asymptotic results for a broad class of estimators. Knowing the asymptotic normality of an estimator allows us to derive <strong>confidence intervals</strong> that are valid in large samples. Finally, the <strong>delta method</strong> is a general tool for finding the asymptotic distribution of an estimator that is a function of another estimator with a known asymptotic distribution.</p>
<p>In the next chapter, we will leverage these asymptotic results to introduce another important tool for statistical inference: the hypothesis test.</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Due to Wasserman (2004), Chapter 5.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Technically, a sequence can also converge in probability to another random variable, but the use case of converging to a single number is much more common in evaluating estimators.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Implicit in this analysis is that the standard error estimate is consistent.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./estimation.html" class="pagination-link" aria-label="Model-based inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Model-based inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./hypothesis_tests.html" class="pagination-link" aria-label="Hypothesis tests">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mattblackwell/gov2002-book/edit/main/asymptotics.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>