[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimation, Inference, and Regression",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "\\(\\,\\)"
  },
  {
    "objectID": "02_estimation.html#introduction",
    "href": "02_estimation.html#introduction",
    "title": "2  Estimation",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWhen studying probability, we assumed that we knew the parameter of a distribution (the mean, the variance, etc), and we used probability to understand what kind of data we would observe. Now we will put this engine in reverse and try to learn about the data generating process or some feature of it using only the observed data that we have. There are two main goals here: estimation which is how we formulate our best guess about a parameter of the DGP, and inference which is how we formalize and express uncertainty about our estimates.\nInsert two-direction diagram\n\nExample 2.1 (Randomized control trial) Suppose we are conducting a randomized experiment on framing effects. All respondents receive some factual information about recent levels of immigration, but the message for the treatment group (\\(D_i = 1\\)) has an additional framing of the positive benefits of immigration and the control group (\\(D_i = 0\\)) receives no additional framing. The outcome is a binary outcome on whether the respondent supports increasing legal immigration limits (\\(Y_i = 1\\)) or not (\\(Y_i = 0\\)). The observed data consists of \\(n\\) pairs of random variables, the outcome and the treatment assignment: \\(\\{(Y_1, D_1), \\ldots, (Y_n, D_n)\\}\\). Define the two sample means/proportions in each group as \\[\n\\overline{Y}_1 = \\frac{1}{n_1} \\sum_{i: D_i = 1} Y_i,  \\qquad\\qquad \\overline{Y}_0 = \\frac{1}{n_0} \\sum_{i: D_i = 0} Y_i,\n\\] where \\(n_1 = \\sum_{i=1}^n D_i\\) is the number of treated units and \\(n_0 = n - n_1\\) is the number of control units.\nA common estimator for the treatment effect in a study like this would be the difference in means, \\(\\overline{Y}_1 - \\overline{Y}_0\\). But this is only one possible estimator. We could also estimate the effect by taking this difference in means separately by party identification and then averaging those party-specific effects by the size of those groups. This is commonly called a poststratification estimator, but it’s unclear at first glance which of these two estimators we should prefer.\n\nWhat are the goals of studying estimators? In short, we prefer to use good estimators rather than bad estimators. But what makes an estimator good or bad? You probably have some intuitive sense that, say, an estimator that returns the value 3 is bad, but it will be helpful for us to formally define and explore some properties of estimators that will allow us to compare them and choose the good over the bad."
  },
  {
    "objectID": "02_estimation.html#samples-and-populations",
    "href": "02_estimation.html#samples-and-populations",
    "title": "2  Estimation",
    "section": "2.2 Samples and populations",
    "text": "2.2 Samples and populations\nFor most of this class, we’ll focus on a relatively simple setting where we have a random vectors \\(\\{X_1, \\ldots, X_n\\}\\) that are independent and identically distributed (iid) draws from a distribution with cumulative distribution function (cdf) \\(F\\). They are independent in the sense that the random vectors \\(X_i\\) and \\(X_j\\) are independent for all \\(i \\neq j\\), and the they are identically distributed in the sense that each of the random variables \\(X_i\\) have the same marginal distribution, \\(F\\).\nYou can think of each of these vectors, \\(X_i\\), as the rows in your data frame. Note that we’re being purposely vague about this cdf—it just represents the unknown distribution of the data, otherwise known as the data generating process (DGP). Sometimes \\(F\\) is also referred to as the population distribution or even just population, which has its roots in viewing the data as a random sample from some larger population. As a shorthand, we often say that the collection of random vectors \\(\\{X_1, \\ldots, X_n\\}\\) is a random sample from population \\(F\\) if \\(\\{X_1, \\ldots, X_n\\}\\) is iid with distribution \\(F\\). The sample size \\(n\\) is the number of units in the sample.\nThere are two metaphors that can help build intuition about the concept of viewing the data as an iid draw from \\(F\\):\n\nRandom sampling. Suppose we have a population of size \\(N\\) which is much, much larger than our sample size \\(n\\), and we take a simple random sample of size \\(n\\) from this population. Then the distribution of the data in the random sample will be iid draws from the population distribution of the variables we are sampling. For instance, suppose we take a random sample from a population of US citizens where the population proportion of Democratic party identifiers is 0.33. Then if we randomly sample \\(n = 100\\) US citizens, each data point \\(X_i\\) will be distributed Bernoulli with probability of success 0.33.\nGroundhog Day. Random sampling does not always make sense as a justification for iid data, especially when the units are not really samples at all, but rather countries, states, or subnational units. In this case, we have to appeal to thought experiment, where \\(F\\) represents the fundamental uncertainty about how the data was generated. The metaphor here is that if we could re-run history many times, like the 1993 American classic comedy Groundhog Day, data and outcomes would change slightly due to the inherently stochastic nature of the world. The iid assumption, then, is that each of the units in our data have the same DGP producing this data or the same distribution of outcomes under the Groundhog Day scenario.[1]\n\nNote that there are many, many situations where the iid assumption does not make sense. We will cover some of those later in the semester. But much of the innovation and growth in statistics over the last, say, 50 years has been in figuring out how to do statistical inference when iid is violated and often the solutions are specific to the type of iid violation you have (spatial, time-series, network, clustered, etc). As a rule of thumb, though, if you suspect a violation of iid data, your statements of uncertainty will likely be overconfident (for example, confidence intervals, which we’ll cover later, are too small)."
  },
  {
    "objectID": "02_estimation.html#point-estimation",
    "href": "02_estimation.html#point-estimation",
    "title": "2  Estimation",
    "section": "2.3 Point estimation",
    "text": "2.3 Point estimation\n\n2.3.1 Quantities of interest\nOur goal is to learn about the data generating process, represented by the cdf, \\(F\\). We might be interested in estimating the cdf at a general level or we might only be interested in estimating some feature of the distribution, like a mean or conditional expectation function. We will almost always have a particular goal in mind, but it’s useful to introduce the idea of estimation in a general way since most of the concepts about estimation, so we’ll let \\(\\theta\\) represent the quantity of interest. Point estimation is the process of providing a single “best guess” about theta whatever quantity of interest we choose, \\(\\theta\\).\n\n\n\n\n\n\nNote\n\n\n\nQuantities of interest are also referred to as parameters or estimands (that is, the target of estimation).\n\n\n\nExample 2.2 (Population mean) Suppose we wanted to know the proportion of US citizens who support increasing the level of legal immigration in the US, which we denote as \\(Y_i = 1\\). Then our quantity of interest is the mean of this random variable, \\(\\mu = \\mathbb{E}[Y_i]\\), which is also the probability of randomly drawing someone from the population that supports increasing legal immigration.\n\n\nExample 2.3 (Population variance) Feeling thermometer scores are a very common way to assess how a survey respondent feels about a particular person or group of people. Respondents are asked how warmly they feel about a group from 0 to 100, which we will denote \\(Y_i\\). We might be interested in how polarized views are on a group in the population and one measure of polarization could the be variance, or spread, of the distribution of \\(Y_i\\) around the mean. In this case, \\(\\sigma^2 = \\mathbb{V}[Y_i]\\) would be our quantity of interest.\n\n\nExample 2.4 (RCT, continued) In Example 2.1 we discussed a common estimator for an experimental study with a binary treatment. The goal of that experiment is to learn about the difference between two conditional probabilities (or expectations): the average support for increasing legal immigration in the treatment group, \\(\\mu_1 = \\mathbb{E}[Y_i \\mid D_i = 1]\\), and the same average in the control group, \\(\\mu_0 = \\mathbb{E}[Y_i \\mid D_i = 0]\\). That is, we want to know about \\(\\mu_1 - \\mu_0\\), a function of unknown features of these two conditional distributions.\n\nEach of these is a function of the (possibly joint) distribution of the data, \\(F\\). In each of these, we are not necessarily interested in the entire distribution, just summaries of it (central tendency, spread). Of course there are situations where we are also interested in the complete distribution.\n\n\n2.3.2 Estimators\nWhen our sample size is more than a few observations, it makes no sense to work with the raw data, \\(X_1, \\ldots, X_n\\), and we inevitably will need to summarize the data in some way. We can represent this summary as a function, \\(g(x_1, \\ldots, x_n)\\), which might the formula for the sample mean or sample variance. This function is just a regular function that takes in \\(n\\) numbers (or vectors) and returns a number (or vector). We can also define a random variable based on this function, \\(Y = g(X_1, \\ldots, X_n)\\), which inherits its randomness from the randomness of the data: before we see the data, we don’t know what values of \\(X_1, \\ldots, X_n\\) we will see and so we don’t know what value of \\(Y\\) we’ll see either. We call the random variable \\(Y = g(X_1, \\ldots, X_n)\\) a statistic (or sometimes sample statistics) and we refer to the probability distribution of a statistic \\(Y\\) as the sampling distribution of \\(Y\\).\n\n\n\n\n\n\nWarning\n\n\n\nThere is one potential confusion in how we talk about “statistics.” Just above we defined a statistic as a random variable, based on it being a function of random variables (ie, the data). But we also sometimes refer to the calculated value as a statistic as well, which is a specific number that you see in your R output. To be precise, we should call the latter the realized value of the statistic, but message discipline is difficult to enforce in this context. A simple example might help. Suppose that \\(X_1\\) and \\(X_2\\) are the result of a roll of two standard six-sided dice. Then the statistic \\(Y = X_1 + X_2\\) is a random variable that has a distribution over the numbers from {2, , 12} that describes our uncertainty over what the sum will be before we roll the dice. Once, we roll the dice and observed the realized values \\(X_1 = 3\\) and \\(X_2 = 4\\), we observed the realized value of the statistic, \\(Y = 7\\).\n\n\nAt their most basic, statistics are just summaries of the data without aim or ambition. Estimators are statistics with a purpose: to provide an “educated guess” about some quantity of interest.\n\nDefinition 2.1 An estimator \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\) for some parameter \\(\\theta\\), is a statistic intended as a guess about \\(\\theta\\).\n\nOne important distinction of jargon is between an estimator and an estimate, similar to issues of statistic above. The estimator is a function of data, whereas the estimate is the realized value of the estimator once we see the data. An estimate is a single number, such as 0.38, whereas the estimator is a random variable that has uncertainty over what value it will take. Formally, the estimate is \\(\\theta(x_1, \\ldots, x_n)\\) when the data is \\(\\{X_1, \\ldots, X_n\\} = \\{x_1, \\ldots, x_n\\}\\), whereas we represent the estimator as a function of random variables, \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\n\n\n\n\n\nNote\n\n\n\nIt is very common, though not universal, to use the “hat” notation to define an estimator along with its estimand. For example, \\(\\widehat{\\theta}\\) (or “theta hat”) indicates that this estimator is targeting the parameter \\(\\theta\\).\n\n\n\nExample 2.5 (Estimators for population mean) Suppose we would like to estimate the population mean of \\(F\\), which we will represent as \\(\\mu = \\mathbb{E}[X_i]\\). There are several estimators that we could choose from, all with different properties. \\[\n\\widehat{\\theta}_{n,1} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\quad \\widehat{\\theta}_{n,2} = X_1, \\quad \\widehat{\\theta}_{n,3} = \\text{max}(X_1,\\ldots,X_n), \\quad \\widehat{\\theta}_{n,4} = 3\n\\] The first is just the sample mean, which is an intuitive and natural estimator for the population mean. The second just uses the first observation. While this seems silly, this is a valid statistic (it’s a function of the data!). The third just takes the maximum value in the sample, and the fourth always returns 3 no matter what the data says."
  },
  {
    "objectID": "02_estimation.html#how-to-find-estimators",
    "href": "02_estimation.html#how-to-find-estimators",
    "title": "2  Estimation",
    "section": "2.4 How to find estimators",
    "text": "2.4 How to find estimators\nWhere do estimators come from? There are a couple of different methods that I’ll cover briefly here before describing the ones that will form the bulk of this class.\n\n2.4.1 Parametric models and maximum likelihood\nThe first method for generating estimators is based on parametric models, where the researcher specifies the exact distribution (up to some unknown parameters) of the DGP. Let \\(\\theta\\) be the parameters of this distribution and we then write \\(\\{X_1, \\ldots, X_n\\}\\) are iid draws from \\(F_{\\theta}\\). We should also formally state the set of possible values that the parameters can take, which we call the parameter space and usually denote as \\(\\Theta\\). Because we’re assuming we know the distribution of the data, we can write the p.d.f. as \\(f(X_i \\mid \\theta)\\) and define the likelihood function as the product of these p.d.f.s over the units as a function of the parameters: \\[\nL(\\theta) = \\prod_{i=1}^n f(X_i \\mid \\theta).\n\\] We can then define the maximum likelihood estimator (MLE) for \\(\\theta\\) as the values of the parameter that, well, maximize the likelihood: \\[\n\\widehat{\\theta}_{mle} = \\mathop{\\mathrm{arg\\,max}}_{\\theta \\in \\Theta} \\; L(\\theta)\n\\] Sometimes we can use calculus to derive a closed-form expression for the MLE, but more often we will use iterative techniques that essentially search the parameter space for the maximum.\nMaximum likelihood estimators have very nice properties, especially in large samples. Unfortunately, it also requires the correct knowledge of the parametric model, which is often difficult to justify. Do we really know if a given event count variable should be modeling as Poisson or Negative Binomial? The nice properties of MLE are only as good as our ability to get these types of choices correct.\n\n\n\n\n\n\nNo free lunch\n\n\n\nOne really important intuition to build about statistics is the assumptions-precision tradeoff. You can usually get more precise estimates if you make stronger and potentially more fragile assumptions. Conversely, if you want to weaken your assumptions, you will almost always get less precise estimates.\n\n\n\n\n2.4.2 Plug-in estimators\nThe second broad class of estimators are semiparametric in the sense that we will specify some finite-dimensional parameters of the DGP, but leave the rest of the distribution unspecified. For example, we might specify a population mean, \\(\\mu = \\mathbb{E}[X_i]\\), and a population variance, \\(\\sigma^2 = \\mathbb{V}[X_i]\\) but leave unrestricted the shape of the distribution. This is really important because our estimators will not be as dependent on correctly specifying distributions that maybe have no business specifying.\nThe basic method for constructing estimators in this setting is to use the plug-in estimator, or the estimator that replaces any population mean with a sample mean. Obviously in the case of estimating the population mean, \\(\\mu\\), this means we will use the sample mean as its estimate: \\[\n\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\quad \\text{estimates} \\quad \\mathbb{E}[X_i] = \\int_{\\mathcal{X}} x f(x)dx\n\\] What are we doing here? We are replacing the unknown population distribution \\(f(x)\\) that’s in the population mean with a discrete uniform distribution over our data points, with \\(1/n\\) probability assigned to each unit. Why do this? It encodes that if we have a random sample, our best guess about the population distribution of \\(X_i\\) is the sample distribution in our actual data. If this intuition fails, it is also fine to hold onto an analog principle: sample means of random variables are natural estimators of population means.\nWhat about estimating something more complicated like a the expected value of a function of the data, \\(\\theta = \\mathbb{E}[r(X_i)]\\)? The key is to see that \\(f(X_i)\\) is also a random variable. Let’s call this random variable \\(Y_i = f(X_i)\\) now we can see that \\(\\theta\\) is just the population expectation of this random variable and using the plug-in estimator, we get: \\[\n\\widehat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\nWith these facts in hand, we can describe the more general plug-in estimator. When we want to estimate some quantity of interest that is a function of population means, we can generate a plug-in estimator by replacing any population mean with a sample mean. Formally, let \\(\\alpha = g\\left(\\mathbb{E}[r(X_i)]\\right)\\) be a parameter that is defined as a function of the population mean of a (possibly vector-valued) function of the data. Then, we can estimate this parameter by plugging in the sample mean for the population mean to get the plug-in estimator, \\[\n\\widehat{\\alpha} = g\\left( \\frac{1}{n} \\sum_{i=1}^n r(X_i) \\right) \\quad \\text{estimates} \\quad \\alpha = g\\left(\\mathbb{E}[r(X_i)]\\right)\n\\] This approach to plug-in estimation with sample means is very general and will allow us to derive estimators in a large variety of settings.\n\nExample 2.6 (Estimating population variance) The population variance of a random variable is \\(\\sigma^2 = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])^2]\\). To derive a plug-in estimator for this quantity, we replace the inner \\(\\mathbb{E}[X_i]\\) with \\(\\overline{X}_n\\) and the outer expectation with another sample mean: \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2.\n\\] Note that this plug-in estimator is not the same as the usual sample variance, which divides by \\(n - 1\\) rather than \\(n\\), but this is a very minor difference that does not matter in moderate to large samples.\n\n\nExample 2.7 (Estimating population covariance) Suppose we have two variables, \\((X_i, Y_i)\\). A natural quantity of interest here is the population covariance between these variables, \\[\n\\sigma_{xy} = \\text{Cov}[X_i,Y_i] = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])(Y_i-\\mathbb{E}[Y_i])],\n\\] which has the plug-in estimator, \\[\n\\widehat{\\sigma}_{xy} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)(Y_i - \\overline{Y}_n).\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nGiven the connection between the population mean and the sample mean, you will sometimes see the \\(\\mathbb{E}_n[\\cdot]\\) operator used as a short hand for the sample average: \\[\n\\mathbb{E}_n[r(X_i)] \\equiv \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\n\n\nFinally, we are describing the plug-in estimator only considering replacing population means with sample means, but the idea of plug-in estimation is actually much more broad. We can derive estimators of the population quantiles like the median with sample versions of those quantities. What unifies all of these approaches is replacing the unknown population cdf, \\(F\\), with the empirical cdf, \\[\n\\widehat{F}_n(x) = \\frac{\\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}{n}.\n\\] For a more complete and technical treatment of these ideas, see Wasserman (2004) Chapter 7."
  },
  {
    "objectID": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "href": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "title": "2  Estimation",
    "section": "2.5 The three distributions: population, empirical, and sampling",
    "text": "2.5 The three distributions: population, empirical, and sampling\nOnce we start to wade into estimation, there are several distributions to keep track of and things can quickly become confusing. There are three distributions that are all related and easy to confuse but it’s important to keep them distinct.\nThe population distribution is the distribution of the random variable, \\(X_i\\), which we have labeled \\(F\\). This is the distribution that we want to learn about. Then there is the empirical distribution, which is the distribution of the actual realizations of the random variables in our samples (that is, the numbers in our data frame), \\(X_1, \\ldots, X_n\\). Because this is a random sample from the population distribution and can serve as estimator of \\(F\\), we sometimes call this \\(\\widehat{F}_n\\).\nInsert Sampling distribution figure here\nSeparately from both of these is the sampling distribution of an estimator, which is the probability distribution of \\(\\widehat{\\theta}_n\\). It represents our uncertainty about what our estimate will be before we see the data. Remember that our estimator is itself a random variable because it is a function of random variables: the data itself. That is, we defined the estimator as \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\nExample 2.8 (Likert resposes) Suppose \\(X_i\\) is the answer to a question, “How much do you agree with the following statement: Immigrants are a net positive for the United States,” with a \\(X_i = 0\\) being “strongly disagree,” \\(X_i = 1\\) being “disgree”, \\(X_i = 2\\) being “neither agree nor disagree”, \\(X_i = 3\\) being “agree”, and \\(X_i = 4\\) being “strongly agree.”\nThe population distribution describes the probability of randomly selecting a person with each one of these values, \\(\\mathbb{P}(X_i = x)\\). The empirical distribution would be the actual numeric values 0-4 in our data. And the sampling distribution of the sample mean, \\(\\overline{X}_n\\), would be the distribution of the sample mean across repeated samples from the population.\nSuppose that the population distribution was binomial with 4 trials and probability of success \\(p = 0.4\\). We could generate one sample with \\(n = 10\\) and thus one empirical distribution using rbinom:\n\n\n\n\nmy_samp <- rbinom(n = 10, size = 4, prob = 0.4)\nmy_samp\n\n [1] 1 2 1 3 3 0 2 3 2 1\n\ntable(my_samp)\n\nmy_samp\n0 1 2 3 \n1 3 3 3 \n\n\nAnd we can generate one draw from the sampling distribution of \\(\\overline{X}_n\\) by taking the mean of this sample:\n\nmean(my_samp)\n\n[1] 1.8\n\n\nBut obviously if we had a different sample, it would have a different empirical distribution and thus give us a different estimate of the sample mean:\n\nmy_samp2 <- rbinom(n = 10, size = 4, prob = 0.4)\nmean(my_samp2) \n\n[1] 1.6\n\n\nThe sampling distribution is the distribution of these sample means across repeated sampling."
  },
  {
    "objectID": "02_estimation.html#finite-sample-properties-of-estimators",
    "href": "02_estimation.html#finite-sample-properties-of-estimators",
    "title": "2  Estimation",
    "section": "2.6 Finite-sample properties of estimators",
    "text": "2.6 Finite-sample properties of estimators\nAs we discussed when we introduced estimators, their usefulness is tied to how well they help us learn about the quantity of interest. If we get an estimate \\(\\widehat{\\theta} = 1.6\\), we would like to know that this is “close” to the true parameter \\(\\theta\\). The key to understanding how the behavior of our estimators is the sampling distribution. Intuitively, we would like the sampling distribution of \\(\\widehat{\\theta}_n\\) to be as tightly clustered around the true as \\(\\theta\\) as possible. Here, though, we run into a problem: the sampling distribution depends on the population distribution since it is about repeated samples of the data from that distribution filtered through the function \\(\\theta()\\). Since \\(F\\) is unknown, this implies that the sampling distribution will also usually be unknown.\nEven though we cannot precisely pin down the entire sampling distribution, we will be able to use assumptions to derive certain properties of the sampling distribution that will be useful in comparing estimators.\n\n2.6.1 Bias\nThe first property of the sampling distribution concerns its central tendency. In particular, we will define the bias (or estimation bias) of estimator \\(\\widehat{\\theta}\\) for parameter \\(\\theta\\) as \\[\n\\text{bias}[\\widehat{\\theta}] = \\mathbb{E}[\\widehat{\\theta}] - \\theta.\n\\] This is the difference between the mean of the estimator (across repeated samples) and the true parameter. All else equal, we would like estimation bias to be as small as possible. The smallest possible bias, obviously, is 0 and we define an unbiased estimator as one with with \\(\\text{bias}[\\widehat{\\theta}] = 0\\) or equivalently, \\(\\mathbb{E}[\\widehat{\\theta}] = \\theta\\).\nHowever, all else is not always equal and unbiasedness is not a property to become overly attached to. There are many biased estimators that have other attractive properties and many popular modern estimators are biaseed.\n\nExample 2.9 (Unbiasedness of the sample mean) We can show that the sample mean is unbiased for the population mean when the data is iid and \\(\\mathbb{E}|X| < \\infty\\). In particular, we simply apply the rules of expectations: \\[\\begin{aligned}\n\\mathbb{E}\\left[ \\overline{X}_n \\right] &= \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] & (\\text{definition of } \\overline{X}_n) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}[X_i] & (\\text{linearity of } \\mathbb{E})\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu & (X_i \\text{ identically distributed})\\\\\n&= \\mu.\n\\end{aligned}\\] Notice that we only used the “identically distributed” part of iid. Independence is not needed.\n\n\n\n\n\n\n\nWarning\n\n\n\nProperties like unbiasedness might only hold for a subset of DGPs. For example, we just showed that the sample mean is unbiased, but only when the population mean is finite. There are probability distribution like the Cauchy where the expected value diverges and is not finite. So we are dealing with a restricted class of DGPs that rules out such distributions. You may see this sometimes formalized by defining a class \\(\\mathcal{F}\\) of distributions and unbiasedness might hold in that class if it is unbiased for all \\(F \\in \\mathcal{F}\\).\n\n\n\n\n2.6.2 Estimation variance and the standard error\nIf a “good” estimator tends to be close to the truth, then we should also care how spread out the sampling distribution is. In particular, we define the sampling variance as the variance of an estimator’s sampling distribution, \\(\\mathbb{V}[\\widehat{\\theta}]\\). This measures how spread the estimator it is around its mean. For an unbiased estimator, smaller sampling variance implies the distribution of \\(\\widehat{\\theta}\\) is more concentrated around the truth.\n\nExample 2.10 (Sampling variance of the sample mean:) We can establish the sampling variance of the sample mean of iid data for all \\(F\\) such that \\(\\mathbb{V}[X_i]\\) is finite (more precisely, \\(\\mathbb{E}[X_i^2] < \\infty\\))\n\\[\\begin{aligned}\n  \\mathbb{V}\\left[ \\overline{X}_n \\right] &= \\mathbb{V}\\left[ \\frac{1}{n} \\sum_{i=1}^n X_i \\right] & (\\text{definition of } \\overline{X}_n) \\\\\n                           &=\\frac{1}{n^2} \\mathbb{V}\\left[ \\sum_{i=1}^n X_i \\right] & (\\text{property of } \\mathbb{V})\\\\\n                           &=\\frac{1}{n^2} \\sum_{i=1}^n \\mathbb{V}[X_i] & (\\text{independence})\\\\\n                           &= \\frac{1}{n^2}\\sum_{i=1}^n \\sigma^2 & (X_i \\text{ identically distributed})\\\\\n                           &= \\frac{\\sigma^2}{n}\n\\end{aligned}\\]\n\nAn alternative measure of spread for any distribution is the standard deviation, which is on the same scale as the original random variable. We call the standard deviation of the sampling distribution of \\(\\widehat{\\theta}\\) the standand error of \\(\\widehat{\\theta}\\): \\(\\textsf{se}(\\widehat{\\theta}) = \\sqrt{\\mathbb{V}[\\widehat{\\theta}]}\\).\nGiven the above derivation the standard error of the sample mean under iid sampling is \\(\\sigma / \\sqrt{n}\\)\n\n\n2.6.3 Mean squared error\nBias and sampling variance clearly got at two different aspects of being a “good” estimator. Ideally, we want the estimator to be as close as possible to the true value. One summary measure of the quality of an estimator is the mean squared error or MSE which is defined as. \\[\n\\text{MSE} = \\mathbb{E}[(\\widehat{\\theta}_n-\\theta)^2]\n\\] Ideally, we would have this be as small as possible!\nWe can also relate the MSE to the bias and the sampling variance (provided it is finite) with the following decomposition result: \\[\n\\text{MSE} = \\text{bias}[\\widehat{\\theta}_n]^2 + \\mathbb{V}[\\widehat{\\theta}_n]\n\\] This decomposition implies that, for unbiased estimators, MSE is the sampling variance. It also highlights why we might accept some bias for large reductions in variance for lower overall MSE.\n\n\n\n\n\nTwo sampling distributions\n\n\n\n\nIn this figure, we show the sampling distributions of two estimators, \\(\\widehat{\\theta}_a\\), which is unbiased (centered on the true value \\(\\theta\\)) but with a high sampling variance, and \\(\\widehat{\\theta}_b\\) which is slightly biased but with much lower sampling variance. Even though \\(\\widehat{\\theta}_b\\) is biased, the probability of drawing a value close to the truth is higher than for \\(\\widehat{\\theta}_a\\). This type of balancing between bias and variance is exactly what the MSE helps capture and indeed, in this case, \\(MSE[\\widehat{\\theta}_b] < MSE[\\widehat{\\theta}_a]\\)."
  },
  {
    "objectID": "03_asymptotics.html#introduction",
    "href": "03_asymptotics.html#introduction",
    "title": "3  Asymptotics",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn the last chapter, we defined estimators and started to investigate their finite-sample properties like unbiasedness and the sampling variance. We call these “finite-sample” properties because establishing them generally does not depend on the sample size. We saw that under iid data, the sample mean is unbiased for the population mean, but this result holds as much for \\(n = 10\\) as it does for \\(n = 1,000,000\\). But these properties are also of limited use: we only learn the center and spread of the sampling distribution of \\(\\overline{X}_n\\) from these results. What about the shape of the distribution? We can often derive the shape if we are willing to make certain assumptions on the underlying data (for example, if the data is normal, then the sample means will be normal as well), but this approach is brittle: if our parametric assumption is false, we’re back to square one.\nIn this chapter, we’re going to take a different approach and see what happens to the sampling distribution of estimators as the sample size gets large. The study of the estimators as the sample size goes to infinity is called asymptotic theory, but it’s important to understand everything we do with asymptotics will be an approximation. No one ever has infinite data, but we hope that as our samples get larger, the approximations will be closer to the truth. Why work in this asymptopia, though? It turns out that many expressions are much easier to derive in the limit than in finite samples."
  },
  {
    "objectID": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "href": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "title": "3  Asymptotics",
    "section": "3.2 Why convergence with probability is hard",
    "text": "3.2 Why convergence with probability is hard\nIt’s helpful to review the basic idea of convergence in deterministic sequences from calculus:\n\nDefinition 3.1 A sequence \\(\\{a_n: n = 1, 2, \\ldots\\}\\) has the limit \\(a\\) written \\(a_n \\rightarrow a\\) as \\(n\\rightarrow \\infty\\) of \\(\\lim_{n\\rightarrow \\infty} a_n = a\\) if for all \\(\\epsilon > 0\\) there is some \\(n_{\\epsilon} < \\infty\\) such that for all \\(n \\geq n_{\\epsilon}\\), \\(|a_n - a| \\leq \\epsilon\\).\n\nWe say that \\(a_n\\) converges to \\(a\\) if \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Basically, a sequence converges to a number if the sequence gets closer and closer to that number as the sequence goes on.\nCan we apply this same idea to sequences of random variables (like estimators)? Let’s look at a few examples that might help clarify the difficult in doing so.1 Let’s say that we have a sequence of \\(a_n = a\\) for all \\(n\\) (that is, a constant sequence). Then obviously \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Now let’s say we have a sequence of random variables, \\(X_1, X_2, \\ldots\\), that are all independent with a standard normal distribution, \\(N(0,1)\\). From the analogy to the deterministic case, it is tempting to say that \\(X_n\\) converges to \\(X \\sim N(0, 1)\\), but notice that because they are all different random variables, \\(\\mathbb{P}(X_n = X) = 0\\). Thus, we need to be careful about saying how one variable converges to another variable.\nAnother example highlights subtle problems with a sequence of random variables converging to a single value. Suppose we have a sequence of random variables \\(X_1, X_2, \\ldots\\) where \\(X_n \\sim N(0, 1/n)\\). Clearly, \\(X_n\\) will be concentrated around 0 for large values of \\(n\\), so it is tempting to say that \\(X_n\\) converges to 0. But notice that \\(\\mathbb{P}(X_n = 0) = 0\\) because of the nature of continuous random variables."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "href": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "title": "3  Asymptotics",
    "section": "3.3 Convergence in probability and consistency",
    "text": "3.3 Convergence in probability and consistency\nThere are several different ways that a sequence of random variance can converge. The first type of convergence deals with sequence converging to a single value.2\n\nDefinition 3.2 A sequence of random variables, \\(X_1, X_2, \\ldots\\), is said to converge in probability to a value \\(b\\) if for every \\(\\varepsilon > 0\\), \\[\n\\mathbb{P}(|X_n - b| > \\varepsilon) \\rightarrow 0,\n\\] as \\(n\\rightarrow \\infty\\). We write this \\(X_n \\overset{p}{\\to}b\\).\n\nWith deterministic sequences, we said that \\(a_n\\) converges to \\(a\\) is it gets closer and closer to \\(a\\) as \\(n\\) gets bigger. For convergence in probability, the sequence of random variables converges to \\(b\\) is the probability that random variables are far away from \\(b\\) get smaller and smaller as \\(n\\) gets big.\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will sometimes see convergence in probability written as \\(\\text{plim}(Z_n) = b\\) if \\(Z_n \\overset{p}{\\to}b\\), \\(\\text{plim}\\) stands for “probability limit.”\n\n\nConvergence in probability is incredibly useful for evaluating estimators. While we said that unbiasedness was not the be all and end all of properties of estimators, the following property is a fairly basic and fundamental property that we would like all good estimators to have.\n\nDefinition 3.3 An estimator is consistent if \\(\\widehat{\\theta}_n \\overset{p}{\\to}\\theta\\).\n\nConsistency of an estimator implies that the sampling distribution of this estimator “collapses” on the true value as the sample size gets large. We say an estimator is inconsistent if it converges in probability to any other value, which is obviously a very bad property of an estimator. It means that as the sample size gets large, the probability that the estimator will be close to the truth will approach 0.\nWe can also define convergence in probability for a sequence of random vectors, \\(\\boldsymbol{X}_1, \\boldsymbol{X}_2, \\ldots\\), where \\(\\boldsymbol{X}_i = (X_{i1}, \\ldots, X_{ik})\\) is a random vector of length \\(k\\). This sequence convergences in probbaility to a vector \\(\\boldsymbol{b} = (b_1, \\ldots, b_k)\\) if and only if each random variable in the vector converges to the corresponding element in \\(\\boldsymbol{b}\\), or that \\(X_{nj} \\overset{p}{\\to}b_j\\) for all \\(j = 1, \\ldots, k\\)."
  },
  {
    "objectID": "03_asymptotics.html#useful-inequalities",
    "href": "03_asymptotics.html#useful-inequalities",
    "title": "3  Asymptotics",
    "section": "3.4 Useful inequalities",
    "text": "3.4 Useful inequalities\nAt first glance, it appears establishing consistency of an estimator will be difficult. How can we know if a distribution will collapse to a specific value without knowing the shape or family of the distribution? It turns out that there are certain relationships between the mean and variance of a random variable and certain probability statements that hold for all distributions (that have finite variance at least). This will be incredibly helpful to us.\n\nTheorem 3.1 (Markov Inequality) For any r.v. \\(X\\) and any \\(\\delta >0\\), \\[\n\\mathbb{P}(|X| \\geq \\delta) \\leq \\frac{\\mathbb{E}[|X|]}{\\delta}.\n\\]\n\n\nProof. Notice that we can let \\(Y = |X|/\\delta\\) and rewrite the statement as \\(\\mathbb{P}(Y \\geq 1) \\leq \\mathbb{E}[Y]\\) (since \\(E[|X|]/\\delta = \\mathbb{E}[|X|/\\delta]\\) by the properties of expectation), which is what we will show. But notice that \\[\n\\mathbb{1}(Y \\geq 1) \\leq Y.\n\\] Why does this hold? We can investigate the two possible values of the indicator function to see. If \\(Y\\) is less than 1, then the indicator function will be 0, but \\(Y\\) is non-negative so we know that it must be at least as big as 0 so that inequality holds. If \\(Y \\geq 1\\) then the indicator function 1 but we just said that \\(Y \\geq 1\\) so the inequality holds. If we take the expectation of both sides of this inequality, we obtain the result (remember the expectation of an indicator function is the probability of the event being indicated).\n\nIn words, Markov’s inequality says that the probability of a random variable being large in magnitude cannot be high if the average is not large in magnitude. Blitzstein and Hwang 2019) provide a nice intuition behind this result. Let \\(X\\) be the income of a randomly selected individual in a population and set \\(\\delta = 2\\mathbb{E}[X]\\), so that the inequality becomes \\(\\mathbb{P}(X > 2\\mathbb{E}[X]) < 1/2\\) (assuming that all income is nonnegative). Here, the inequality says that the share of the population that has an income twice the average must be less than 0.5, since if more than half the population was making twice the average income then the average would have to be higher.\nIt’s quite astounding how general this result is since it holds for all random variables. Of course, its generality comes at the expense of not being very informative. If \\(\\mathbb{E}[|X|] = 5\\), for instance, the inequality tells us that \\(\\mathbb{P}(|X| \\geq 1) \\leq 5\\) which is not very helpful since we already know that probabilities are less than 1! If we are willing to make some assumptions about \\(X\\), we can get tighter bounds.\n\nTheorem 3.2 (Chebyshev Inequality) Suppose that \\(X\\) is r.v. for which \\(\\mathbb{V}[X] < \\infty\\). Then, for every real number \\(\\delta > 0\\), \\[\n\\mathbb{P}(|X-\\mathbb{E}[X]| \\geq \\delta) \\leq \\frac{\\mathbb{V}[X]}{\\delta^2}.\n\\]\n\n\nProof. To prove this, we only need to square both sides of the inequality inside the probability statement and apply Markov’s inequality: \\[\n\\mathbb{P}\\left( |X - \\mathbb{E}[X]| \\geq \\delta \\right) = \\mathbb{P}((X-\\mathbb{E}[X])^2 \\geq \\delta^2) \\leq \\frac{\\mathbb{E}[(X - \\mathbb{E}[X])^2]}{\\delta^2} = \\frac{\\mathbb{V}[X]}{\\delta^2},\n\\] with the last equality holding by the definition of variance.\n\nThis is a straightforward extension of the Markov result: the probability of a random variable being far away from its mean (that is, \\(|X-\\mathbb{E}[X]|\\) being large) is limited by the variance of the random variable. If we let \\(\\delta = c\\sigma\\), where \\(\\sigma\\) is the standard deviation of \\(X\\), then we can use this result to bound the normalized: \\[\n\\mathbb{P}\\left(\\frac{|X - \\mathbb{E}[X]|}{\\sigma} > c \\right) \\leq \\frac{1}{c^2}.\n\\] This says that the probability of being, say, 2 standard deviations away from the mean must be less than 1/4 = 0.25. Notice that this bound can be quite wide. If \\(X\\) is normally distributed, then we know that just about 5% of draws will be greater than 2 SDs away from the mean, which is much lower than the 25% bound implied by Chebyshev’s inequality."
  },
  {
    "objectID": "03_asymptotics.html#the-law-of-large-numbers",
    "href": "03_asymptotics.html#the-law-of-large-numbers",
    "title": "3  Asymptotics",
    "section": "3.5 The law of large numbers",
    "text": "3.5 The law of large numbers\nWe can now use these inequalities to show how certain estimators are consistent for certain quantities of interest. Why are these inequalities useful for this purpose? Remember that convergence in probability was about the probability of an estimator being far away from a value going to zero. Chebyshev’s inequality shows that we can bound these exact probabilities.\nThe most famous consistency result has a special name.\n\nTheorem 3.3 (Weak Law of Large Numbers) Let \\(X_1, \\ldots, X_n\\) be a an i.i.d. draws from a distribution with mean \\(\\mu = \\mathbb{E}[X_i]\\) and variance \\(\\sigma^2 = \\mathbb{V}[X_i] < \\infty\\). Let \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i =1}^n X_i\\). Then, \\(\\overline{X}_n \\overset{p}{\\to}\\mu\\).\n\n\nProof. Recall that the sample mean is unbiased, so \\(\\mathbb{E}[\\overline{X}_n] = \\mu\\) with sampling variance \\(\\sigma^2/n\\). We can then simply apply Chebyshev to the sample mean to get \\[\n\\mathbb{P}(|\\overline{X}_n - \\mu| \\geq \\delta) \\leq \\frac{\\sigma^2}{n\\delta^2}\n\\] An \\(n\\rightarrow\\infty\\), the right-hand side goes to 0 which means that the left-hand side also must go to 0 which is the definition of \\(\\overline{X}_n\\) converging in probability to \\(\\mu\\).\n\nThe weak law of large numbers (WLLN) shows that, under general conditions, the sample mean gets closer to the population mean as \\(n\\rightarrow\\infty\\). In fact, this result holds even when the variance of the is infinite, though that’s a situation that most analysts will rarely face.\n\n\n\n\n\n\nNote\n\n\n\nThe naming of the “weak” law of large numbers seems to imply the existence of a “strong” law of large numbers (SLLN) and this is true. The SLLN states that the sample mean converges to the population mean with probability 1. This type of convergence, called almost sure convergence, is stronger than convergence in probability which only says that the probability of the sample mean being close to the population mean converges to 1. While it is nice to know that this stronger form of convergence holds for the sample mean under the same assumptions, it is very rare for folks outside of theoretical probability and statistics to need to rely on almost sure convergence.\n\n\n\nExample 3.1 It can be helpful to see how the distribution of the sample mean changes as a function of the sample size to appreciate the WLLN. We can show this by taking repeated iid samples of different sizes from an exponential rv with rate 0.5 so that \\(\\mathbb{E}[X_i] = 2\\). In Figure 3.1, we show the distribution of the sample mean (across repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue), and 1000 (green). What we can see is how the distribution of the sample mean is “collapsing” on the true population mean, 2. The probability of being far away from 2 becomes progressively smaller.\n\n\n\n\n\nFigure 3.1: Sampling distribution of the sample mean as a function of sample size\n\n\n\n\n\nThe WLLN also holds for random vectors in addition to random variables. Let \\((\\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_n)\\) be an iid sample of random vectors of length \\(k\\), \\(\\boldsymbol{X}_i = (X_{i1}, \\ldots, X_{ik})\\). We can define the vector sample mean as just the vector of sample means for each of the entries:\n\\[\n\\overline{\\boldsymbol{X}}_n = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol{X}_i =\n\\begin{pmatrix}\n\\overline{X}_{n,1} \\\\ \\overline{X}_{n,2} \\\\ \\vdots \\\\ \\overline{X}_{n, k}\n\\end{pmatrix}\n\\] Since this is just a vector of sample means, each random variable in the random vector will converge in probability to the mean of that random variable. Fortunately, this is the exact definition of convergence in probability for random vectors. We formally write this in the following theorem.\n\nTheorem 3.4 If \\(\\boldsymbol{X}_i \\in \\mathbb{R}^k\\) are iid draws from a distribution with \\(\\mathbb{E}[X_{ij}] < \\infty\\) for all \\(j=1,\\ldots,k\\) then as \\(n\\rightarrow\\infty\\)\n\\[\n\\overline{\\boldsymbol{X}}_n \\overset{p}{\\to}\\mathbb{E}[\\boldsymbol{X}]  =\n\\begin{pmatrix}\n\\mathbb{E}[X_{i1}] \\\\ \\mathbb{E}[X_{i2}] \\\\ \\vdots \\\\ \\mathbb{E}[X_{ik}]\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will have noticed that many of the formal results we have presented so far have “moment conditions” that certain moments are finite. For the vector WLLN, we saw that applied to the mean of each variable in the vector. Some books use a short hand for this: \\(\\mathbb{E}\\Vert \\boldsymbol{X}_i\\Vert < \\infty\\), where \\[\n\\Vert\\boldsymbol{X}_i\\Vert = \\left(X_{i1}^2 + X_{i2}^2 + \\ldots + X_{ik}^2\\right)^{1/2}.\n\\] This is slightly more compact notation, but why does it work? One can show that this function, called the Euclidean norm or \\(L_2\\)-norm is a convex function, so we can apply Jensen’s inequality to show that: \\[\n\\mathbb{E}\\Vert \\boldsymbol{X}_i\\Vert \\geq \\Vert \\mathbb{E}[\\boldsymbol{X}_i] \\Vert = (\\mathbb{E}[X_{i1}]^2 + \\ldots + \\mathbb{E}[X_{ik}]^2)^{1/2}.\n\\] So if \\(\\mathbb{E}\\Vert \\boldsymbol{X}_i\\Vert\\) is finite, it means that all the component means are finite otherwise the right-hand side of the previous equation would be infinite."
  },
  {
    "objectID": "03_asymptotics.html#consistency-of-estimators",
    "href": "03_asymptotics.html#consistency-of-estimators",
    "title": "3  Asymptotics",
    "section": "3.6 Consistency of estimators",
    "text": "3.6 Consistency of estimators\nThe WLLN shows that the sample mean of iid draws is consistent for the population mean, which is a massive result given that so many estimators can be written as sample means. What about other estimators? The proof of the WLLN points to one way to determine if an estimator is consistent: if it is unbiased and the sampling variance shrinks as the sample size grows. The next theorem\n\nTheorem 3.5 For any estimator \\(\\widehat{\\theta}_n\\), if \\(\\text{bias}[\\widehat{\\theta}_n] \\to 0\\) and \\(\\mathbb{V}[\\widehat{\\theta}_n] \\rightarrow 0\\) as \\(n\\rightarrow \\infty\\), then \\(\\widehat{\\theta}_n\\) is consistent.\n\nThus, if we can characterize the bias and sampling variance of an estimator, then we should be able to tell if it consistent or not. This is handy since working with the kinds of probability inequalities used for the WLLN can sometimes be quite confusing.\nWhat do we do if it is difficult or impossible to characterize the bias? Consider a plug-in estimator like \\(\\widehat{\\alpha} = \\log(\\overline{X}_n)\\) where \\(X_1, \\ldots, X_n\\) are iid from a population with mean \\(\\mu\\). We know that for nonlinear functions like logarithms we have \\(\\log\\left(\\mathbb{E}[Z]\\right) \\neq \\mathbb{E}[\\log(Z)]\\), so \\(\\mathbb{E}[\\widehat{\\alpha}] \\neq \\log(\\mathbb{E}[\\overline{X}_n])\\) and the plug-in estimator will be biased for \\(\\log(\\mu)\\). It will also be difficult to obtain an expression for the bias in terms of \\(n\\). Is all hope lost here? Must we give up on consistency? No, and in fact, consistency will be much simpler to show in this setting.\n\nTheorem 3.6 (Properties of convergence in probability) Let \\(X_n\\) and \\(Z_n\\) be two sequences of random variables such that \\(X_n \\overset{p}{\\to}a\\) and \\(Z_n \\overset{p}{\\to}b\\), and let \\(g(\\cdot)\\) be a continuous function. Then,\n\n\\(g(X_n) \\overset{p}{\\to}g(a)\\) (continuous mapping theorem)\n\\(X_n + Z_n \\overset{p}{\\to}a + b\\)\n\\(X_nZ_n \\overset{p}{\\to}ab\\)\n\\(X_n/Z_n \\overset{p}{\\to}a/b\\) if \\(b > 0\\).\n\n\nWe can now see that many of the nasty problems with expectations and nonlinear functions are made considerably easier with convergence in probability in the asymptotic setting. So while we know that \\(\\log(\\overline{X}_n)\\) is biased for \\(\\log(\\mu)\\), we know that it is consistent since \\(\\log(\\overline{X}_n) \\overset{p}{\\to}\\log(\\mu)\\) because \\(\\log\\) is a continuous function.\n\nExample 3.2 Suppose we implemented a survey by randomly selecting a sample from the population of size \\(n\\), but not everyone responded to our survey. Let the data consist of pairs of random variables, \\((Y_1, R_1), \\ldots, (Y_n, R_n)\\), where \\(Y_i\\) is the question of interest and \\(R_i\\) is a binary indicator for if the respondent answered the question (\\(R_i = 1\\)) or not (\\(R_i = 0\\)). Our goal is to estimate the mean of the question for responders: \\(\\mathbb{E}[Y_i \\mid R_i = 1]\\). We can use the law of iterated expectation to which we can rewrite as \\[\n\\begin{aligned}\n\\mathbb{E}[Y_iR_i] &= \\mathbb{E}[Y_i \\mid R_i = 1]\\mathbb{P}(R_i = 1) + \\mathbb{E}[ 0 \\mid R_i = 0]\\mathbb{P}(R_i = 0) \\\\\n\\implies \\mathbb{E}[Y_i \\mid R_i = 1] &= \\frac{\\mathbb{E}[Y_iR_i]}{\\mathbb{P}(R_i = 1)}\n\\end{aligned}\n\\]\nThe relevant estimator for this quantity is the mean of the of the outcome among those who responded, which is slightly more complicated than a typical sample mean because the denominator is a random variable: \\[\n\\widehat{\\theta}_n = \\frac{\\sum_{i=1}^n Y_iR_i}{\\sum_{i=1}^n R_i}.\n\\] Notice that this estimator is the ratio of two random variables. The numerator has mean \\(n\\mathbb{E}[Y_iR_i]\\) and the denominator has mean \\(n\\mathbb{P}(R_i = 1)\\). It is then tempting to say that we can take the ratio of these means as the mean of \\(\\widehat{\\theta}_n\\), but expectations are not preserved in nonlinear functions like this one.\nWe can establish consistency of our estimator, though, by noting that we can rewrite the estimator as a ratio of sample means \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i},\n\\] where by the WLLN the numerator \\((1/n)\\sum_{i=1}^n Y_iR_i \\overset{p}{\\to}\\mathbb{E}[Y_iR_i]\\) and the denominator \\((1/n)\\sum_{i=1}^n R_i \\overset{p}{\\to}\\mathbb{P}(R_i = 1)\\). Thus, by Theorem 3.6, we have \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i} \\overset{p}{\\to}\\frac{\\mathbb{E}[Y_iR_i]}{\\mathbb{P}[R_i = 1]} = \\mathbb{E}[Y_i \\mid R_i = 1]\n\\] so long as the probability of responding is greater than zero. This establishes that our sample mean among responders while biased for the conditional expectation among responders, it is consistent for that quantity.\n\nIt is very important to keep the difference between unbiased and consistent clear in your mind. There are very many silly unbiased estimators that are inconsistent. Let’s go back to our iid sample, \\(X_1, \\ldots, X_n\\) from a population with \\(E[X_i] = \\mu\\). There is nothing in the rule book against defining an estimator \\(\\widehat{\\theta}_{first} = X_1\\) that just uses the first observation as the estimate. This seems like an obviously silly estimator, but it is actually unbiased since \\(\\mathbb{E}[\\widehat{\\theta}_{first}] = \\mathbb{E}[X_1] = \\mu\\). It is inconsistent since the sampling variance of this estimator is just the variance of the population distribution, \\(\\mathbb{V}[\\widehat{\\theta}_{first}] = \\mathbb{V}[X_i] = \\sigma^2\\), which does not change as a function of the sample size. Generally speaking, we can regard “unbiased, but inconsistent” estimators as silly and not worth our time (along with bias and inconsistent estimators).\nThere are also estimators that are biased but consistent that are often much more interesting. We already saw one such estimator in Example 3.2, but there are many more. Maximum likelihood estimators, for example, are (under some regularity conditions) consistent for the parameters of a parametric model, but they are often biased.\n\nExample 3.3 (Plug-in variance estimator) Last chapter, we introduced the plug-in estimator for the population variance, \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2,\n\\] which we will now show is biased but consistent. To see the bias note that we can rewrite the sum of square deviations \\[\\sum_{i=1}^n (X_i - \\overline{X}_n)^2 = \\sum_{i=1}^n X_i^2 - n\\overline{X}_n. \\] Then, the expectation of the plug-in estimator is \\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\sigma}^2] & = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i^2\\right] - \\mathbb{E}[\\overline{X}_n^2] \\\\\n&= \\mathbb{E}[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E}[X_iX_j] \\\\\n&= \\mathbb{E}[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\mathbb{E}[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j\\neq i} \\underbrace{\\mathbb{E}[X_i]\\mathbb{E}[X_j]}_{\\text{independence}} \\\\\n&= \\mathbb{E}[X_i^2] - \\frac{1}{n}\\mathbb{E}[X_i^2] - \\frac{1}{n^2} n(n-1)\\mu^2 \\\\\n&= \\frac{n-1}{n} \\left(\\mathbb{E}[X_i^2] - \\mu^2\\right) \\\\\n&= \\frac{n-1}{n} \\sigma^2 = \\sigma^2 - \\frac{1}{n}\\sigma^2\n\\end{aligned}.\n\\] Thus, we can see that the bias of the plug-in estimator is \\(-(1/n)\\sigma^2\\) so it slightly underestimates the variance. Nicely, though, the bias shrinks as a function of the sample size, so according to Theorem 3.5 it will be consistent so long as the sampling variance of \\(\\widehat{\\sigma}^2\\) shrinks as a function of the sample size, which it does (though omit that proof here). Of course, simply multiplying this estimator by \\(n/(n-1)\\) will give an unbiased and consistent estimator that is also the typical sample variance estimator."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "href": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "title": "3  Asymptotics",
    "section": "3.7 Convergence in distribution and the central limit theorem",
    "text": "3.7 Convergence in distribution and the central limit theorem\nConvergence in probability and the law of large numbers are very useful for understanding how our estimators will (or will not) collapse to their estimand as the sample size increases. But what about the shape of the sampling distribution of our estimators? For the purposes of statistical inference, we would like to be able to make probability statements such as \\(\\mathbb{P}(a \\leq \\widehat{\\theta}_n \\leq b)\\). These types of statements will be the basis of hypothesis testing and confidence intervals. But in order make those types of statements, we need to know the entire distribution of \\(\\widehat{\\theta}_n\\), not just the mean and variance. Luckily, there are established results that will allow us to approximate the sampling distribution of a huge swath of estimators when our sample sizes are large.\nTo see how we will develop these approximations, we need to first describe a weaker form of convergence to a distribution rather than to a single value.\n\nDefinition 3.4 Let \\(X_1,X_2,\\ldots\\), be a sequence of r.v.s, and for \\(n = 1,2, \\ldots\\) let \\(F_n(x)\\) be the c.d.f. of \\(X_n\\). Then it is said that \\(X_1,X_2, \\ldots\\) converges in distribution to r.v. \\(X\\) with c.d.f. \\(F(x)\\) if \\[\n\\lim_{n\\rightarrow \\infty} F_n(x) = F(x),\n\\] for all values of \\(x\\) for which \\(F(x)\\) is continuous. We write this as \\(X_n \\overset{d}{\\to}X\\) or sometimes \\(X_n ⇝ X\\).\n\nEssentially, convergence in distribution means that as \\(n\\) gets large, the distribution of \\(X_n\\) becomes more and more similar to the distribution of \\(X\\), which we often call the asymptotic distribution of \\(X_n\\) (other names include the large-sample distribution). If we know that \\(X_n \\overset{d}{\\to}X\\), then we can use the distribution of \\(X\\) as an approximation to the distribution of \\(X_n\\) and that distribution can be fairly accurate.\nOne of the most remarkable results in probability and statistics is that a large class of estimators will converge in distribution to one particular family of distributions: the normal. This is one reason that we study the normal so much and why investing in building intuition about it will pay off across many domains of applied work. We call this broad class of results the “central limit theorem,” (CLT) but it would probably be more accurate to refer to them as “central limit theorems” since much of statistics is devoted to showing the result in different settings. We now present the simplest CLT for the sample mean.\n\nTheorem 3.7 (Central Limit Theorem) Let \\(X_1, \\ldots, X_n\\) be i.i.d. r.v.s from a distribution with mean \\(\\mu = \\mathbb{E}[X_i]\\) and variance \\(\\sigma^2 = \\mathbb{V}[X_i]\\). Then if \\(\\mathbb{E}[X_i^2] < \\infty\\), we have \\[\n\\frac{\\overline{X}_n - \\mu}{\\sqrt{\\mathbb{V}[\\overline{X}_n]}} = \\frac{\\sqrt{n}\\left(\\overline{X}_n - \\mu\\right)}{\\sigma} \\overset{d}{\\to}\\mathcal{N}(0, 1).\n\\]\n\nIn words: the sample mean of a random sample from a population with finite mean and variance will be approximately normally distributed in large samples. Notice how we have not made any assumptions about the distribution of the underlying random variables, \\(X_i\\). They could binary, event count, continuous, anything. This means the CLT is incredibly broadly applicable.\n\n\n\n\n\n\nNotation alert\n\n\n\nWhy do we state the CLT in terms of the sample mean after centering and scaling by its standard error? If we don’t normalize the sample mean in this way, it’s difficult to talk about convergence in distribution because we know from the WLLN that \\(\\overline{X}_n \\overset{p}{\\to}\\mu\\) so in the limit the distribution of \\(\\overline{X}_n\\) is concentrated at point mass around that value. Normalizing by centering and rescaling ensures that the variance of the resulting quantity will be fixed as a function of \\(n\\), so it makes sense to talk about its distribution converging. Sometimes you will see the equivalent result as \\[\n\\sqrt{n}\\left(\\overline{X}_n - \\mu\\right) \\overset{d}{\\to}\\mathcal{N}(0, \\sigma^2).\n\\]\n\n\nWe can use this result to state approximations that we can use when discussing estimators such as \\[\n\\overline{X}_n \\overset{a}{\\sim} N(\\mu, \\sigma^2/n),\n\\] where we use \\(\\overset{a}{\\sim}\\) to be “approximately distributed as in large samples.” This allow us to say things like: “in large samples, we should expect the sample mean to between within \\(2\\sigma/\\sqrt{n}\\) of the true mean in 95% of repeated samples.” As you might guess, this will be very important for hypothesis tests and confidence intervals! Estimators so often follow the CLT that we have an expression for this property.\n\nDefinition 3.5 An estimator \\(\\widehat{\\theta}_n\\) is asymptotically normal if for some \\(\\theta\\) \\[\n\\sqrt{n}\\left( \\widehat{\\theta}_n - \\theta \\right) \\overset{d}{\\to}N\\left(0,\\mathbb{V}[\\widehat{\\theta}_n]\\right).\n\\]\n\n\nExample 3.4 To illustrate how the CLT works, we can simulate the sampling distribution of the (normalized) sample mean at different sample sizes. Let \\(X_1, \\ldots, X_n\\) be iid samples from a Bernoulli with probability of success 0.25. We then draw repeated samples of size \\(n=30\\) and \\(n=100\\) and calculate \\(\\sqrt{n}(\\overline{X}_n - 0.25)/\\sigma\\) for each random sample. Figure 3.2 plots the density of these two sampling distributions along with a standard normal reference. We can see that even at \\(n=30\\), the rough shape of the density looks normal, with spikes and valleys due to the discrete nature of the data (the sample mean can only take on 31 possible values in this case). By \\(n=100\\), the sampling distribution is very close to the true standard normal.\n\n\n\n\n\nFigure 3.2: Sampling distributions of the normalized sample mean at n=30 and n=100.\n\n\n\n\n\nThere are several properties of convergence in distribution that are helpful to us.\n\nTheorem 3.8 (Properties of convergence in distribution) Let \\(X_n\\) be a sequence of random variables \\(X_1,X_2,\\ldots\\) that converges in distribution to some rv \\(X\\) and let \\(Y_n\\) be a sequence of random variables \\(Y_1,Y_2,\\ldots\\) that converges in probability to some number, \\(c\\). Then,\n\n\\(g(X_n) \\overset{d}{\\to}g(X)\\) for all continuous functions \\(g\\).\n\\(X_nY_n\\) converges in distribution to \\(cX\\)\n\\(X_n + Y_n\\) converges in distribution to \\(X + c\\)\n\\(X_n / Y_n\\) converges in distribution to \\(X / c\\) if \\(c \\neq 0\\)\n\n\nThe last 3 of these results are sometimes referred to as Slutsky’s theorem. These results are very commonly used when trying to determine the asymptotic distribution of an estimator.\nLike with the WLLN, the CLT holds for random vectors of sample means, where their centered and scaled versions converge to a multivariate normal distribution with a covariance matrix equal to covariance matrix of the underlying random vectors of data, \\(\\boldsymbol{X}_i\\).\n\nTheorem 3.9 If \\(\\boldsymbol{X}_i \\in \\mathbb{R}^k\\) are i.i.d. and \\(\\mathbb{E}\\Vert \\boldsymbol{X}_i \\Vert^2 < \\infty\\), then as \\(n \\to \\infty\\), \\[\n\\sqrt{n}\\left( \\overline{\\boldsymbol{X}}_n - \\boldsymbol{\\mu}\\right) \\overset{d}{\\to}\\mathcal{N}(0, \\boldsymbol{\\Sigma}),\n\\] where \\(\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{X}_i]\\) and \\(\\boldsymbol{\\Sigma} = \\mathbb{V}[\\boldsymbol{X}_i] = \\mathbb{E}\\left[(\\boldsymbol{X}_i-\\boldsymbol{\\mu})(\\boldsymbol{X}_i - \\boldsymbol{\\mu})'\\right]\\).\n\nHere, notice that \\(\\boldsymbol{\\mu}\\) is the vector of population means for all the random variables in \\(\\boldsymbol{X}_i\\) and \\(\\boldsymbol{\\Sigma}\\) is the variance-covariance matrix for that vector.\n\n\n\n\n\n\nNote\n\n\n\nAs with the notation alert with the WLLN, we are using a shorthand here, \\(\\mathbb{E}\\Vert \\boldsymbol{X}_i \\Vert^2 < \\infty\\), which implies that \\(\\mathbb{E}[X_{ij}^2] < \\infty\\) for all \\(j = 1,\\ldots, k\\), or equivalently, that the variances of each variable in the sample means has finite variance."
  },
  {
    "objectID": "03_asymptotics.html#delta-method",
    "href": "03_asymptotics.html#delta-method",
    "title": "3  Asymptotics",
    "section": "3.8 Delta method",
    "text": "3.8 Delta method\nSuppose that we know that an estimator follows the CLT and so we have \\[\n\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta  \\right) \\overset{d}{\\to}\\mathcal{N}(0, V),\n\\] but we actually want to estimate \\(h(\\theta)\\) so we use the plug-in estimator, \\(h(\\widehat{\\theta}_n)\\). It seems like we should be able to apply part 1 of Theorem 3.8, the CLT established the large-sample distribution of the centered and scaled random sequence, \\(\\sqrt{n}(\\widehat{\\theta}_n - \\theta)\\), not to the original estimator itself like we would need to investigate the asymptotic distribution of \\(h(\\widehat{\\theta}_n)\\). We can use a little bit of calculus to get an approximation to the distribution we need.\n\nTheorem 3.10 If \\(\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta\\right) \\overset{d}{\\to}\\mathcal{N}(0, V)\\) and \\(h(u)\\) is continuously differentiable in a neighborhood around \\(\\theta\\), then as \\(n\\to\\infty\\), \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta}_n) - h(\\theta)  \\right) \\overset{d}{\\to}\\mathcal{N}(0, (h'(\\theta))^2 V).\n\\]\n\nIt’s useful to understand what’s happening here since it might help give intuition as to when this might go wrong. Why do we focus on continuously differentiable functions, \\(h()\\)? These are functions that can be well-approximated with a line in a neighborhood around a given point like \\(\\theta\\). In Figure 3.3, we show this where the tangent line at \\(\\theta_0\\), which has slope \\(h'(\\theta_0)\\), is very similar to \\(h(\\theta)\\) for values close to \\(\\theta_0\\). Because of this, we can approximate the difference between \\(h(\\widehat{\\theta}_n)\\) and \\(h(\\theta_0)\\) with the what this tangent line would give us: \\[\n\\underbrace{\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right)}_{\\text{change in } y} \\approx \\underbrace{h'(\\theta_0)}_{\\text{slope}} \\underbrace{\\left(\\widehat{\\theta}_n - \\theta_0\\right)}_{\\text{change in } x},\n\\] and then multiplying both sides by the \\(\\sqrt{n}\\) gives \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right) \\approx h'(\\theta_0)\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta_0\\right).\n\\] The right-hand side of this approximation converges to \\(h'(\\theta_0)Z\\), where \\(Z\\) is a random variable variable with \\(\\mathcal{N}(0, V)\\). The variance of this quantity will be \\[\n\\mathbb{V}[h'(\\theta_0)Z] = (h'(\\theta_0))^2\\mathbb{V}[Z] = (h'(\\theta_0))^2V,\n\\] by the properties of variances.\n\n\n\n\n\nFigure 3.3: Linear approximation to nonlinear functions\n\n\n\n\n\nExample 3.5 Let’s return to the iid sample \\(X_1, \\ldots, X_n\\) with mean \\(\\mu = \\mathbb{E}[X_i]\\) and variance \\(\\sigma^2 = \\mathbb{V}[X_i]\\). From the CLT, we know that \\(\\sqrt{n}(\\overline{X}_n - \\mu) \\overset{d}{\\to}\\mathcal{N}(0, \\sigma^2)\\). Suppose that we want to estimate \\(\\log(\\mu)\\) so we use the plug-in estimator \\(\\log(\\overline{X}_n)\\) (assuming that \\(X_i > 0\\) for all \\(i\\) so that we can actually take the log). What is the asymptotic distribution of this estimator? This is a situation where \\(\\widehat{\\theta}_n = \\overline{X}_n\\) and \\(h(\\mu) = \\log(\\mu)\\). From basic calculus we know that \\[\nh'(\\mu) = \\frac{\\partial \\log(\\mu)}{\\partial \\mu} = \\frac{1}{\\mu},\n\\] so applying the delta method, we can determine that \\[\n\\sqrt{n}\\left(\\log(\\overline{X}_n) - \\log(\\mu)\\right) \\overset{d}{\\to}\\mathcal{N}\\left(0,\\frac{\\sigma^2}{\\mu^2} \\right).\n\\]\n\n\nExample 3.6 What about if we want to estimate the \\(\\exp(\\mu)\\) with \\(\\exp(\\overline{X}_n)\\)? Recall that \\[\nh'(\\mu) = \\frac{\\partial \\exp(\\mu)}{\\partial \\mu} = \\exp(\\mu)\n\\] so applying the delta method, we have \\[\n\\sqrt{n}\\left(\\exp(\\overline{X}_n) - \\exp(\\mu)\\right) \\overset{d}{\\to}\\mathcal{N}(0, \\exp(2mu)\\sigma^2),\n\\] since \\(\\exp(\\mu)^2 = \\exp(2\\mu)\\).\n\nLike all of the results in this chapter, there is a multivariate version of the delta method that is incredibly useful in practical applications. This is because we often will take two different estimators (or two different estimated parameters) and combine them to estimate another quantity. We now let \\(\\boldsymbol{h}(\\boldsymbol{\\theta}) = (h_1(\\boldsymbol{\\theta}), \\ldots, h_m(\\boldsymbol{\\theta}))\\) map from \\(\\mathbb{R}^k \\to \\mathbb{R}^m\\) and be continuously differentiable (we make the function bold since it ). It will help us use more compact matrix notation if we introduce a \\(m \\times k\\) Jacobian matrix of all partial derivatives \\[\n\\boldsymbol{H}(\\boldsymbol{\\theta}) = \\boldsymbol{\\nabla}_{\\boldsymbol{\\theta}}\\boldsymbol{h}(\\boldsymbol{\\theta}) = \\begin{pmatrix}\n  \\frac{\\partial h_1(\\boldsymbol{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_1(\\boldsymbol{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_1(\\boldsymbol{\\theta})}{\\partial \\theta_k} \\\\\n  \\frac{\\partial h_2(\\boldsymbol{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_2(\\boldsymbol{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_2(\\boldsymbol{\\theta})}{\\partial \\theta_k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\frac{\\partial h_m(\\boldsymbol{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_m(\\boldsymbol{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_m(\\boldsymbol{\\theta})}{\\partial \\theta_k}\n\\end{pmatrix},\n\\] which we can use to generate the equivalent multivariate linear approximation \\[\n\\left(\\boldsymbol{h}(\\widehat{\\boldsymbol{\\theta}}_n) - \\boldsymbol{h}(\\boldsymbol{\\theta}_0)\\right) \\approx \\boldsymbol{H}(\\boldsymbol{\\theta}_0)'\\left(\\widehat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0\\right).\n\\] We can use this fact to derive the multivariate delta method.\n\nTheorem 3.11 Suppose that \\(\\sqrt{n}\\left(\\widehat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta}_0 \\right) \\overset{d}{\\to}\\mathcal{N}(0, \\boldsymbol{\\Sigma})\\), then for any function \\(\\boldsymbol{h}\\) that is continuously differentiable in a neighborhood of \\(\\boldsymbol{\\theta}_0\\), we have \\[\n\\sqrt{n}\\left(\\boldsymbol{h}(\\widehat{\\boldsymbol{\\theta}}_n) - \\boldsymbol{h}(\\boldsymbol{\\theta}_0) \\right) \\overset{d}{\\to}\\mathcal{N}(0, \\boldsymbol{H}\\boldsymbol{\\Sigma}\\boldsymbol{H}'),\n\\] where \\(\\boldsymbol{H} = \\boldsymbol{H}(\\boldsymbol{\\theta}_0)\\).\n\nThis result follows from the approximation above plus rules about variances of random vectors. Remember that for any compatible matrix of constants, \\(\\boldsymbol{A}\\), we have \\(\\mathbb{V}[\\boldsymbol{A}'\\boldsymbol{Z}] = \\boldsymbol{A}\\mathbb{V}[\\boldsymbol{Z}]\\boldsymbol{A}'\\). You can see that the matrix of constants appears twice here, sort of like the matrix version of “squaring the constant” rule for variance.\nThe delta method is a very useful for generating closed-form approximations for asymptotic standard errors, but the math is often quite complex for even simple estimators. For applied researchers, it is usually more straightforward to use computational tools like the bootstrap to approximate the standard errors we need. This has the trade-off of taking more computational time to implement than the delta method, but is more easily adaptable across different estimators and domains with little human thinking time."
  }
]