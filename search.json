[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A User’s Guide to Statistical Inference and Regression",
    "section": "",
    "text": "$$\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\bfbeta}{\\bs{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\bs{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\n\nPreface\nThis is a set of notes for Government 2002: Quantitative Social Science Methods II at Harvard University taught by Matthew Blackwell. The goal of this text is to provide a rigorous yet accessible introduction to the foundational topics in statistical inference with a special application to linear regression, a workhorse tool in the social sciences. The material is intended for first-year PhD students in political science, but it may be of interest more broadly. Much of the material has been adopted from various sources (far too many to recount now), but this book is especially indebted to the following texts:\n\nHansen, Bruce. Probability & Statistics for Economists. Princeton University Press.\nHansen, Bruce. Econometrics. Princeton University Press.\nWasserman, Larry. All of Statistics: A Concise Course in Statistical Inference. Springer.\nWooldridge, Jeffrey. Econometric Analysis of Cross Section and Panel Data\n\nYou can find the source for this book at https://github.com/mattblackwell/gov2002-book. Any typos or errors can be reported at https://github.com/mattblackwell/gov2002-book/issues. Thanks for reading.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\n\\(\\,\\)"
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "$$\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\bfbeta}{\\bs{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\bs{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\\(\\,\\)\nThis book, like so many books before it, will try to teach you statistics. The field of statistics describes how we learn about the world from quantitative data. In the social sciences, the vast majority of empirical studies use statistical methods to provide evidence for their arguments. While it is possible to conduct quantitative research without understanding statistics, one must advise against it. Quantitative research involves a host of choices about what model to use, what variables to include, what tuning parameters to set, what assumptions to make, and so on. Without a deep understanding of statistics, you will find these choices bewildering and often yield to the default settings of your statistical software. The goal of this book is to give you the foundation to confidently make those choices for your specific application.\nWe will focus on two key goals in this book.\n\nUnderstand the basic ways to assess estimators With quantitative data, we often want to make statistical inferences about some unknown feature of the world. We use estimators (which are just ways of summarizing our data) to estimate these features. One major goal of this book is to show the basics of this task at a general enough level to be applicable to almost any estimator that you are likely to encounter in research. The ideas of bias, sampling variance, consistency, and asymptotic normality are common to such a large swath of (frequentist) inference that you get a tremendous return on your investment of time in these topics. Understand these core ideas and you will have a language to analyze any fancy new estimator that pops up in the next few decades.\nApply these ideas to estimation of regressions This book will apply these ideas to one particular workhorse task in the social sciences: estimating regression functions. So many methods are either use regression estimators like ordinary least squares or extend it in some way. Understanding how these estimators work is vital for conducting research in the social sciences. Regression and regression estimators also provide an entry point for discussing parametric models explicitly as approximation and projections rather than as rigid assumptions about the truth of a given specification.\n\nWhy write a book on statistics and regression when so many already exist? Aside from hubris, my goal in this book is to find a level of mathematical sophistication that will challenge and push political scientists to develop stronger foundations in the material. While some textbooks at this level exist in statistics and economics, they tend to focus on applications less relevant to political science. This book attempts to correct this."
  },
  {
    "objectID": "02_estimation.html#introduction",
    "href": "02_estimation.html#introduction",
    "title": "2  Estimation",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWhen studying probability, we assumed that we knew the parameter of a distribution (the mean or the variance) and used probability theory to understand what kind of data we would observe. Estimation and inference put this engine in reverse and try to learn some aspect of the data-generating process using only our observed data. There are two main goals here: estimation, which is how we formulate our best guess about a parameter of the DGP, and inference, which is how we formalize and express uncertainty about our estimates.\n\n\nExample 2.1 (Randomized control trial) Suppose we are conducting a randomized experiment on framing effects. All respondents receive some factual information about current levels of immigration. The message for the treatment group (\\(D_i = 1\\)) has an additional framing of the positive benefits of immigration, while the control group (\\(D_i = 0\\)) receives no additional framing. The outcome is a binary outcome on whether the respondent supports increasing legal immigration limits (\\(Y_i = 1\\)) or not (\\(Y_i = 0\\)). The observed data consists of \\(n\\) pairs of random variables, the outcome, and the treatment assignment: \\(\\{(Y_1, D_1), \\ldots, (Y_n, D_n)\\}\\). Define the two sample means/proportions in each group as \\[\n\\Ybar_1 = \\frac{1}{n_1} \\sum_{i: D_i = 1} Y_i,  \\qquad\\qquad \\Ybar_0 = \\frac{1}{n_0} \\sum_{i: D_i = 0} Y_i,\n\\] where \\(n_1 = \\sum_{i=1}^n D_i\\) is the number of treated units and \\(n_0 = n - n_1\\) is the number of control units.\nA standard estimator for the treatment effect in a study like this would be the difference in means, \\(\\Ybar_1 - \\Ybar_0\\). But this is only one possible estimator. We could also estimate the effect by taking this difference in means separately by party identification and then averaging those party-specific effects by the size of those groups. This estimator is commonly called a poststratification estimator, but it’s unclear at first glance which of these two estimators we should prefer.\n\nWhat are the goals of studying estimators? In short, we prefer to use good estimators rather than bad estimators. But what makes an estimator good or bad? You probably have some intuitive sense that, for example, an estimator that always returns the value 3 is bad. Still, it will be helpful for us to formally define and explore some properties of estimators that will allow us to compare them and choose the good over the bad."
  },
  {
    "objectID": "02_estimation.html#samples-and-populations",
    "href": "02_estimation.html#samples-and-populations",
    "title": "2  Estimation",
    "section": "2.2 Samples and populations",
    "text": "2.2 Samples and populations\nFor most of this class, we’ll focus on a relatively simple setting where we have a set of random vectors \\(\\{X_1, \\ldots, X_n\\}\\) that are independent and identically distributed (iid) draws from a distribution with cumulative distribution function (cdf) \\(F\\). They are independent in that the random vectors \\(X_i\\) and \\(X_j\\) are independent for all \\(i \\neq j\\), and they are “identically distributed” in the sense that each of the random variables \\(X_i\\) have the same marginal distribution, \\(F\\).\nYou can think of each vector, \\(X_i\\), as the rows in your data frame. Note that we’re being purposely vague about this cdf—it simply represents the unknown distribution of the data, otherwise known as the data generating process (DGP). Sometimes \\(F\\) is also referred to as the population distribution or even just population, which has its roots in viewing the data as a random sample from some larger population.1 As a shorthand, we often say that the collection of random vectors \\(\\{X_1, \\ldots, X_n\\}\\) is a random sample from population \\(F\\) if \\(\\{X_1, \\ldots, X_n\\}\\) is iid with distribution \\(F\\). The sample size \\(n\\) is the number of units in the sample.\nTwo metaphors can help build intuition about the concept of viewing the data as an iid draw from \\(F\\):\n\nRandom sampling. Suppose we have a population of size \\(N\\) that is much larger than our sample size \\(n\\), and we take a random sample of size \\(n\\) from this population with replacement. Then the distribution of the data in the random sample will be iid draws from the population distribution of the variables we are sampling. For instance, suppose we take a random sample from a population of US citizens where the population proportion of Democratic party identifiers is 0.33. Then if we randomly sample \\(n = 100\\) US citizens, each data point \\(X_i\\) will be distributed Bernoulli with probability of success 0.33.\nGroundhog Day. Random sampling does not always make sense as a justification for iid data, especially when the units are not samples at all but rather countries, states, or subnational units. In this case, we have to appeal to a thought experiment where \\(F\\) represents the fundamental uncertainty in the data-generating process. The metaphor here is that if we could re-run history many times, like the 1993 American classic comedy Groundhog Day, data and outcomes would change slightly due to the inherently stochastic nature of the world. The iid assumption, then, is that each of the units in our data has the same DGP producing this data or the same distribution of outcomes under the Groundhog Day scenario. The set of all these infinite possible draws from the DGP is sometimes referred to as the superpopulation.\n\nNote that there are many situations where the iid assumption is not appropriate. We will cover some of those later in the semester. But much of the innovation and growth in statistics over the last 50 years has been figuring out how to perform statistical inference when iid does not hold. Often, the solutions are specific to the type of iid violation you have (spatial, time-series, network, or clustered). As a rule of thumb, though, if you suspect iid is incorrect, your uncertainty statements will likely be overconfident (for example, confidence intervals, which we’ll cover later, are too small)."
  },
  {
    "objectID": "02_estimation.html#point-estimation",
    "href": "02_estimation.html#point-estimation",
    "title": "2  Estimation",
    "section": "2.3 Point estimation",
    "text": "2.3 Point estimation\n\n2.3.1 Quantities of interest\nWe aim to learn about the data-generating process, represented by the cdf, \\(F\\). We might be interested in estimating the cdf at a general level or only some feature of the distribution, like a mean or conditional expectation function. We will almost always have a particular quantity in mind, but we’ll introduce estimation at a general level. So we’ll let \\(\\theta\\) represent the quantity of interest. Point estimation describes how we obtain a single “best guess” about \\(\\theta\\).\n\n\n\n\n\n\nNote\n\n\n\nSome refer to quantities of interest as parameters or estimands (that is, the target of estimation).\n\n\n\nExample 2.2 (Population mean) Suppose we wanted to know the proportion of US citizens who support increasing legal immigration in the US, which we denote as \\(Y_i = 1\\). Then our quantity of interest is the mean of this random variable, \\(\\mu = \\E[Y_i]\\), which is the probability of randomly drawing someone from the population supporting increased legal immigration.\n\n\nExample 2.3 (Population variance) Feeling thermometer scores are a prevalent way to assess how a survey respondent feels about a particular person or group. A survey asks respondents how warmly they feel about a group from 0 to 100, which we will denote \\(Y_i\\). We might be interested in how polarized views are on a group in the population, and one measure of polarization could be the variance, or spread, of the distribution of \\(Y_i\\) around the mean. In this case, \\(\\sigma^2 = \\V[Y_i]\\) would be our quantity of interest.\n\n\nExample 2.4 (RCT continued) In Example 2.1, we discussed a typical estimator for an experimental study with a binary treatment. The goal of that experiment is to learn about the difference between two conditional probabilities (or expectations): the average support for increasing legal immigration in the treatment group, \\(\\mu_1 = \\E[Y_i \\mid D_i = 1]\\), and the same average in the control group, \\(\\mu_0 = \\E[Y_i \\mid D_i = 0]\\). This difference, \\(\\mu_1 - \\mu_0\\), is a function of unknown features of these two conditional distributions.\n\nEach of these is a function of the (possibly joint) distribution of the data, \\(F\\). In each of these, we are not necessarily interested in the entire distribution, just summaries of it (central tendency, spread). Of course, there are situations where we are also interested in the complete distribution.\n\n\n2.3.2 Estimators\nWhen our sample size is more than a few observations, it makes no sense to work with the raw data, \\(X_1, \\ldots, X_n\\), and we inevitably will need to summarize the data in some way. We can represent this summary as a function, \\(g(x_1, \\ldots, x_n)\\), which might be the formula for the sample mean or sample variance. This function is just a regular function that takes in \\(n\\) numbers (or vectors) and returns a number (or vector). We can also define a random variable based on this function, \\(Y = g(X_1, \\ldots, X_n)\\), which inherits its randomness from the randomness of the data. Before we see the data, we don’t know what values of \\(X_1, \\ldots, X_n\\) we will see, so we don’t know what value of \\(Y\\) we’ll see either. We call the random variable \\(Y = g(X_1, \\ldots, X_n)\\) a statistic (or sometimes sample statistics), and we refer to the probability distribution of a statistic \\(Y\\) as the sampling distribution of \\(Y\\).\n\n\n\n\n\n\nWarning\n\n\n\nThere is one potential confusion in how we talk about “statistics.” Just above, we defined a statistic as a random variable based on it being a function of random variables (the data). But we sometimes refer to the calculated value as a statistic as well, which is a specific number that you see in your R output. To be precise, we should call the latter the realized value of the statistic, but message discipline is difficult to enforce in this context. A simple example might help. Suppose that \\(X_1\\) and \\(X_2\\) are the results of a roll of two standard six-sided dice. Then the statistic \\(Y = X_1 + X_2\\) is a random variable that has a distribution over the numbers from {2, , 12} that describes our uncertainty over what the sum will be before we roll the dice. Once we have rolled the dice and observed the realized values \\(X_1 = 3\\) and \\(X_2 = 4\\), we observed the realized value of the statistic, \\(Y = 7\\).\n\n\nAt their most basic, statistics are just data summaries without aim or ambition. Estimators are statistics with a purpose: to provide an “educated guess” about some quantity of interest.\n\nDefinition 2.1 An estimator \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\) for some parameter \\(\\theta\\), is a statistic intended as a guess about \\(\\theta\\).\n\nOne important distinction of jargon is between an estimator and an estimate, similar to the issues with “statistic” described above. The estimator is a function of the data, whereas the estimate is the realized value of the estimator once we see the data. An estimate is a single number, such as 0.38, whereas the estimator is a random variable that has uncertainty over what value it will take. Formally, the estimate is \\(\\theta(x_1, \\ldots, x_n)\\) when the data is \\(\\{X_1, \\ldots, X_n\\} = \\{x_1, \\ldots, x_n\\}\\), whereas we represent the estimator as a function of random variables, \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\n\n\n\n\n\nNote\n\n\n\nIt is widespread, though not universal, to use the “hat” notation to define an estimator and its estimand. For example, \\(\\widehat{\\theta}\\) (or “theta hat”) indicates that this estimator is targeting the parameter \\(\\theta\\).\n\n\n\nExample 2.5 (Estimators for the population mean) Suppose we would like to estimate the population mean of \\(F\\), which we will represent as \\(\\mu = \\E[X_i]\\). We could choose from several estimators, all with different properties. \\[\n\\widehat{\\theta}_{n,1} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\quad \\widehat{\\theta}_{n,2} = X_1, \\quad \\widehat{\\theta}_{n,3} = \\text{max}(X_1,\\ldots,X_n), \\quad \\widehat{\\theta}_{n,4} = 3\n\\] The first is just the sample mean, which is an intuitive and natural estimator for the population mean. The second just uses the first observation. While this seems silly, this is a valid statistic (it’s a function of the data!). The third takes the maximum value in the sample, and the fourth always returns three, regardless of the data."
  },
  {
    "objectID": "02_estimation.html#how-to-find-estimators",
    "href": "02_estimation.html#how-to-find-estimators",
    "title": "2  Estimation",
    "section": "2.4 How to find estimators",
    "text": "2.4 How to find estimators\nWhere do estimators come from? There are a couple of different methods that I’ll cover briefly here before describing the ones that will form the bulk of this class.\n\n2.4.1 Parametric models and maximum likelihood\nThe first method for generating estimators relies on parametric models, where the researcher specifies the exact distribution (up to some unknown parameters) of the DGP. Let \\(\\theta\\) be the parameters of this distribution and we then write \\(\\{X_1, \\ldots, X_n\\}\\) are iid draws from \\(F_{\\theta}\\). We should also formally state the set of possible values the parameters can take, which we call the parameter space and usually denote as \\(\\Theta\\). Because we’re assuming we know the distribution of the data, we can write the p.d.f. as \\(f(X_i \\mid \\theta)\\) and define the likelihood function as the product of these p.d.f.s over the units as a function of the parameters: \\[\nL(\\theta) = \\prod_{i=1}^n f(X_i \\mid \\theta).\n\\] We can then define the maximum likelihood estimator (MLE) for \\(\\theta\\) as the values of the parameter that, well, maximize the likelihood: \\[\n\\widehat{\\theta}_{mle} = \\argmax_{\\theta \\in \\Theta} \\; L(\\theta)\n\\] Sometimes we can use calculus to derive a closed-form expression for the MLE. Still, we often use iterative techniques that search the parameter space for the maximum.\nMaximum likelihood estimators have very nice properties, especially in large samples. Unfortunately, it also requires the correct knowledge of the parametric model, which is often difficult to justify. Do we really know if we should model a given event count variable as Poisson or Negative Binomial? The attractive properties of MLE are only as good as our ability to specify the parametric model.\n\n\n\n\n\n\nNo free lunch\n\n\n\nOne essential intuition to build about statistics is the assumptions-precision tradeoff. You can usually get more precise estimates if you make stronger and potentially more fragile assumptions. Conversely, you will almost always get less accurate estimates if you weaken your assumptions.\n\n\n\n\n2.4.2 Plug-in estimators\nThe second broad class of estimators is semiparametric in that we will specify some finite-dimensional parameters of the DGP but leave the rest of the distribution unspecified. For example, we might define a population mean, \\(\\mu = \\E[X_i]\\), and a population variance, \\(\\sigma^2 = \\V[X_i]\\) but leave unrestricted the shape of the distribution. This approach ensures that our estimators will be less dependent on correctly specifying distributions we have little intuition about.\nThe primary method for constructing estimators in this setting is to use the plug-in estimator, or the estimator that replaces any population mean with a sample mean. Obviously, in the case of estimating the population mean, \\(\\mu\\), this means we will use the sample mean as its estimate: \\[\n\\Xbar_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\quad \\text{estimates} \\quad \\E[X_i] = \\int_{\\mathcal{X}} x f(x)dx\n\\] What are we doing here? We are replacing the unknown population distribution \\(f(x)\\) in the population mean with a discrete uniform distribution over our data points, with \\(1/n\\) probability assigned to each unit. Why do this? It encodes that if we have a random sample, our best guess about the population distribution of \\(X_i\\) is the sample distribution in our actual data. If this intuition fails, you can hold onto an analog principle: sample means of random variables are natural estimators of population means.\nWhat about estimating something more complicated, like the expected value of a function of the data, \\(\\theta = \\E[r(X_i)]\\)? The key is to see that \\(f(X_i)\\) is also a random variable. Let’s call this random variable \\(Y_i = f(X_i)\\). Now we can see that \\(\\theta\\) is just the population expectation of this random variable, and using the plug-in estimator, we get: \\[\n\\widehat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\nWith these facts in hand, we can describe the more general plug-in estimator. When we want to estimate some quantity of interest that is a function of population means, we can generate a plug-in estimator by replacing any population mean with a sample mean. Formally, let \\(\\alpha = g\\left(\\E[r(X_i)]\\right)\\) be a parameter that is defined as a function of the population mean of a (possibly vector-valued) function of the data. Then, we can estimate this parameter by plugging in the sample mean for the population mean to get the plug-in estimator, \\[\n\\widehat{\\alpha} = g\\left( \\frac{1}{n} \\sum_{i=1}^n r(X_i) \\right) \\quad \\text{estimates} \\quad \\alpha = g\\left(\\E[r(X_i)]\\right)\n\\] This approach to plug-in estimation with sample means is very general and will allow us to derive estimators in various settings.\n\nExample 2.6 (Estimating population variance) The population variance of a random variable is \\(\\sigma^2 = \\E[(X_i - \\E[X_i])^2]\\). To derive a plug-in estimator for this quantity, we replace the inner \\(\\E[X_i]\\) with \\(\\Xbar_n\\) and the outer expectation with another sample mean: \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2.\n\\] This plug-in estimator differs from the standard sample variance, which divides by \\(n - 1\\) rather than \\(n\\). This minor difference does not matter in moderate to large samples.\n\n\nExample 2.7 (Estimating population covariance) Suppose we have two variables, \\((X_i, Y_i)\\). A natural quantity of interest here is the population covariance between these variables, \\[\n\\sigma_{xy} = \\text{Cov}[X_i,Y_i] = \\E[(X_i - \\E[X_i])(Y_i-\\E[Y_i])],\n\\] which has the plug-in estimator, \\[\n\\widehat{\\sigma}_{xy} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)(Y_i - \\Ybar_n).\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nGiven the connection between the population mean and the sample mean, you will sometimes see the \\(\\E_n[\\cdot]\\) operator used as a shorthand for the sample average: \\[\n\\E_n[r(X_i)] \\equiv \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\n\n\nFinally, plug-in estimation goes beyond just replacing population means with sample means. We can derive estimators of the population quantiles like the median with sample versions of those quantities. What unifies all of these approaches is replacing the unknown population cdf, \\(F\\), with the empirical cdf, \\[\n\\widehat{F}_n(x) = \\frac{\\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}{n}.\n\\] For a more complete and technical treatment of these ideas, see Wasserman (2004) Chapter 7."
  },
  {
    "objectID": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "href": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "title": "2  Estimation",
    "section": "2.5 The three distributions: population, empirical, and sampling",
    "text": "2.5 The three distributions: population, empirical, and sampling\nOnce we start to wade into estimation, there are several distributions to keep track of, and things can quickly become confusing. Three specific distributions are all related and easy to confuse, but keeping them distinct is crucial.\nThe population distribution is the distribution of the random variable, \\(X_i\\), which we have labeled \\(F\\) and is our target of inference. Then there is the empirical distribution, which is the distribution of the actual realizations of the random variables in our samples (that is, the numbers in our data frame), \\(X_1, \\ldots, X_n\\). Because this is a random sample from the population distribution and can serve as an estimator of \\(F\\), we sometimes call this \\(\\widehat{F}_n\\).\n\nSeparately from both is the sampling distribution of an estimator, which is the probability distribution of \\(\\widehat{\\theta}_n\\). It represents our uncertainty about our estimate before we see the data. Remember that our estimator is itself a random variable because it is a function of random variables: the data itself. That is, we defined the estimator as \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\nExample 2.8 (Likert responses) Suppose \\(X_i\\) is the answer to a question, “How much do you agree with the following statement: Immigrants are a net positive for the United States,” with a \\(X_i = 0\\) being “strongly disagree,” \\(X_i = 1\\) being “disagree,” \\(X_i = 2\\) being “neither agree nor disagree,” \\(X_i = 3\\) being “agree,” and \\(X_i = 4\\) being “strongly agree.”\nThe population distribution describes the probability of randomly selecting a person with each one of these values, \\(\\P(X_i = x)\\). The empirical distribution would be the fraction of our data taking each value. And the sampling distribution of the sample mean, \\(\\Xbar_n\\), would be the distribution of the sample mean across repeated samples from the population.\nSuppose the population distribution was binomial with four trials and probability of success \\(p = 0.4\\). We could generate one sample with \\(n = 10\\) and thus one empirical distribution using rbinom:\n\n\n\n\nmy_samp <- rbinom(n = 10, size = 4, prob = 0.4)\nmy_samp\n\n [1] 1 2 1 3 3 0 2 3 2 1\n\ntable(my_samp)\n\nmy_samp\n0 1 2 3 \n1 3 3 3 \n\n\nAnd we can generate one draw from the sampling distribution of \\(\\Xbar_n\\) by taking the mean of this sample:\n\nmean(my_samp)\n\n[1] 1.8\n\n\nBut, if we had a different sample, it would have a different empirical distribution and thus give us a different estimate of the sample mean:\n\nmy_samp2 <- rbinom(n = 10, size = 4, prob = 0.4)\nmean(my_samp2) \n\n[1] 1.6\n\n\nThe sampling distribution is the distribution of these sample means across repeated sampling."
  },
  {
    "objectID": "02_estimation.html#finite-sample-properties-of-estimators",
    "href": "02_estimation.html#finite-sample-properties-of-estimators",
    "title": "2  Estimation",
    "section": "2.6 Finite-sample properties of estimators",
    "text": "2.6 Finite-sample properties of estimators\nAs we discussed when we introduced estimators, their usefulness depends on how well they help us learn about the quantity of interest. If we get an estimate \\(\\widehat{\\theta} = 1.6\\), we would like to know that this is “close” to the true parameter \\(\\theta\\). The sampling distribution is the key to answering these questions. Intuitively, we would like the sampling distribution of \\(\\widehat{\\theta}_n\\) to be as tightly clustered around the true as \\(\\theta\\) as possible. Here, though, we run into a problem: the sampling distribution depends on the population distribution since it is about repeated samples of the data from that distribution filtered through the function \\(\\theta()\\). Since \\(F\\) is unknown, this implies that the sampling distribution will also usually be unknown.\nEven though we cannot precisely pin down the entire sampling distribution, we can use assumptions to derive specific properties of the sampling distribution that will be useful in comparing estimators.\n\n2.6.1 Bias\nThe first property of the sampling distribution concerns its central tendency. In particular, we will define the bias (or estimation bias) of estimator \\(\\widehat{\\theta}\\) for parameter \\(\\theta\\) as \\[\n\\text{bias}[\\widehat{\\theta}] = \\E[\\widehat{\\theta}] - \\theta,\n\\] which is the difference between the mean of the estimator (across repeated samples) and the true parameter. All else equal, we would like estimation bias to be as small as possible. The smallest possible bias, obviously, is 0, and we define an unbiased estimator as one with \\(\\text{bias}[\\widehat{\\theta}] = 0\\) or equivalently, \\(\\E[\\widehat{\\theta}] = \\theta\\).\nHowever, all else is not always equal, and unbiasedness is not a property to become overly attached to. Many biased estimators have other attractive properties, and many popular modern estimators are biased.\n\nExample 2.9 (Unbiasedness of the sample mean) We can show that the sample mean is unbiased for the population mean when the data is iid and \\(\\E|X| < \\infty\\). In particular, we simply apply the rules of expectations: \\[\\begin{aligned}\n\\E\\left[ \\Xbar_n \\right] &= \\E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] & (\\text{definition of } \\Xbar_n) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\E[X_i] & (\\text{linearity of } \\E)\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu & (X_i \\text{ identically distributed})\\\\\n&= \\mu.\n\\end{aligned}\\] Notice that we only used the “identically distributed” part of iid. Independence is not needed.\n\n\n\n\n\n\n\nWarning\n\n\n\nProperties like unbiasedness might only hold for a subset of DGPs. For example, we just showed that the sample mean is unbiased, but only when the population mean is finite. There are probability distributions like the Cauchy where the expected value diverges and is not finite. So we are dealing with a restricted class of DGPs that rules out such distributions. You may see this sometimes formalized by defining a class \\(\\mathcal{F}\\) of distributions, and unbiasedness might hold in that class if it is unbiased for all \\(F \\in \\mathcal{F}\\).\n\n\n\n\n2.6.2 Estimation variance and the standard error\nIf a “good” estimator tends to be close to the truth, we should also care about the spread of the sampling distribution. In particular, we define the sampling variance as the variance of an estimator’s sampling distribution, \\(\\V[\\widehat{\\theta}]\\), which measures how spread out the estimator is around its mean. For an unbiased estimator, lower sampling variance implies the distribution of \\(\\widehat{\\theta}\\) is more concentrated around the truth.\n\nExample 2.10 (Sampling variance of the sample mean:) We can establish the sampling variance of the sample mean of iid data for all \\(F\\) such that \\(\\V[X_i]\\) is finite (more precisely, \\(\\E[X_i^2] < \\infty\\))\n\\[\\begin{aligned}\n  \\V\\left[ \\Xbar_n \\right] &= \\V\\left[ \\frac{1}{n} \\sum_{i=1}^n X_i \\right] & (\\text{definition of } \\Xbar_n) \\\\\n                           &=\\frac{1}{n^2} \\V\\left[ \\sum_{i=1}^n X_i \\right] & (\\text{property of } \\V)\\\\\n                           &=\\frac{1}{n^2} \\sum_{i=1}^n \\V[X_i] & (\\text{independence})\\\\\n                           &= \\frac{1}{n^2}\\sum_{i=1}^n \\sigma^2 & (X_i \\text{ identically distributed})\\\\\n                           &= \\frac{\\sigma^2}{n}\n\\end{aligned}\\]\n\nAn alternative measure of spread for any distribution is the standard deviation, which is on the same scale as the original random variable. We call the standard deviation of the sampling distribution of \\(\\widehat{\\theta}\\) the standand error of \\(\\widehat{\\theta}\\): \\(\\se(\\widehat{\\theta}) = \\sqrt{\\V[\\widehat{\\theta}]}\\).\nGiven the above derivation, the standard error of the sample mean under iid sampling is \\(\\sigma / \\sqrt{n}\\)\n\n\n2.6.3 Mean squared error\nBias and sampling variance measure two different aspects of being a “good” estimator. Ideally, we want the estimator to be as close as possible to the true value. One summary measure of the quality of an estimator is the mean squared error or MSE, which is\n\\[\n\\text{MSE} = \\E[(\\widehat{\\theta}_n-\\theta)^2].\n\\] Ideally, we would have this be as small as possible!\nWe can also relate the MSE to the bias and the sampling variance (provided it is finite) with the following decomposition result: \\[\n\\text{MSE} = \\text{bias}[\\widehat{\\theta}_n]^2 + \\V[\\widehat{\\theta}_n]\n\\] This decomposition implies that, for unbiased estimators, MSE is the sampling variance. It also highlights why we might accept some bias for significant reductions in variance for lower overall MSE.\n\n\n\n\n\nTwo sampling distributions\n\n\n\n\nIn this figure, we show the sampling distributions of two estimators, \\(\\widehat{\\theta}_a\\), which is unbiased (centered on the true value \\(\\theta\\)) but with a high sampling variance, and \\(\\widehat{\\theta}_b\\) which is slightly biased but with much lower sampling variance. Even though \\(\\widehat{\\theta}_b\\) is biased, the probability of drawing a value close to the truth is higher than for \\(\\widehat{\\theta}_a\\). This balancing between bias and variance is precisely what the MSE helps capture and, indeed, in this case, \\(MSE[\\widehat{\\theta}_b] < MSE[\\widehat{\\theta}_a]\\)."
  },
  {
    "objectID": "03_asymptotics.html#introduction",
    "href": "03_asymptotics.html#introduction",
    "title": "3  Asymptotics",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn the last chapter, we defined estimators and started to investigate their finite-sample properties like unbiasedness and the sampling variance. We call these “finite-sample” properties because establishing them generally does not depend on the sample size. We saw that under iid data, the sample mean is unbiased for the population mean, but this result holds as much for \\(n = 10\\) as it does for \\(n = 1,000,000\\). But these properties are also of limited use: we only learn the center and spread of the sampling distribution of \\(\\Xbar_n\\) from these results. What about the shape of the distribution? We can often derive the shape if we are willing to make certain assumptions on the underlying data (for example, if the data is normal, then the sample means will be normal as well), but this approach is brittle: if our parametric assumption is false, we’re back to square one.\nIn this chapter, we’re going to take a different approach and see what happens to the sampling distribution of estimators as the sample size gets large. The study of the estimators as the sample size goes to infinity is called asymptotic theory, but it’s important to understand everything we do with asymptotics will be an approximation. No one ever has infinite data, but we hope that as our samples get larger, the approximations will be closer to the truth. Why work in this asymptopia, though? It turns out that many expressions are much easier to derive in the limit than in finite samples."
  },
  {
    "objectID": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "href": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "title": "3  Asymptotics",
    "section": "3.2 Why convergence with probability is hard",
    "text": "3.2 Why convergence with probability is hard\nIt’s helpful to review the basic idea of convergence in deterministic sequences from calculus:\n\nDefinition 3.1 A sequence \\(\\{a_n: n = 1, 2, \\ldots\\}\\) has the limit \\(a\\) written \\(a_n \\rightarrow a\\) as \\(n\\rightarrow \\infty\\) of \\(\\lim_{n\\rightarrow \\infty} a_n = a\\) if for all \\(\\epsilon > 0\\) there is some \\(n_{\\epsilon} < \\infty\\) such that for all \\(n \\geq n_{\\epsilon}\\), \\(|a_n - a| \\leq \\epsilon\\).\n\nWe say that \\(a_n\\) converges to \\(a\\) if \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Basically, a sequence converges to a number if the sequence gets closer and closer to that number as the sequence goes on.\nCan we apply this same idea to sequences of random variables (like estimators)? Let’s look at a few examples that might help clarify the difficult in doing so.1 Let’s say that we have a sequence of \\(a_n = a\\) for all \\(n\\) (that is, a constant sequence). Then obviously \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Now let’s say we have a sequence of random variables, \\(X_1, X_2, \\ldots\\), that are all independent with a standard normal distribution, \\(N(0,1)\\). From the analogy to the deterministic case, it is tempting to say that \\(X_n\\) converges to \\(X \\sim N(0, 1)\\), but notice that because they are all different random variables, \\(\\P(X_n = X) = 0\\). Thus, we need to be careful about saying how one variable converges to another variable.\nAnother example highlights subtle problems with a sequence of random variables converging to a single value. Suppose we have a sequence of random variables \\(X_1, X_2, \\ldots\\) where \\(X_n \\sim N(0, 1/n)\\). Clearly, \\(X_n\\) will be concentrated around 0 for large values of \\(n\\), so it is tempting to say that \\(X_n\\) converges to 0. But notice that \\(\\P(X_n = 0) = 0\\) because of the nature of continuous random variables."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "href": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "title": "3  Asymptotics",
    "section": "3.3 Convergence in probability and consistency",
    "text": "3.3 Convergence in probability and consistency\nThere are several different ways that a sequence of random variance can converge. The first type of convergence deals with sequence converging to a single value.2\n\nDefinition 3.2 A sequence of random variables, \\(X_1, X_2, \\ldots\\), is said to converge in probability to a value \\(b\\) if for every \\(\\varepsilon > 0\\), \\[\n\\P(|X_n - b| > \\varepsilon) \\rightarrow 0,\n\\] as \\(n\\rightarrow \\infty\\). We write this \\(X_n \\inprob b\\).\n\nWith deterministic sequences, we said that \\(a_n\\) converges to \\(a\\) is it gets closer and closer to \\(a\\) as \\(n\\) gets bigger. For convergence in probability, the sequence of random variables converges to \\(b\\) is the probability that random variables are far away from \\(b\\) get smaller and smaller as \\(n\\) gets big.\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will sometimes see convergence in probability written as \\(\\text{plim}(Z_n) = b\\) if \\(Z_n \\inprob b\\), \\(\\text{plim}\\) stands for “probability limit.”\n\n\nConvergence in probability is incredibly useful for evaluating estimators. While we said that unbiasedness was not the be all and end all of properties of estimators, the following property is a fairly basic and fundamental property that we would like all good estimators to have.\n\nDefinition 3.3 An estimator is consistent if \\(\\widehat{\\theta}_n \\inprob \\theta\\).\n\nConsistency of an estimator implies that the sampling distribution of this estimator “collapses” on the true value as the sample size gets large. We say an estimator is inconsistent if it converges in probability to any other value, which is obviously a very bad property of an estimator. It means that as the sample size gets large, the probability that the estimator will be close to the truth will approach 0.\nWe can also define convergence in probability for a sequence of random vectors, \\(\\X_1, \\X_2, \\ldots\\), where \\(\\X_i = (X_{i1}, \\ldots, X_{ik})\\) is a random vector of length \\(k\\). This sequence convergences in probbaility to a vector \\(\\mb{b} = (b_1, \\ldots, b_k)\\) if and only if each random variable in the vector converges to the corresponding element in \\(\\mb{b}\\), or that \\(X_{nj} \\inprob b_j\\) for all \\(j = 1, \\ldots, k\\)."
  },
  {
    "objectID": "03_asymptotics.html#useful-inequalities",
    "href": "03_asymptotics.html#useful-inequalities",
    "title": "3  Asymptotics",
    "section": "3.4 Useful inequalities",
    "text": "3.4 Useful inequalities\nAt first glance, it appears establishing consistency of an estimator will be difficult. How can we know if a distribution will collapse to a specific value without knowing the shape or family of the distribution? It turns out that there are certain relationships between the mean and variance of a random variable and certain probability statements that hold for all distributions (that have finite variance at least). This will be incredibly helpful to us.\n\nTheorem 3.1 (Markov Inequality) For any r.v. \\(X\\) and any \\(\\delta >0\\), \\[\n\\P(|X| \\geq \\delta) \\leq \\frac{\\E[|X|]}{\\delta}.\n\\]\n\n\nProof. Notice that we can let \\(Y = |X|/\\delta\\) and rewrite the statement as \\(\\P(Y \\geq 1) \\leq \\E[Y]\\) (since \\(E[|X|]/\\delta = \\E[|X|/\\delta]\\) by the properties of expectation), which is what we will show. But notice that \\[\n\\mathbb{1}(Y \\geq 1) \\leq Y.\n\\] Why does this hold? We can investigate the two possible values of the indicator function to see. If \\(Y\\) is less than 1, then the indicator function will be 0, but \\(Y\\) is non-negative so we know that it must be at least as big as 0 so that inequality holds. If \\(Y \\geq 1\\) then the indicator function 1 but we just said that \\(Y \\geq 1\\) so the inequality holds. If we take the expectation of both sides of this inequality, we obtain the result (remember the expectation of an indicator function is the probability of the event being indicated).\n\nIn words, Markov’s inequality says that the probability of a random variable being large in magnitude cannot be high if the average is not large in magnitude. Blitzstein and Hwang 2019) provide a nice intuition behind this result. Let \\(X\\) be the income of a randomly selected individual in a population and set \\(\\delta = 2\\E[X]\\), so that the inequality becomes \\(\\P(X > 2\\E[X]) < 1/2\\) (assuming that all income is nonnegative). Here, the inequality says that the share of the population that has an income twice the average must be less than 0.5, since if more than half the population was making twice the average income then the average would have to be higher.\nIt’s quite astounding how general this result is since it holds for all random variables. Of course, its generality comes at the expense of not being very informative. If \\(\\E[|X|] = 5\\), for instance, the inequality tells us that \\(\\P(|X| \\geq 1) \\leq 5\\) which is not very helpful since we already know that probabilities are less than 1! If we are willing to make some assumptions about \\(X\\), we can get tighter bounds.\n\nTheorem 3.2 (Chebyshev Inequality) Suppose that \\(X\\) is r.v. for which \\(\\V[X] < \\infty\\). Then, for every real number \\(\\delta > 0\\), \\[\n\\P(|X-\\E[X]| \\geq \\delta) \\leq \\frac{\\V[X]}{\\delta^2}.\n\\]\n\n\nProof. To prove this, we only need to square both sides of the inequality inside the probability statement and apply Markov’s inequality: \\[\n\\P\\left( |X - \\E[X]| \\geq \\delta \\right) = \\P((X-\\E[X])^2 \\geq \\delta^2) \\leq \\frac{\\E[(X - \\E[X])^2]}{\\delta^2} = \\frac{\\V[X]}{\\delta^2},\n\\] with the last equality holding by the definition of variance.\n\nThis is a straightforward extension of the Markov result: the probability of a random variable being far away from its mean (that is, \\(|X-\\E[X]|\\) being large) is limited by the variance of the random variable. If we let \\(\\delta = c\\sigma\\), where \\(\\sigma\\) is the standard deviation of \\(X\\), then we can use this result to bound the normalized: \\[\n\\P\\left(\\frac{|X - \\E[X]|}{\\sigma} > c \\right) \\leq \\frac{1}{c^2}.\n\\] This says that the probability of being, say, 2 standard deviations away from the mean must be less than 1/4 = 0.25. Notice that this bound can be quite wide. If \\(X\\) is normally distributed, then we know that just about 5% of draws will be greater than 2 SDs away from the mean, which is much lower than the 25% bound implied by Chebyshev’s inequality."
  },
  {
    "objectID": "03_asymptotics.html#the-law-of-large-numbers",
    "href": "03_asymptotics.html#the-law-of-large-numbers",
    "title": "3  Asymptotics",
    "section": "3.5 The law of large numbers",
    "text": "3.5 The law of large numbers\nWe can now use these inequalities to show how certain estimators are consistent for certain quantities of interest. Why are these inequalities useful for this purpose? Remember that convergence in probability was about the probability of an estimator being far away from a value going to zero. Chebyshev’s inequality shows that we can bound these exact probabilities.\nThe most famous consistency result has a special name.\n\nTheorem 3.3 (Weak Law of Large Numbers) Let \\(X_1, \\ldots, X_n\\) be a an i.i.d. draws from a distribution with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i] < \\infty\\). Let \\(\\Xbar_n = \\frac{1}{n} \\sum_{i =1}^n X_i\\). Then, \\(\\Xbar_n \\inprob \\mu\\).\n\n\nProof. Recall that the sample mean is unbiased, so \\(\\E[\\Xbar_n] = \\mu\\) with sampling variance \\(\\sigma^2/n\\). We can then simply apply Chebyshev to the sample mean to get \\[\n\\P(|\\Xbar_n - \\mu| \\geq \\delta) \\leq \\frac{\\sigma^2}{n\\delta^2}\n\\] An \\(n\\rightarrow\\infty\\), the right-hand side goes to 0 which means that the left-hand side also must go to 0 which is the definition of \\(\\Xbar_n\\) converging in probability to \\(\\mu\\).\n\nThe weak law of large numbers (WLLN) shows that, under general conditions, the sample mean gets closer to the population mean as \\(n\\rightarrow\\infty\\). In fact, this result holds even when the variance of the is infinite, though that’s a situation that most analysts will rarely face.\n\n\n\n\n\n\nNote\n\n\n\nThe naming of the “weak” law of large numbers seems to imply the existence of a “strong” law of large numbers (SLLN) and this is true. The SLLN states that the sample mean converges to the population mean with probability 1. This type of convergence, called almost sure convergence, is stronger than convergence in probability which only says that the probability of the sample mean being close to the population mean converges to 1. While it is nice to know that this stronger form of convergence holds for the sample mean under the same assumptions, it is very rare for folks outside of theoretical probability and statistics to need to rely on almost sure convergence.\n\n\n\nExample 3.1 It can be helpful to see how the distribution of the sample mean changes as a function of the sample size to appreciate the WLLN. We can show this by taking repeated iid samples of different sizes from an exponential rv with rate 0.5 so that \\(\\E[X_i] = 2\\). In Figure 3.1, we show the distribution of the sample mean (across repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue), and 1000 (green). What we can see is how the distribution of the sample mean is “collapsing” on the true population mean, 2. The probability of being far away from 2 becomes progressively smaller.\n\n\n\n\n\nFigure 3.1: Sampling distribution of the sample mean as a function of sample size\n\n\n\n\n\nThe WLLN also holds for random vectors in addition to random variables. Let \\((\\X_1, \\ldots, \\X_n)\\) be an iid sample of random vectors of length \\(k\\), \\(\\mb{X}_i = (X_{i1}, \\ldots, X_{ik})\\). We can define the vector sample mean as just the vector of sample means for each of the entries:\n\\[\n\\overline{\\mb{X}}_n = \\frac{1}{n} \\sum_{i=1}^n \\mb{X}_i =\n\\begin{pmatrix}\n\\Xbar_{n,1} \\\\ \\Xbar_{n,2} \\\\ \\vdots \\\\ \\Xbar_{n, k}\n\\end{pmatrix}\n\\] Since this is just a vector of sample means, each random variable in the random vector will converge in probability to the mean of that random variable. Fortunately, this is the exact definition of convergence in probability for random vectors. We formally write this in the following theorem.\n\nTheorem 3.4 If \\(\\X_i \\in \\mathbb{R}^k\\) are iid draws from a distribution with \\(\\E[X_{ij}] < \\infty\\) for all \\(j=1,\\ldots,k\\) then as \\(n\\rightarrow\\infty\\)\n\\[\n\\overline{\\mb{X}}_n \\inprob \\E[\\X]  =\n\\begin{pmatrix}\n\\E[X_{i1}] \\\\ \\E[X_{i2}] \\\\ \\vdots \\\\ \\E[X_{ik}]\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will have noticed that many of the formal results we have presented so far have “moment conditions” that certain moments are finite. For the vector WLLN, we saw that applied to the mean of each variable in the vector. Some books use a short hand for this: \\(\\E\\Vert \\X_i\\Vert < \\infty\\), where \\[\n\\Vert\\X_i\\Vert = \\left(X_{i1}^2 + X_{i2}^2 + \\ldots + X_{ik}^2\\right)^{1/2}.\n\\] This is slightly more compact notation, but why does it work? One can show that this function, called the Euclidean norm or \\(L_2\\)-norm is a convex function, so we can apply Jensen’s inequality to show that: \\[\n\\E\\Vert \\X_i\\Vert \\geq \\Vert \\E[\\X_i] \\Vert = (\\E[X_{i1}]^2 + \\ldots + \\E[X_{ik}]^2)^{1/2}.\n\\] So if \\(\\E\\Vert \\X_i\\Vert\\) is finite, it means that all the component means are finite otherwise the right-hand side of the previous equation would be infinite."
  },
  {
    "objectID": "03_asymptotics.html#consistency-of-estimators",
    "href": "03_asymptotics.html#consistency-of-estimators",
    "title": "3  Asymptotics",
    "section": "3.6 Consistency of estimators",
    "text": "3.6 Consistency of estimators\nThe WLLN shows that the sample mean of iid draws is consistent for the population mean, which is a massive result given that so many estimators can be written as sample means. What about other estimators? The proof of the WLLN points to one way to determine if an estimator is consistent: if it is unbiased and the sampling variance shrinks as the sample size grows. The next theorem\n\nTheorem 3.5 For any estimator \\(\\widehat{\\theta}_n\\), if \\(\\text{bias}[\\widehat{\\theta}_n] \\to 0\\) and \\(\\V[\\widehat{\\theta}_n] \\rightarrow 0\\) as \\(n\\rightarrow \\infty\\), then \\(\\widehat{\\theta}_n\\) is consistent.\n\nThus, if we can characterize the bias and sampling variance of an estimator, then we should be able to tell if it consistent or not. This is handy since working with the kinds of probability inequalities used for the WLLN can sometimes be quite confusing.\nWhat do we do if it is difficult or impossible to characterize the bias? Consider a plug-in estimator like \\(\\widehat{\\alpha} = \\log(\\Xbar_n)\\) where \\(X_1, \\ldots, X_n\\) are iid from a population with mean \\(\\mu\\). We know that for nonlinear functions like logarithms we have \\(\\log\\left(\\E[Z]\\right) \\neq \\E[\\log(Z)]\\), so \\(\\E[\\widehat{\\alpha}] \\neq \\log(\\E[\\Xbar_n])\\) and the plug-in estimator will be biased for \\(\\log(\\mu)\\). It will also be difficult to obtain an expression for the bias in terms of \\(n\\). Is all hope lost here? Must we give up on consistency? No, and in fact, consistency will be much simpler to show in this setting.\n\nTheorem 3.6 (Properties of convergence in probability) Let \\(X_n\\) and \\(Z_n\\) be two sequences of random variables such that \\(X_n \\inprob a\\) and \\(Z_n \\inprob b\\), and let \\(g(\\cdot)\\) be a continuous function. Then,\n\n\\(g(X_n) \\inprob g(a)\\) (continuous mapping theorem)\n\\(X_n + Z_n \\inprob a + b\\)\n\\(X_nZ_n \\inprob ab\\)\n\\(X_n/Z_n \\inprob a/b\\) if \\(b > 0\\).\n\n\nWe can now see that many of the nasty problems with expectations and nonlinear functions are made considerably easier with convergence in probability in the asymptotic setting. So while we know that \\(\\log(\\Xbar_n)\\) is biased for \\(\\log(\\mu)\\), we know that it is consistent since \\(\\log(\\Xbar_n) \\inprob \\log(\\mu)\\) because \\(\\log\\) is a continuous function.\n\nExample 3.2 Suppose we implemented a survey by randomly selecting a sample from the population of size \\(n\\), but not everyone responded to our survey. Let the data consist of pairs of random variables, \\((Y_1, R_1), \\ldots, (Y_n, R_n)\\), where \\(Y_i\\) is the question of interest and \\(R_i\\) is a binary indicator for if the respondent answered the question (\\(R_i = 1\\)) or not (\\(R_i = 0\\)). Our goal is to estimate the mean of the question for responders: \\(\\E[Y_i \\mid R_i = 1]\\). We can use the law of iterated expectation to which we can rewrite as \\[\n\\begin{aligned}\n\\E[Y_iR_i] &= \\E[Y_i \\mid R_i = 1]\\P(R_i = 1) + \\E[ 0 \\mid R_i = 0]\\P(R_i = 0) \\\\\n\\implies \\E[Y_i \\mid R_i = 1] &= \\frac{\\E[Y_iR_i]}{\\P(R_i = 1)}\n\\end{aligned}\n\\]\nThe relevant estimator for this quantity is the mean of the of the outcome among those who responded, which is slightly more complicated than a typical sample mean because the denominator is a random variable: \\[\n\\widehat{\\theta}_n = \\frac{\\sum_{i=1}^n Y_iR_i}{\\sum_{i=1}^n R_i}.\n\\] Notice that this estimator is the ratio of two random variables. The numerator has mean \\(n\\E[Y_iR_i]\\) and the denominator has mean \\(n\\P(R_i = 1)\\). It is then tempting to say that we can take the ratio of these means as the mean of \\(\\widehat{\\theta}_n\\), but expectations are not preserved in nonlinear functions like this one.\nWe can establish consistency of our estimator, though, by noting that we can rewrite the estimator as a ratio of sample means \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i},\n\\] where by the WLLN the numerator \\((1/n)\\sum_{i=1}^n Y_iR_i \\inprob \\E[Y_iR_i]\\) and the denominator \\((1/n)\\sum_{i=1}^n R_i \\inprob \\P(R_i = 1)\\). Thus, by Theorem 3.6, we have \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i} \\inprob \\frac{\\E[Y_iR_i]}{\\P[R_i = 1]} = \\E[Y_i \\mid R_i = 1]\n\\] so long as the probability of responding is greater than zero. This establishes that our sample mean among responders while biased for the conditional expectation among responders, it is consistent for that quantity.\n\nIt is very important to keep the difference between unbiased and consistent clear in your mind. There are very many silly unbiased estimators that are inconsistent. Let’s go back to our iid sample, \\(X_1, \\ldots, X_n\\) from a population with \\(E[X_i] = \\mu\\). There is nothing in the rule book against defining an estimator \\(\\widehat{\\theta}_{first} = X_1\\) that just uses the first observation as the estimate. This seems like an obviously silly estimator, but it is actually unbiased since \\(\\E[\\widehat{\\theta}_{first}] = \\E[X_1] = \\mu\\). It is inconsistent since the sampling variance of this estimator is just the variance of the population distribution, \\(\\V[\\widehat{\\theta}_{first}] = \\V[X_i] = \\sigma^2\\), which does not change as a function of the sample size. Generally speaking, we can regard “unbiased, but inconsistent” estimators as silly and not worth our time (along with bias and inconsistent estimators).\nThere are also estimators that are biased but consistent that are often much more interesting. We already saw one such estimator in Example 3.2, but there are many more. Maximum likelihood estimators, for example, are (under some regularity conditions) consistent for the parameters of a parametric model, but they are often biased.\n\nExample 3.3 (Plug-in variance estimator) Last chapter, we introduced the plug-in estimator for the population variance, \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2,\n\\] which we will now show is biased but consistent. To see the bias note that we can rewrite the sum of square deviations \\[\\sum_{i=1}^n (X_i - \\Xbar_n)^2 = \\sum_{i=1}^n X_i^2 - n\\Xbar_n. \\] Then, the expectation of the plug-in estimator is \\[\n\\begin{aligned}\n\\E[\\widehat{\\sigma}^2] & = \\E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i^2\\right] - \\E[\\Xbar_n^2] \\\\\n&= \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n \\E[X_iX_j] \\\\\n&= \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j\\neq i} \\underbrace{\\E[X_i]\\E[X_j]}_{\\text{independence}} \\\\\n&= \\E[X_i^2] - \\frac{1}{n}\\E[X_i^2] - \\frac{1}{n^2} n(n-1)\\mu^2 \\\\\n&= \\frac{n-1}{n} \\left(\\E[X_i^2] - \\mu^2\\right) \\\\\n&= \\frac{n-1}{n} \\sigma^2 = \\sigma^2 - \\frac{1}{n}\\sigma^2\n\\end{aligned}.\n\\] Thus, we can see that the bias of the plug-in estimator is \\(-(1/n)\\sigma^2\\) so it slightly underestimates the variance. Nicely, though, the bias shrinks as a function of the sample size, so according to Theorem 3.5 it will be consistent so long as the sampling variance of \\(\\widehat{\\sigma}^2\\) shrinks as a function of the sample size, which it does (though omit that proof here). Of course, simply multiplying this estimator by \\(n/(n-1)\\) will give an unbiased and consistent estimator that is also the typical sample variance estimator."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "href": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "title": "3  Asymptotics",
    "section": "3.7 Convergence in distribution and the central limit theorem",
    "text": "3.7 Convergence in distribution and the central limit theorem\nConvergence in probability and the law of large numbers are very useful for understanding how our estimators will (or will not) collapse to their estimand as the sample size increases. But what about the shape of the sampling distribution of our estimators? For the purposes of statistical inference, we would like to be able to make probability statements such as \\(\\P(a \\leq \\widehat{\\theta}_n \\leq b)\\). These types of statements will be the basis of hypothesis testing and confidence intervals. But in order make those types of statements, we need to know the entire distribution of \\(\\widehat{\\theta}_n\\), not just the mean and variance. Luckily, there are established results that will allow us to approximate the sampling distribution of a huge swath of estimators when our sample sizes are large.\nTo see how we will develop these approximations, we need to first describe a weaker form of convergence to a distribution rather than to a single value.\n\nDefinition 3.4 Let \\(X_1,X_2,\\ldots\\), be a sequence of r.v.s, and for \\(n = 1,2, \\ldots\\) let \\(F_n(x)\\) be the c.d.f. of \\(X_n\\). Then it is said that \\(X_1,X_2, \\ldots\\) converges in distribution to r.v. \\(X\\) with c.d.f. \\(F(x)\\) if \\[\n\\lim_{n\\rightarrow \\infty} F_n(x) = F(x),\n\\] for all values of \\(x\\) for which \\(F(x)\\) is continuous. We write this as \\(X_n \\indist X\\) or sometimes \\(X_n ⇝ X\\).\n\nEssentially, convergence in distribution means that as \\(n\\) gets large, the distribution of \\(X_n\\) becomes more and more similar to the distribution of \\(X\\), which we often call the asymptotic distribution of \\(X_n\\) (other names include the large-sample distribution). If we know that \\(X_n \\indist X\\), then we can use the distribution of \\(X\\) as an approximation to the distribution of \\(X_n\\) and that distribution can be fairly accurate.\nOne of the most remarkable results in probability and statistics is that a large class of estimators will converge in distribution to one particular family of distributions: the normal. This is one reason that we study the normal so much and why investing in building intuition about it will pay off across many domains of applied work. We call this broad class of results the “central limit theorem,” (CLT) but it would probably be more accurate to refer to them as “central limit theorems” since much of statistics is devoted to showing the result in different settings. We now present the simplest CLT for the sample mean.\n\nTheorem 3.7 (Central Limit Theorem) Let \\(X_1, \\ldots, X_n\\) be i.i.d. r.v.s from a distribution with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i]\\). Then if \\(\\E[X_i^2] < \\infty\\), we have \\[\n\\frac{\\Xbar_n - \\mu}{\\sqrt{\\V[\\Xbar_n]}} = \\frac{\\sqrt{n}\\left(\\Xbar_n - \\mu\\right)}{\\sigma} \\indist \\N(0, 1).\n\\]\n\nIn words: the sample mean of a random sample from a population with finite mean and variance will be approximately normally distributed in large samples. Notice how we have not made any assumptions about the distribution of the underlying random variables, \\(X_i\\). They could binary, event count, continuous, anything. This means the CLT is incredibly broadly applicable.\n\n\n\n\n\n\nNotation alert\n\n\n\nWhy do we state the CLT in terms of the sample mean after centering and scaling by its standard error? If we don’t normalize the sample mean in this way, it’s difficult to talk about convergence in distribution because we know from the WLLN that \\(\\Xbar_n \\inprob \\mu\\) so in the limit the distribution of \\(\\Xbar_n\\) is concentrated at point mass around that value. Normalizing by centering and rescaling ensures that the variance of the resulting quantity will be fixed as a function of \\(n\\), so it makes sense to talk about its distribution converging. Sometimes you will see the equivalent result as \\[\n\\sqrt{n}\\left(\\Xbar_n - \\mu\\right) \\indist \\N(0, \\sigma^2).\n\\]\n\n\nWe can use this result to state approximations that we can use when discussing estimators such as \\[\n\\Xbar_n \\overset{a}{\\sim} N(\\mu, \\sigma^2/n),\n\\] where we use \\(\\overset{a}{\\sim}\\) to be “approximately distributed as in large samples.” This allow us to say things like: “in large samples, we should expect the sample mean to between within \\(2\\sigma/\\sqrt{n}\\) of the true mean in 95% of repeated samples.” As you might guess, this will be very important for hypothesis tests and confidence intervals! Estimators so often follow the CLT that we have an expression for this property.\n\nDefinition 3.5 An estimator \\(\\widehat{\\theta}_n\\) is asymptotically normal if for some \\(\\theta\\) \\[\n\\sqrt{n}\\left( \\widehat{\\theta}_n - \\theta \\right) \\indist N\\left(0,\\V[\\widehat{\\theta}_n]\\right).\n\\]\n\n\nExample 3.4 To illustrate how the CLT works, we can simulate the sampling distribution of the (normalized) sample mean at different sample sizes. Let \\(X_1, \\ldots, X_n\\) be iid samples from a Bernoulli with probability of success 0.25. We then draw repeated samples of size \\(n=30\\) and \\(n=100\\) and calculate \\(\\sqrt{n}(\\Xbar_n - 0.25)/\\sigma\\) for each random sample. Figure 3.2 plots the density of these two sampling distributions along with a standard normal reference. We can see that even at \\(n=30\\), the rough shape of the density looks normal, with spikes and valleys due to the discrete nature of the data (the sample mean can only take on 31 possible values in this case). By \\(n=100\\), the sampling distribution is very close to the true standard normal.\n\n\n\n\n\nFigure 3.2: Sampling distributions of the normalized sample mean at n=30 and n=100.\n\n\n\n\n\nThere are several properties of convergence in distribution that are helpful to us.\n\nTheorem 3.8 (Properties of convergence in distribution) Let \\(X_n\\) be a sequence of random variables \\(X_1,X_2,\\ldots\\) that converges in distribution to some rv \\(X\\) and let \\(Y_n\\) be a sequence of random variables \\(Y_1,Y_2,\\ldots\\) that converges in probability to some number, \\(c\\). Then,\n\n\\(g(X_n) \\indist g(X)\\) for all continuous functions \\(g\\).\n\\(X_nY_n\\) converges in distribution to \\(cX\\)\n\\(X_n + Y_n\\) converges in distribution to \\(X + c\\)\n\\(X_n / Y_n\\) converges in distribution to \\(X / c\\) if \\(c \\neq 0\\)\n\n\nThe last 3 of these results are sometimes referred to as Slutsky’s theorem. These results are very commonly used when trying to determine the asymptotic distribution of an estimator.\nOne important application of Slutsky’s theorem is when we replace the (unknown) popoulation variance in the CLT with an estimate. Recall the definition of the sample variance as \\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\Xbar_n)^2,\n\\] with the sample standard deviation defined as \\(s = \\sqrt{s^2}\\). It’s easy to show that these are consistent estimators for their respective population parameters \\[\ns^2 \\inprob \\sigma^2 = \\V[X_i], \\qquad s \\inprob \\sigma,\n\\] which by Slutsky’s theorem implies that \\[\n\\frac{\\sqrt{n}\\left(\\Xbar_n - \\mu\\right)}{s} \\indist \\N(0, 1)\n\\] Comparing this result to the statement of CLT, we see that replacing the population variance with a consistent estimate of the variance (or standard deviation) does not affect the asymptotic distribution.\nLike with the WLLN, the CLT holds for random vectors of sample means, where their centered and scaled versions converge to a multivariate normal distribution with a covariance matrix equal to covariance matrix of the underlying random vectors of data, \\(\\X_i\\).\n\nTheorem 3.9 If \\(\\mb{X}_i \\in \\mathbb{R}^k\\) are i.i.d. and \\(\\E\\Vert \\mb{X}_i \\Vert^2 < \\infty\\), then as \\(n \\to \\infty\\), \\[\n\\sqrt{n}\\left( \\overline{\\mb{X}}_n - \\mb{\\mu}\\right) \\indist \\N(0, \\mb{\\Sigma}),\n\\] where \\(\\mb{\\mu} = \\E[\\mb{X}_i]\\) and \\(\\mb{\\Sigma} = \\V[\\mb{X}_i] = \\E\\left[(\\mb{X}_i-\\mb{\\mu})(\\mb{X}_i - \\mb{\\mu})'\\right]\\).\n\nHere, notice that \\(\\mb{\\mu}\\) is the vector of population means for all the random variables in \\(\\X_i\\) and \\(\\mb{\\Sigma}\\) is the variance-covariance matrix for that vector.\n\n\n\n\n\n\nNote\n\n\n\nAs with the notation alert with the WLLN, we are using a shorthand here, \\(\\E\\Vert \\mb{X}_i \\Vert^2 < \\infty\\), which implies that \\(\\E[X_{ij}^2] < \\infty\\) for all \\(j = 1,\\ldots, k\\), or equivalently, that the variances of each variable in the sample means has finite variance."
  },
  {
    "objectID": "03_asymptotics.html#delta-method",
    "href": "03_asymptotics.html#delta-method",
    "title": "3  Asymptotics",
    "section": "3.8 Delta method",
    "text": "3.8 Delta method\nSuppose that we know that an estimator follows the CLT and so we have \\[\n\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta  \\right) \\indist \\N(0, V),\n\\] but we actually want to estimate \\(h(\\theta)\\) so we use the plug-in estimator, \\(h(\\widehat{\\theta}_n)\\). It seems like we should be able to apply part 1 of Theorem 3.8, the CLT established the large-sample distribution of the centered and scaled random sequence, \\(\\sqrt{n}(\\widehat{\\theta}_n - \\theta)\\), not to the original estimator itself like we would need to investigate the asymptotic distribution of \\(h(\\widehat{\\theta}_n)\\). We can use a little bit of calculus to get an approximation to the distribution we need.\n\nTheorem 3.10 If \\(\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta\\right) \\indist \\N(0, V)\\) and \\(h(u)\\) is continuously differentiable in a neighborhood around \\(\\theta\\), then as \\(n\\to\\infty\\), \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta}_n) - h(\\theta)  \\right) \\indist \\N(0, (h'(\\theta))^2 V).\n\\]\n\nIt’s useful to understand what’s happening here since it might help give intuition as to when this might go wrong. Why do we focus on continuously differentiable functions, \\(h()\\)? These are functions that can be well-approximated with a line in a neighborhood around a given point like \\(\\theta\\). In Figure 3.3, we show this where the tangent line at \\(\\theta_0\\), which has slope \\(h'(\\theta_0)\\), is very similar to \\(h(\\theta)\\) for values close to \\(\\theta_0\\). Because of this, we can approximate the difference between \\(h(\\widehat{\\theta}_n)\\) and \\(h(\\theta_0)\\) with the what this tangent line would give us: \\[\n\\underbrace{\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right)}_{\\text{change in } y} \\approx \\underbrace{h'(\\theta_0)}_{\\text{slope}} \\underbrace{\\left(\\widehat{\\theta}_n - \\theta_0\\right)}_{\\text{change in } x},\n\\] and then multiplying both sides by the \\(\\sqrt{n}\\) gives \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right) \\approx h'(\\theta_0)\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta_0\\right).\n\\] The right-hand side of this approximation converges to \\(h'(\\theta_0)Z\\), where \\(Z\\) is a random variable variable with \\(\\N(0, V)\\). The variance of this quantity will be \\[\n\\V[h'(\\theta_0)Z] = (h'(\\theta_0))^2\\V[Z] = (h'(\\theta_0))^2V,\n\\] by the properties of variances.\n\n\n\n\n\nFigure 3.3: Linear approximation to nonlinear functions\n\n\n\n\n\nExample 3.5 Let’s return to the iid sample \\(X_1, \\ldots, X_n\\) with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i]\\). From the CLT, we know that \\(\\sqrt{n}(\\Xbar_n - \\mu) \\indist \\N(0, \\sigma^2)\\). Suppose that we want to estimate \\(\\log(\\mu)\\) so we use the plug-in estimator \\(\\log(\\Xbar_n)\\) (assuming that \\(X_i > 0\\) for all \\(i\\) so that we can actually take the log). What is the asymptotic distribution of this estimator? This is a situation where \\(\\widehat{\\theta}_n = \\Xbar_n\\) and \\(h(\\mu) = \\log(\\mu)\\). From basic calculus we know that \\[\nh'(\\mu) = \\frac{\\partial \\log(\\mu)}{\\partial \\mu} = \\frac{1}{\\mu},\n\\] so applying the delta method, we can determine that \\[\n\\sqrt{n}\\left(\\log(\\Xbar_n) - \\log(\\mu)\\right) \\indist \\N\\left(0,\\frac{\\sigma^2}{\\mu^2} \\right).\n\\]\n\n\nExample 3.6 What about if we want to estimate the \\(\\exp(\\mu)\\) with \\(\\exp(\\Xbar_n)\\)? Recall that \\[\nh'(\\mu) = \\frac{\\partial \\exp(\\mu)}{\\partial \\mu} = \\exp(\\mu)\n\\] so applying the delta method, we have \\[\n\\sqrt{n}\\left(\\exp(\\Xbar_n) - \\exp(\\mu)\\right) \\indist \\N(0, \\exp(2mu)\\sigma^2),\n\\] since \\(\\exp(\\mu)^2 = \\exp(2\\mu)\\).\n\nLike all of the results in this chapter, there is a multivariate version of the delta method that is incredibly useful in practical applications. This is because we often will take two different estimators (or two different estimated parameters) and combine them to estimate another quantity. We now let \\(\\mb{h}(\\mb{\\theta}) = (h_1(\\mb{\\theta}), \\ldots, h_m(\\mb{\\theta}))\\) map from \\(\\mathbb{R}^k \\to \\mathbb{R}^m\\) and be continuously differentiable (we make the function bold since it ). It will help us use more compact matrix notation if we introduce a \\(m \\times k\\) Jacobian matrix of all partial derivatives \\[\n\\mb{H}(\\mb{\\theta}) = \\mb{\\nabla}_{\\mb{\\theta}}\\mb{h}(\\mb{\\theta}) = \\begin{pmatrix}\n  \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_k} \\\\\n  \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_k}\n\\end{pmatrix},\n\\] which we can use to generate the equivalent multivariate linear approximation \\[\n\\left(\\mb{h}(\\widehat{\\mb{\\theta}}_n) - \\mb{h}(\\mb{\\theta}_0)\\right) \\approx \\mb{H}(\\mb{\\theta}_0)'\\left(\\widehat{\\mb{\\theta}}_n - \\mb{\\theta}_0\\right).\n\\] We can use this fact to derive the multivariate delta method.\n\nTheorem 3.11 Suppose that \\(\\sqrt{n}\\left(\\widehat{\\mb{\\theta}}_n - \\mb{\\theta}_0 \\right) \\indist \\N(0, \\mb{\\Sigma})\\), then for any function \\(\\mb{h}\\) that is continuously differentiable in a neighborhood of \\(\\mb{\\theta}_0\\), we have \\[\n\\sqrt{n}\\left(\\mb{h}(\\widehat{\\mb{\\theta}}_n) - \\mb{h}(\\mb{\\theta}_0) \\right) \\indist \\N(0, \\mb{H}\\mb{\\Sigma}\\mb{H}'),\n\\] where \\(\\mb{H} = \\mb{H}(\\mb{\\theta}_0)\\).\n\nThis result follows from the approximation above plus rules about variances of random vectors. Remember that for any compatible matrix of constants, \\(\\mb{A}\\), we have \\(\\V[\\mb{A}'\\mb{Z}] = \\mb{A}\\V[\\mb{Z}]\\mb{A}'\\). You can see that the matrix of constants appears twice here, sort of like the matrix version of “squaring the constant” rule for variance.\nThe delta method is a very useful for generating closed-form approximations for asymptotic standard errors, but the math is often quite complex for even simple estimators. For applied researchers, it is usually more straightforward to use computational tools like the bootstrap to approximate the standard errors we need. This has the trade-off of taking more computational time to implement than the delta method, but is more easily adaptable across different estimators and domains with little human thinking time."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-lady-tasting-tea",
    "href": "04_hypothesis_tests.html#the-lady-tasting-tea",
    "title": "4  Hypothesis tests",
    "section": "4.1 The lady tasting tea",
    "text": "4.1 The lady tasting tea\nThe lady tasting tea is an example of the core ideas behind hypothesis testing due to R.A. Fisher.1 Fisher had prepared a cup of tea for his colleague, the algologist Muriel Bristol. Knowing that she preferred milk in her tea, he poured milk into a tea cup and then poured the hot tea into the milk. Bristol rejected the cup, stating that she preferred the tea to be poured first, then milk. Fisher was apparently incredulous at the idea anyone could tell the difference between a cup poured milk-first or tea-first. So he and another colleague, William Roach, devised a test to see if Bristol had the ability to distinguish the two preparation methods.\nFor this test, Fisher and Roach prepared 8 cups of tea, 4 prepared milk-first and 4 prepared tea-first. They then presented the cups to Bristol in a random order (though she knew there were 4 of each type), and she proceeded to identify all of the cups correctly. At a first glance, this seems like good evidence that she can tell the difference between the two types, but a skeptic like Fisher raised the question: “could she have just been randomly guessing and got lucky?” This led Fisher to a statistical thought experiment: what would the probability of guessing all cups correctly if she were guessing randomly?\nTo calculate the probability of Bristol’s achievement, we can note that “randomly guessing” here would mean that she were selecting a group of 4 cups to be labeled milk-first from the 8 cups available. Using basic combinatorics, there are 70 ways to choose 4 cups among 8, but only 1 of those arrangements would be correct. Thus, if randomly guessing means choosing among those 70 options with equal chance, then the probability of guessing correctly is 1/70 or \\(\\approx 0.014\\). This probability being so low implies that the hypothesis of random guessing may be implausible.\nThe story of the lady tasting tea encapsulates many of the core elements of hypothesis testing. Hypothesis testing is about taking our observed estimate (Bristol guessing all the cups correctly) and seeing how likely that observed estimate would be under some assumption or hypothesis about the data generating process (Bristol was randomly guessing). When the observed estimate is very unlikely under the maintained hypothesis, we might view this as evidence against that hypothesis. Thus, hypothesis tests help us assess evidence for particular guesses about the DGP.\n\n\n\n\n\n\nNotation alert\n\n\n\nFor the rest of this chapter, we’ll introduce the concepts following the notation in the past chapters. We’ll usually assume that we have a random (iid) sample of random variables \\(X_1, \\ldots, X_n\\) from a distribution, \\(F\\). We’ll focus on estimating some parameter, \\(\\theta\\) of this distribution (like the mean, median, variance, etc). We’ll refer to \\(\\Theta\\) as the set of possible values of \\(\\theta\\), or the parameter space."
  },
  {
    "objectID": "04_hypothesis_tests.html#hypotheses",
    "href": "04_hypothesis_tests.html#hypotheses",
    "title": "4  Hypothesis tests",
    "section": "4.2 Hypotheses",
    "text": "4.2 Hypotheses\nIn the context of hypothesis testing, hypotheses are just statements about the population distribution. In particular, we will make statements that \\(\\theta = \\theta_0\\) where \\(\\theta_0 \\in \\Theta\\) is the hypothesized value of \\(\\theta\\). Hypotheses are ubiquitous in empirical work, but here are some examples to give you a flavor:\n\nThe population proportion of US citizens that identify as Democrats is 0.33.\nThe population difference in average voter turnout between households who received receiving get-out-the-vote mailers vs those who did not is 0.\nThe difference in average incidence of human rights abuse in countries that signed a human rights treaty vs those countries that did not sign is 0.\n\nEach of these is a statement about the true DGP. The latter two are very common: when \\(\\theta\\) represents the difference in means between two groups, then \\(\\theta = 0\\) is the hypothesis of no true difference in population means, or no treatment effect (if the causal effect is identified).\nThe goal of hypothesis testing is to adjudicate between two complementary hypotheses.\n\nDefinition 4.1 The two hypotheses in a hypothesis test are called the null hypothesis and the alternative hypothesis, denoted as \\(H_0\\) and \\(H_1\\), respectively.\n\nThese hypotheses are complementary, so that if the null hypothesis \\(H_0: \\theta \\in \\Theta_0\\), then the alternative hypothesis is \\(H_1: \\theta \\in \\Theta_0^c\\). The “null” in null hypothesis might seem odd until you realize that most hypotheses being tested are that there is no effect of some treatment or no difference in means. For example, suppose \\(\\theta\\) is the difference in mean support for expanding legal immigration between a treatment group that received a pro-immigrant message along with some facts about immigration and a control group that just received the factual information. Then, the typical null hypothesis would be no difference in means or \\(H_0: \\theta = 0\\) and the alternative would be \\(H_1: \\theta \\neq 0\\).\nThere are two types of tests that differ by the form of their null and alternative hypotheses. A two-sided test is of the form \\[\nH_0: \\theta = \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta \\neq \\theta_0,\n\\] where the “two-sided” part refers to how the alternative contains values of \\(\\theta\\) above and below the null value \\(\\theta_0\\). A one-sided test has the form \\[\nH_0: \\theta \\leq \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta > \\theta_0,\n\\] or \\[\nH_0: \\theta \\geq \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta < \\theta_0.\n\\] Two-sided tests are much more common in the social science where we want to know if there is any evidence, positive or negative, against the presumption of no treatment effect or no relationship between two variables. One-sided tests are for situations where we only want evidence in one direction, which is rarely relevant to social science research. One-sided tests also have the downside of being misused to inflate the strength of evidence against the null and so should be avoided. Unfortunately, the math of two-sided tests are also more complicated."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-procedure-of-hypothesis-testing",
    "href": "04_hypothesis_tests.html#the-procedure-of-hypothesis-testing",
    "title": "4  Hypothesis tests",
    "section": "4.3 The procedure of hypothesis testing",
    "text": "4.3 The procedure of hypothesis testing\nAt the most basic level, a hypothesis test is a rule that specifies values of the sample data for which we will decide to reject the null hypothesis. Let \\(\\mathcal{X}_n\\) be the range of the sample—that is, all possible vectors \\((x_1, \\ldots, x_n)\\) that have positive probability of occurring. Then, a hypothesis test describes a region of this space, \\(R \\subset \\mathcal{X}_n\\), called the rejection region where when \\((X_1, \\ldots, X_n) \\in R\\) we will reject \\(H_0\\) and when the data is outside this region, \\((X_1, \\ldots, X_n) \\notin R\\) we retain, accept, or fail to reject the null hypothesis.2\nHow do we decide what the rejection region should be? Even though we define the rejection region in terms of the sample space, \\(\\mathcal{X}_n\\), it’s unwieldy to work with the entire vector of data. Instead, we often formulate the rejection region in terms of a test statistic, \\(T = T(X_1, \\ldots, X_n)\\), where the rejection region becomes \\[\nR = \\left\\{(x_1, \\ldots, x_n) : T(x_1, \\ldots, x_n) > c\\right\\},\n\\] where \\(c\\) is called the critical value. In words, this says that the rejection region are the parts of the sample space that make the test statistic sufficiently large. We reject null hypotheses when the observed data is incompatible with those hypotheses, where the test statistic should be a measure of this incompatibility. Note that the test statistic is a random variable and has a distribution—we will exploit this to understand the different properties of a hypothesis test.\n\nExample 4.1 Suppose that \\((X_1, \\ldots, X_n)\\) measure whether a sample of US citizens support the current US president (\\(X_i = 1\\)) or not (\\(X_i = 0\\)). We might be interested in the test of the null hypothesis that the president does not have the support of a majority of American citizens. Let \\(\\mu = \\E[X_i] = \\P(X_i = 1)\\). Then, a one-sided test would compare the two hypotheses \\[\nH_0: \\mu \\leq 0.5 \\quad\\text{versus}\\quad H_1: \\mu > 0.5.\n\\] In this case, we might use the sample mean as the test statistic, so that \\(T(X_1, \\ldots, X_n) = \\Xbar_n\\) and we have to find some threshold above 0.5 such that we would reject the null, \\[\nR = \\left\\{(x_1, \\ldots, x_n): \\Xbar_n > c\\right\\}.\n\\] In words, how much support should we see for the current president before we decide to reject the notion that they lack majority support. Below we are going to select the critical value, \\(c\\), to have nice statistical properties.\n\nThe structure of a reject region will depend on whether a test is one- or two-sided. One-sided tests will take the form \\(T > c\\), whereas two-sided tests will take the form \\(|T| > c\\), since we want to count deviations from either side of the null hypothesis as evidence against that null."
  },
  {
    "objectID": "04_hypothesis_tests.html#testing-errors",
    "href": "04_hypothesis_tests.html#testing-errors",
    "title": "4  Hypothesis tests",
    "section": "4.4 Testing errors",
    "text": "4.4 Testing errors\nIf we are making a decision about whether to reject a null hypothesis or not, it is possible that we will make the incorrect decision. In particular, there are two ways for us to make errors and two ways for us to be correct in this setting, as shown in Table 4.1. The labels are confusing, but it’s helpful to remember that type I errors (said “type one”) are labelled so because they are the worse of the two types of errors. This is when we reject a null (say there is a true treatment effect or relationship) when in fact the null is true (there is no true treatment effect or relationship). Type I errors are what we see in the replication crisis: lots of “significant” effects that turn out later to be null. Type II errors (said “type two”) are considered less problematic: there is a true relationship, but we cannot detect it with our test (cannot reject the null).\n\n\nTable 4.1: Typology of testing errors\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nRetain \\(H_0\\)\nAwesome\nType II error\n\n\nReject \\(H_0\\)\nType I error\nGreat\n\n\n\n\nIdeally, we would minimize the chances that we make either a type I or type II error. Unfortunately, because the test statistic is a random variable, we cannot remove the probability of an error completely. Instead, we will try to derive tests that have some guaranteed performance in terms of minimizing the probability of type I error. To derive this, we can define the power function of a test, \\[\n\\pi(\\theta) = \\P\\left(  \\text{Reject } H_0 \\mid \\theta \\right) = \\P\\left( T \\in R \\mid \\theta \\right),\n\\] which is the probability of rejection as a function of the parameter of interest, \\(\\theta\\). This tells us, for example, how likely we are to reject the null of no treatment effect as we vary the true size of the treatment effect.\nFrom the power function we can define the probability of type I error.\n\nDefinition 4.2 The size of a hypothesis test with the null hypothesis \\(H_0: \\theta = \\theta_0\\) is \\[\n\\pi(\\theta_0) = \\P\\left( \\text{Reject } H_0 \\mid \\theta_0 \\right).\n\\]\n\nYou can think of the size of a test as the rate of false positives (or false discoveries) produced by the test. Figure 4.1 shows an example of rejection regions, size, and power for a one-sided test. In the left panel, we have the distribution of the test statistic under the null, with \\(H_0: \\theta = \\theta_0\\) and the rejection region is defined by values \\(T > c\\). The shaded grey region is the probability of rejection under this null hypothesis, or the size of the test. Sometimes by random chance we will get samples that are extreme even under the null, leading to false discoveries.3\nIn the right panel, we overlay the distribution of the test statistic under one particular alternative, \\(\\theta = \\theta_1 > \\theta_0\\). The red shaded region is the probability of rejecting the null when this alternative is true, or the power—it’s the probability of correctly rejecting the null when it is false. Intuitively, we can see that alternatives that produces test statistics closer to the rejection region will have higher power. This makes sense: it should be easier to detect big deviations from the null than small deviations from the null.\n\n\n\n\n\nFigure 4.1: Size of a test and power against an alternative\n\n\n\n\nFigure 4.1 also hints at a tradeoff between size and power. Notice that we could make the size smaller (lower the false positive rate) by increasing the critical value to \\(c' > c\\). This would make the probability of being in the rejection region smaller, \\(\\P(T > c' \\mid \\theta_0) < \\P(T > c \\mid \\theta_0)\\), leading to a lower sized test. Unfortunately, it would also reduce power in the right panel since the probability of being in the rejection region will be lower under any alternative, \\(\\P(T > c' \\mid \\theta_1) < \\P(T > c \\mid \\theta_1)\\). This means we usually cannot simultaneously reduce both types of errors at the same time."
  },
  {
    "objectID": "04_hypothesis_tests.html#determining-the-rejection-region",
    "href": "04_hypothesis_tests.html#determining-the-rejection-region",
    "title": "4  Hypothesis tests",
    "section": "4.5 Determining the rejection region",
    "text": "4.5 Determining the rejection region\nIf we cannot simultaneously optimize both the size and power of a test, how should we determine where the reject region is? That is, how should we determine what empirical evidence will be strong enough for us to reject the null? The standard approach to this problem in hypothesis testing is to control the size of a test (that is, control the rate of false positives) and try to maximize the power of the test subject to that constraint. So we say “I’m willing to accept at most x%” of findings will be false positives and do whatever we can to maximize power subject to that constraint.\n\nDefinition 4.3 A test has significance level \\(\\alpha\\) if its size is less than or equal to \\(\\alpha\\), or \\(\\pi(\\theta_0) \\leq \\alpha\\).\n\nA test with a significance level of \\(\\alpha = 0.05\\) means that the test will have a false positive/type I error rate no larger than 0.05. This is an extremely common level for tests in the social science, though you also will \\(\\alpha = 0.01\\) or \\(\\alpha = 0.1\\). Frequentists justify this by saying this means that with \\(\\alpha = 0.05\\), there will only 5% of studies that will produce false discoveries.\nOur task, then, is to construct the rejection region so that the null distribution of the test statistic \\(G_0(t) = \\P(T \\leq t \\mid \\theta_0)\\) to have less than \\(\\alpha\\) probability in that region. One-sided tests like in Figure 4.1 are easiest to show this even though we have warned that you shouldn’t use them. We want to choose \\(c\\) that puts no more than \\(\\alpha\\) probability in the tail, or \\[\n\\P(T > c \\mid \\theta_0) = 1 - G_0(c) \\leq \\alpha.\n\\] Remembering that the smaller the value of \\(c\\) we can use will maximize power, which implies that the critical value for the maximum power while maintaining the significance level is when \\(1 - G_0(c) = \\alpha\\). We can use the quantile function of the null distribution to find the exact value of \\(c\\) we need, \\[\nc = G^{-1}_0(1 - \\alpha),\n\\] which is just fancy math to say “the value at which \\(1-\\alpha\\) of the null distribution is below.”\nThe determination of the rejection region follows the same principles with two-sided tests, but it is slightly more complicated because typically need to reject when the magnitude of the test statistic is large, \\(|T| > c\\). Figure 4.2 shows that basic setup. Notice that because there are two (disjoint) regions, we can write the size (false positive rate) as \\[\n\\pi(\\theta_0) = G_0(-c) + 1 - G_0(c)\n\\] In most cases that we will see, the null distribution for such a test will be symmetric around 0 (usually asymptotically standard normal, actually), which means that \\(G_0(-c) = 1 - G_0(c)\\), which implies that the size is \\[\n\\pi(\\theta_0) = 2(1 - G_0(c)).\n\\] Solving for the critical value that would make this \\(\\alpha\\) gives \\[\nc = G^{-1}_0(1 - \\alpha/2).\n\\] Again, this formula can seem dense, but just remember what you are doing: finding the value that puts \\(\\alpha/2\\) of the probability of the null distribution in each of the tails.\n\n\n\n\n\nFigure 4.2: Rejection regions for a two-sided test."
  },
  {
    "objectID": "04_hypothesis_tests.html#hypothesis-tests-of-the-sample-mean",
    "href": "04_hypothesis_tests.html#hypothesis-tests-of-the-sample-mean",
    "title": "4  Hypothesis tests",
    "section": "4.6 Hypothesis tests of the sample mean",
    "text": "4.6 Hypothesis tests of the sample mean\nLet’s go through an extended example about hypothesis testing of a sample mean, sometimes called a one-sample test. Let’s say \\(X_i\\) are feeling thermometer scores about “liberals” as a group on a scale of 0 to 100, with values closer to 0 indicating cooler feelings about liberals and values closer to 100 indicating warmer feelings about liberals. We might want to know if the true population average is different from a neutral value of 50. We can write this two-sided test as \\[\nH_0: \\mu = 50 \\quad\\text{versus}\\quad H_1: \\mu \\neq 50,\n\\] where \\(\\mu = \\E[X_i]\\). The standard test statistic for this type of test is the so-called t-statistic, \\[\nT = \\frac{\\left( \\Xbar_n - \\mu_0 \\right)}{\\sqrt{s^2 / n}} =\\frac{\\left( \\Xbar_n - 50 \\right)}{\\sqrt{s^2 / n}},\n\\] where \\(\\mu_0\\) is the null value of interest and \\(s^2\\) is the sample variance. If the null hypothesis is true, then by the CLT we know that the t-statistic is asympotically normal, \\(T \\indist \\N(0, 1)\\). Thus, we know that the null distribution can be approximated with standard normal!\nLet’s say that we want to create a test with level \\(\\alpha = 0.05\\). Then we need to find the rejection region that puts \\(0.05\\) probability in the tails of the null distribution, which we just saw was \\(\\N(0,1)\\). Let \\(\\Phi()\\) be the CDF for the standard normal and let \\(\\Phi^{-1}()\\) be the quantile function for the standard normal. Drawing on what we developed above, you can find the value \\(c\\) so that \\(\\P(|T| > c \\mid \\mu_0)\\) is 0.05 with \\[\nc = \\Phi^{-1}(1 - 0.05/2) \\approx 1.96,\n\\] which means that a test where we reject when \\(|T| > 1.96\\) would have a level that will be 0.05 asymptotically."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-wald-test",
    "href": "04_hypothesis_tests.html#the-wald-test",
    "title": "4  Hypothesis tests",
    "section": "4.7 The Wald test",
    "text": "4.7 The Wald test\nWe can generalize the hypothesis test for the sample mean to estimators more broadly. Let \\(\\widehat{\\theta}_n\\) be an estimator for some parameter \\(\\theta\\) and let \\(\\widehat{\\textsf{se}}[\\widehat{\\theta}_n]\\) be a consistent estimate of the standard error of the estimator, \\(\\textsf{se}[\\widehat{\\theta}_n] = \\sqrt{\\V[\\widehat{\\theta}_n]}\\). We consider the two-sided test \\[\nH_0: \\theta = \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta \\neq \\theta_0\n\\]\nIn many cases, our estimators will be asymptotically normal by a version of the CLT. This means that under the null hypothesis, we will have \\[\nT = \\frac{\\widehat{\\theta}_n - \\theta_0}{\\widehat{\\textsf{se}}[\\widehat{\\theta}_n]} \\indist \\N(0, 1).\n\\] The Wald test rejects \\(H_0\\) when \\(|T| > z_{\\alpha/2}\\), where \\(z_{\\alpha/2}\\) that puts \\(\\alpha/2\\) in the upper tail of the standard normal. That is, if \\(Z \\sim \\N(0, 1)\\), then \\(z_{\\alpha/2}\\) satisfies \\(\\P(Z \\geq z_{\\alpha/2}) = \\alpha/2\\).\n\n\n\n\n\n\nNote\n\n\n\nIn R, you can find the \\(z_{\\alpha/2}\\) values easily with the qnorm() function:\n\nqnorm(0.05 / 2, lower.tail = FALSE)\n\n[1] 1.959964\n\n\n\n\n\nTheorem 4.1 Asymptotically, the Wald test has size \\(\\alpha\\) such that \\[\n\\P(|T| > z_{\\alpha/2} \\mid \\theta_0) \\to \\alpha.\n\\]\n\nThis is a very general result and it means that many, many hypothesis tests based on estimators will have the same form. The main difference across estimators will be the exact way that we calculate the estimated standard error.\n\nExample 4.2 (Difference in proportions) In get-out-the-vote (GOTV) experiments, we might randomly assign a group of citizens to receive mailers that encourage them to vote, whereas a control group receives no message. We’ll define the turnout variables in the treatment group \\(Y_{1}, Y_{2}, \\ldots, Y_{n_t}\\) as iid draws from a Bernoulli distribution with success \\(p_t\\) which represents the population turnout rate among treated citizens. The outcomes in the control group \\(X_{1}, X_{2}, \\ldots, X_{n_c}\\) are iid draws from another Bernoulli distribution with success \\(p_c\\), which represents the population turnout rate among citizens not receiving a mailer.\nOur goal is to learn about the treatment effect of this treatment on whether or not the citizen votes, \\(\\tau = p_t - p_c\\) and we will use the sample difference in means/proportions as our estimator, \\(\\widehat{\\tau} = \\Ybar - \\Xbar\\). To perform a Wald test, we need to know/estimate the standard error of this estimator. Notice that because these are independent samples, the variance is \\[\n\\V[\\widehat{\\tau}_n] =  \\V[\\Ybar - \\Xbar] = \\V[\\Ybar] + \\V[\\Xbar] = \\frac{p_t(1-p_t)}{n_t} + \\frac{p_c(1-p_c)}{n_c},\n\\] where the third equality comes from the fact that the underlying outcome variables \\(Y_i\\) and \\(X_j\\) are binary. Obviously we do not know the true population proportions \\(p_t\\) and \\(p_c\\) (that’s why we’re doing the test!), but we can estimate the standard error by replacing them with their estimates \\[\n\\widehat{\\textsf{se}}[\\widehat{\\tau}] = \\sqrt{\\frac{\\Ybar(1 -\\Ybar)}{n_t} + \\frac{\\Xbar(1-\\Xbar)}{n_c}}.\n\\]\nThe typical null hypothesis test in this case is “no treatment effect” vs “some treatment effect” or \\[\nH_0: \\tau = p_t - p_c = 0 \\quad\\text{versus}\\quad H_1: \\tau \\neq 0,\n\\] which gives the following test statistic for the Wald test \\[\nT = \\frac{\\Ybar - \\Xbar}{\\sqrt{\\frac{\\Ybar(1 -\\Ybar)}{n_t} + \\frac{\\Xbar(1-\\Xbar)}{n_c}}}.\n\\] If we wanted a test with level \\(\\alpha = 0.01\\), we would reject the null when \\(|T| > 2.58\\) since\n\nqnorm(0.01/2, lower.tail = FALSE)\n\n[1] 2.575829\n\n\n\n\nExample 4.3 (Difference in means) Let’s take a similar setting to the last example with randomly assigned treatment and control groups, but now the treatment is an appeal for donations and the outcomes are continuous measures of how much a person donated to the political campaign. Now the treatment data \\(Y_1, \\ldots, Y_{n_t}\\) are iid draws from a population with mean \\(\\mu_t = \\E[Y_i]\\) and population variance \\(\\sigma^2_t = \\V[Y_i]\\). The control data \\(X_1, \\ldots, X_{n_c}\\) are iid draws (independent of the \\(Y_i\\)) from a population with mean \\(\\mu_c = \\E[X_i]\\) and population variance \\(\\sigma^2_c = \\V[X_i]\\). The parameter of interest is similar to before: the population difference in means, \\(\\tau = \\mu_t - \\mu_c\\), and we’ll form the usual hypothesis test of \\[\nH_0: \\tau = \\mu_t - \\mu_c = 0 \\quad\\text{versus}\\quad H_1: \\tau \\neq 0.\n\\]\nThe only difference between this setting and the difference in proportions is the standard error here will be different because we cannot rely on the Bernoulli. Instead, we’ll use our knowledge of the sampling variance of the sample means and independence between the samples to derive \\[  \n\\V[\\widehat{\\tau}] = \\V[\\Ybar] + \\V[\\Xbar] = \\frac{\\sigma^2_t}{n_t} + \\frac{\\sigma^2_c}{n_c},\n\\] where we can come up with an estimate of the unknown population variance with sample variances \\[  \n\\widehat{\\se}[\\widehat{\\tau}] = \\sqrt{\\frac{s^2_t}{n_t} + \\frac{s^2_c}{n_c}}.\n\\] This leads to the Wald test statistic of \\[\nT = \\frac{\\widehat{\\tau} - 0}{\\widehat{\\se}[\\widehat{\\tau}]} = \\frac{\\Ybar - \\Xbar}{\\sqrt{\\frac{s^2_t}{n_t} + \\frac{s^2_c}{n_c}}},\n\\] and if want an asymptotically level of 0.05, we can reject when \\(|T| > 1.96\\)."
  },
  {
    "objectID": "04_hypothesis_tests.html#p-values",
    "href": "04_hypothesis_tests.html#p-values",
    "title": "4  Hypothesis tests",
    "section": "4.8 p-values",
    "text": "4.8 p-values\nThe hypothesis testing framework is clearly designed for actually making a decision in the face of uncertainty. You choose a level of wrongness you are comfortable with (rate of false positives) and then make a decision null vs alternative based firmly on the rejection region. This has the downside that when we’re not really making a decision, we are somewhat artificially discarding information about the strength of evidence. We “accept” the null if \\(T = 1.95\\) in the last example but reject it if \\(T = 1.97\\) even though these two situations are actually very similar. Just reporting the reject/retain decision also fails to give us a sense of at what other levels we might have rejected the null. Again, this makes sense if we need to make a single decision: other tests don’t matter because we carefully considered our \\(\\alpha\\) level test. But in the lower stakes world of the academic social sciences, we can afford to be more informative.\nOne alternative to reporting the reject/retain decision is to report a p-value.\n\nDefinition 4.4 The p-value of a test is the probability of observing a test statistic is at least as extreme as the observed test statistic in the direction of the alternative hypothesis.\n\nThe line about “in the direction of the alternative hypothesis” deals with the unfortunate headache of one-sided versus two-sided tests. For a one-sided test where larger values of \\(T\\) correspond to more evidence for \\(H_1\\), the p-value is \\[\n\\P(T(X_1,\\ldots,X_n) > T \\mid \\theta_0) = 1 - G_0(T),\n\\] whereas for a (symmetric) two-sided test we have \\[\n\\P(|T(X_1, \\ldots, X_n)| > |T| \\mid \\theta_0) = 2(1 - G_0(|T|)).\n\\]\nIn either case, the interpretation of the p-value is the same. It is the smallest size \\(\\alpha\\) at which a test would reject null. Presenting a p-value allows the reader to determine their own \\(\\alpha\\) level and determine quickly if the evidence would warrant rejecting \\(H_0\\) in that case. Thus, the p-value is a more continuous measure of evidence against the null, where lower values are stronger evidence against the null because the observed result is less likely under the null.\nThere is quite a bit of controversy surrounding p-values but most of it focuses on arbitrary p-value cutoffs for determining statistical significance and sometimes publication decisions. This really isn’t the fault of p-values, but rather the hyperfixation on the reject/retain decision for arbitrary test levels like \\(\\alpha = 0.05\\). It might be best to view p-values as a transformation of the test statistic onto a common scale between 0 and 1.\n\n\n\n\n\n\nWarning\n\n\n\nThere are many statistical shibboleths that people use to purportedly identify people that don’t understand statistics and they usually hinge on seemingly subtle differences in interpretation that are easy to miss. If you understand the core concepts, the statistical shibboleths tend to be overblown, but it would be malpractice not flag them for you.\nThe shibboleth with p-values is that sometimes people will interpret them as “the probability that the null hypothesis is true.” Of course, this doesn’t make sense from our definition because the p-values conditions on the null hypothesis—it cannot tell us anything about the probability of that null hypothesis. Instead, the metaphor you should always carry is that hypothesis tests are statistical thought experiments and that p-values answer the question: how likely would my data be if the null were true?"
  },
  {
    "objectID": "04_hypothesis_tests.html#power-analysis",
    "href": "04_hypothesis_tests.html#power-analysis",
    "title": "4  Hypothesis tests",
    "section": "4.9 Power analysis",
    "text": "4.9 Power analysis\nImagine you have spent a large research budget on a big experiment to test your amazing theory and the results come back and… you fail to reject the null of no treatment effect. When this happens, there are two possible states of the world: the null is true and you correctly identified that, or the null is false but the test had lower power to detect the true effect. Because of this uncertainty after the fact, it is common for researchers to conduct power analyses prior to running studies that try to forecast what sample size is necessary to ensure you will be able to reject the null under a hypothesized effect size.\nGenerally power analyses involve calculating the power function \\(\\pi(\\theta) = \\P(T(X_1, \\ldots, X_n) \\in R \\mid \\theta)\\) for different values of \\(\\theta\\). It might also involve sample size calculations for a particular alternative, \\(\\theta_1\\). In that case, we try to find the sample size \\(n\\) that would make the power \\(\\pi(\\theta_1)\\) as close to a particular value (often 0.8) as possible. In simple one-sided tests, it is possible to solve for this sample size explicitly, but for more general situations or two-sided tests, we typically need to use numerical or simulation-based approaches to finding the optimal sample size.\nWith Wald tests, we can characterize the power function quite easily even if it does not allow us to back out sample size calculations easily.\n\nTheorem 4.2 For a Wald test with an asymptotically normal estimator, the power function for a particular alternative \\(\\theta_1 \\neq \\theta_0\\) is \\[\n\\pi(\\theta_1) = 1 - \\Phi\\left( \\frac{\\theta_0 - \\theta_1}{\\widehat{\\se}[\\widehat{\\theta}_n]} + z_{\\alpha/2} \\right) + \\Phi\\left( \\frac{\\theta_0 - \\theta_1}{\\widehat{\\se}[\\widehat{\\theta}_n]}-z_{\\alpha/2} \\right).\n\\]"
  },
  {
    "objectID": "04_hypothesis_tests.html#exact-tests-under-normal-data",
    "href": "04_hypothesis_tests.html#exact-tests-under-normal-data",
    "title": "4  Hypothesis tests",
    "section": "4.10 Exact tests under normal data",
    "text": "4.10 Exact tests under normal data\nThe Wald test above relies on large sample approximations. In finite samples, these approximation may not be valid. Is it possible to get exact inferences at any sample size? Yes, if we make stronger assumptions about the data. In particular, assume a parametric model for the data where \\(X_1,\\ldots,X_n\\) are i.i.d. samples from \\(N(\\mu,\\sigma^2)\\). Under null of \\(H_0: \\mu = \\mu_0\\), we can show that \\[\nT_n = \\frac{\\Xbar_n - \\mu_0}{s_n/\\sqrt{n}} \\sim t_{n-1},\n\\] where $t_{n-1} is the $Student’s t-distribution with \\(n-1\\) degrees of freedom. This implies the null distribution is \\(t\\) so we use quantiles of \\(t\\) for critical values. For one-sided test \\(c = G^{-1}_0(1 - \\alpha)\\) but now \\(G_0\\) is \\(t\\) with \\(n-1\\) df and so we use qt() instead of qnorm() to calculate these critical values.\nThe critical values for the \\(t\\) distribution are always larger than the normal because the t has fatter tails as shown in Figure 4.3. As \\(n\\to\\infty\\), however, the \\(t\\) converges to the standard normal and so it is asymptotically equivalent to the Wald test but slightly more conservative in finite samples. Oddly, most software packages calculate p-values and rejection regions based on the \\(t\\) to take advantage of this conservativeness.\n\n\n\n\n\nFigure 4.3: Normal versus t distribution\n\n\n\n\n\n\n\n\nSenn, Stephen. 2012. “Tea for Three: Of Infusions and Inferences and Milk in First.” Significance 9 (6): 30–33. https://doi.org/https://doi.org/10.1111/j.1740-9713.2012.00620.x."
  },
  {
    "objectID": "05_confidence_intervals.html#deriving-confidence-intervals",
    "href": "05_confidence_intervals.html#deriving-confidence-intervals",
    "title": "5  Confidence intervals",
    "section": "5.1 Deriving confidence intervals",
    "text": "5.1 Deriving confidence intervals\nIf you have taken any statistics before, you probably have seen the standard formula for the 95% confidence interval of the sample mean, \\[\n\\left[\\Xbar_n - 1.96\\frac{s}{\\sqrt{n}},\\; \\Xbar_n + 1.96\\frac{s}{\\sqrt{n}}\\right],\n\\] where you can recall that \\(s\\) is the sample standard deviation and \\(s/\\sqrt{n}\\) is the estimate of the standard error of the sample mean. If this is a 95% confidence interval, then the probability that it contains the population mean \\(\\mu\\) should be 0.95, but how can derive this? It turns out that this will be justified by the central limit theorem and we will show the logic for a generic asymptotically normal estimator.\nLet’s say that we have an estimator, \\(\\widehat{\\theta}_n\\) for the parameter \\(\\theta\\) with estimated standard error \\(\\widehat{\\se}[\\widehat{\\theta}_n]\\). If the estimator is asymptotically normal, then in large samples, we know that \\[\n\\frac{\\widehat{\\theta}_n - \\theta}{\\widehat{\\se}[\\widehat{\\theta}_n]} \\sim \\N(0, 1).\n\\] We can then use our knowledge of the standard normal and the empirical rule to find \\[\n\\P\\left( -1.96 \\leq \\frac{\\widehat{\\theta}_n - \\theta}{\\widehat{\\se}[\\widehat{\\theta}_n]} \\leq 1.96\\right) = 0.95\n\\] and by multiplying each part of the inequality by \\(\\widehat{\\se}[\\widehat{\\theta}_n]\\), we get \\[\n\\P\\left( -1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\leq \\widehat{\\theta}_n - \\theta \\leq 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n]\\right) = 0.95,\n\\] We then subtract all part by the estimator to get \\[\n\\P\\left(-\\widehat{\\theta}_n - 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\leq  - \\theta \\leq -\\widehat{\\theta}_n + 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n]\\right) = 0.95,\n\\] and finally we multiply all parts by \\(-1\\) (and flipping the inequalities) to arrive at \\[\n\\P\\left(\\widehat{\\theta}_n - 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\leq  \\theta \\leq \\widehat{\\theta}_n + 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n]\\right) = 0.95.\n\\] To connect back to the definition of the confidence interval, we have now shown that the random interval \\([L, U]\\) where \\[\n\\begin{aligned}\n  L = L(X_1, \\ldots, X_n) &= \\widehat{\\theta}_n - 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\\\\n  U = U(X_1, \\ldots, X_n) &= \\widehat{\\theta}_n + 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n],\n\\end{aligned}\n\\] is an asymptotically valid estimator.1 Replacing \\(\\Xbar_n\\) for \\(\\widehat{\\theta}_n\\) and \\(s/\\sqrt{n}\\) for \\(\\widehat{\\se}[\\widehat{\\theta}_n]\\) establishes how the standard 95% confidence interval for the sample mean above asymptotically valid.\n\n\n\n\n\nFigure 5.1: Critical values for the standard normal.\n\n\n\n\nHow can we generalize this to \\(1-\\alpha\\) confidence intervals? For a standard normal rv, \\(Z\\), we know that \\[\n\\P(-z_{\\alpha/2} \\leq Z \\leq z_{\\alpha/2}) = 1-\\alpha\n\\] which implies that we can obtain a \\(1-\\alpha\\) asymptotic confidence intervals by using the interval \\([L, U]\\), where \\[\nL = \\widehat{\\theta}_{n} - z_{\\alpha/2} \\widehat{\\se}[\\widehat{\\theta}_{n}], \\quad U = \\widehat{\\theta}_{n} + z_{\\alpha/2} \\widehat{\\se}[\\widehat{\\theta}_{n}].\n\\] This is sometimes shortened to \\(\\widehat{\\theta}_n \\pm z_{\\alpha/2} \\widehat{\\se}[\\widehat{\\theta}_{n}]\\). Remember that we can obtain the values of \\(z_{\\alpha/2}\\) easily from R:\n\n## alpha = 0.1 for 90% CI\nqnorm(0.1 / 2, lower.tail = FALSE)\n\n[1] 1.644854\n\n\nAs a concrete example, then, we could derive a 90% asymptotic confidence interval for the sample mean as \\[\n\\left[\\Xbar_{n} - 1.64 \\frac{\\widehat{\\sigma}}{\\sqrt{n}}, \\Xbar_{n} + 1.64 \\frac{\\widehat{\\sigma}}{\\sqrt{n}}\\right]\n\\]"
  },
  {
    "objectID": "05_confidence_intervals.html#interpreting-confidence-intervals",
    "href": "05_confidence_intervals.html#interpreting-confidence-intervals",
    "title": "5  Confidence intervals",
    "section": "5.2 Interpreting confidence intervals",
    "text": "5.2 Interpreting confidence intervals\nRemember that the interpretation of cofidence is how the random interval performs over repeated samples. A valid 95% confidence interval is a random interval that will contain the true value in 95% of samples. Simulating repeated samples can help to clarify this a bit.\n\nExample 5.1 Suppose that we are taking samples of size \\(n=500\\) of random variables where \\(X_i \\sim \\N(1, 10)\\), and we want to estimate the population mean \\(\\E[X] = 1\\). To do so, we repeat the following steps:\n\nDraw a sample of \\(n=500\\) from \\(\\N(1, 10)\\).\nCalculate the 95% confidence interval sample mean \\(\\Xbar_n \\pm 1.96\\widehat{\\sigma}/\\sqrt{n}\\).\nPlot the intervals along the x-axis and color them blue if they contain the truth (1) and red if they do not.\n\nFigure 5.2 shows 100 iteraction of these steps. Here we see that, as expected, the large majority of calculated CIs conatain the true value. In fact, exactly 5 of the random samples produce intervals that fail to include 1, which is an exact coverage rate of 95%. Of course, this is just one simulation and a different set of 100 random samples might have produced a slightly different coverage rate. The gauarantee of the 95% confidence intervals is that if we were to continue to take these repeated samples the long-run frequency of intervals covering the truth would approach 0.95.\n\n\n\n\n\nFigure 5.2: 95% confidence intervals from 100 random samples. Intervals are blue if they contain the truth and red if they do not."
  },
  {
    "objectID": "05_confidence_intervals.html#confidence-intervals-and-hypothesis-tests",
    "href": "05_confidence_intervals.html#confidence-intervals-and-hypothesis-tests",
    "title": "5  Confidence intervals",
    "section": "5.3 Confidence intervals and hypothesis tests",
    "text": "5.3 Confidence intervals and hypothesis tests\nAt first glance, we may seem sloppy in using \\(\\alpha\\) in the derivation of a \\(1 - \\alpha\\) confidence interval in this chapter and in an \\(\\alpha\\)-level test in the last chapter. In reality, we were simply foreshadowing the deep connection between the two: every \\(1-\\alpha\\) confidence interval contains all null hypotheses that we would not reject with an \\(\\alpha\\)-level test.\nThis connection is easiest to see with a asymptotically normal estimator, \\(\\widehat{\\theta}_n\\). Consider the hypothesis test of \\[\nH_0: \\theta = \\theta_0 \\quad \\text{vs.}\\quad H_1: \\theta \\neq \\theta_0,\n\\] using the test statistic, \\[\nT = \\frac{\\widehat{\\theta}_{n} - \\theta_{0}}{\\widehat{\\se}[\\widehat{\\theta}_{n}]}.\n\\] As we discussed in the last chapter, an \\(\\alpha = 0.05\\) test would reject this null when \\(|T| > 1.96\\), or when \\[\n|\\widehat{\\theta}_{n} - \\theta_{0}| > 1.96 \\widehat{\\se}[\\widehat{\\theta}_{n}].\n\\] Notice that will be true when \\[\n\\theta_{0} < \\widehat{\\theta}_{n} - 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}]\\quad \\text{ or  }\\quad  \\widehat{\\theta}_{n} +  \\widehat{\\se}[\\widehat{\\theta}_{n}] < \\theta_{0}\n\\] or, equivalently, that null hypothesis is outside of the 95% confidence interval, \\[\\theta_0 \\notin \\left[\\widehat{\\theta}_{n} - 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}], \\widehat{\\theta}_{n} + 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}]\\right].\\] Of course, our choice of the null hypothesis was arbitrary, which means that any null hypothesis that is outside the 95% confidence interval would be rejected by a \\(\\alpha = 0.05\\) level test of that null. And any null hypothesis inside the confidence interval is a null hypothesis that we would not reject.\nThis relationship holds more broadly. Any \\(1-\\alpha\\) confidence interval contains all possible parameter values that would not be rejected as the null hypothesis of an \\(\\alpha\\)-level hypothesis test. This can be really useful for two reasons:\n\nWe can quickly determine if a null hypothesis is rejected at some level by inspecting if it falls in a confidence interval.\nThere are situations where determining a confidence interval might be difficult, but performing a hypothesis test is straightforward. Then, we can find the rejection region for the test and determine what null hypotheses would not be rejected at level \\(\\alpha\\) to formulate the \\(1-\\alpha\\) confidence interval. This process is called inverting a test. One important application of this method is for formulating confidence intervals for treatment effects based on randomization inference in the finite population analysis of experiments."
  },
  {
    "objectID": "06_linear_model.html#why-do-we-need-models",
    "href": "06_linear_model.html#why-do-we-need-models",
    "title": "6  Linear regression",
    "section": "6.1 Why do we need models?",
    "text": "6.1 Why do we need models?\nAt first glance, the connection between the CEF and parametric models might be hazy. For example, imagine we are interested in estimating the average poll wait times (\\(Y_i\\)) for Black voters (\\(X_i = 1\\)) versus non-Black voters (\\(X_i=0\\)). In that case, there are two parameters to estimate, \\[\n\\mu(1) = \\E[Y_i \\mid X_i = 1] \\quad \\text{and}\\quad \\mu(0) = \\E[Y_i \\mid X_i = 0],\n\\] which we could estimate by using the plug-in estimators that replace the population averages with their sample counterparts, \\[\n\\widehat{\\mu}(1) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 1)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 1)} \\qquad \\widehat{\\mu}(0) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 0)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 0)}.\n\\] These are just the sample averages of the wait times for Black and non-Black voters, respectively. And because the race variable here is discrete, this basically mimics the situation of estimating a single mean, just within subpopulations defined by race in this case. The same logic would apply if we had \\(k\\) racial categories: we would have \\(k\\) conditional expectations to estimate and \\(k\\) (conditional) sample means.\nNow imagine that we want to know how the average poll wait time varies as a function of income, so that \\(X_i\\) is (essentially) continuous. Now we have a different conditional expectation for every possible dollar amount from 0 to Bill Gates’s income. Imagine we pick particular income, $42,238, and so we are interested in the conditional expectation \\(\\mu(42,238)= \\E[Y_{i}\\mid X_{i} = 42,238]\\). We could use the same plug-in estimator in the discrete case, \\[\n\\widehat{\\mu}(42,238) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 42,238)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 42,238)}.\n\\] What is the problem with this estimator? In all likelihood there are 0 units in any particular dataset that have that exact income, meaning this estimator is undefined (we would be dividing by zero).\nOne solution to this problem is to use subclassification and turn the continuous variable into a discrete one and proceed with the discrete approach above. We might group incomes into $25,000 bins and then calculate the average wait times of anyone between, say, $25,000 and $50,000 income. When we make this estimator switch for pragmatic purposes, we need to connect it back to the DGP of interest somehow. We could assume that the CEF of interest only depends on these binned means, which would mean we have:\n\\[\n\\mu(x) =\n\\begin{cases}\n  \\E[Y_{i} \\mid 0 \\leq X_{i} < 25,000]  &\\text{if }  0 \\leq x < 25,000 \\\\\n  \\E[Y_{i} \\mid 25,000 \\leq X_{i} < 50,000]  &\\text{if }  25,000 \\leq x < 50,000\\\\\n  \\E[Y_{i} \\mid 50,000 \\leq X_{i} < 100,000]  &\\text{if }  50,000 \\leq x < 100,000\\\\\n  \\vdots \\\\\n  \\E[Y_{i} \\mid 200,000 \\leq X_{i}]  &\\text{if }  200,000 \\leq x\\\\\n\\end{cases}\n\\] This assumes, perhaps incorrectly, that the average wait time does not vary within the bins. Figure 6.1 shows a hypothetical joint distribution between income and wait times with the true CEF, \\(\\mu(x)\\) shown in red. The figure also shows the bins created by subclassification and the implied CEF if we assume bin-constant means in blue. We can see that blue function approximates the true CEF but deviates from it especially close to the bin edges. The trade off is that once we make the assumption, we only have to estimate one mean for every bin, rather than an infinite number of means for each possible income.\n\n\n\n\n\nFigure 6.1: Hypothetical joint distribution of income and poll wait times (contour plot), conditional expectation function (red), and the conditional expectation of the binned income (blue).\n\n\n\n\nSimilarly, we could assume that the CEF follows a simple functional form like a line, \\[\n\\mu(x) = \\E[Y_{i}\\mid X_{i} = x] = \\beta_{0} + \\beta_{1} x.\n\\] This reduces our infinite number of unknowns (the conditional mean at every possible income) to just two unknowns, the slope and intercept. As we will see, we can use the standard ordinary least squares to estimate these parameters. Notice again, though, that if the true CEF is nonlinear this assumption is incorrect, any estimate based off this assumption might be biased or even inconsistent.\nWe call the binning and linear assumptions on \\(\\mu(x)\\) functional form assumptions because restrict the class of functions that \\(\\mu(x)\\) can take. While powerful, these types of assumptions can muddy the roles of defining the quantity of interest and estimation. If our estimator \\(\\widehat{\\mu}(x)\\) performs poorly, it will be difficult to tell if this is because the estimator is flawed in some way or because our functional form assumptions are incorrect.\nTo help clarify these issues, we will pursue a different approach: understanding what linear regression can estimate well under minimal assumptions and then investigating how well this estimand approximates the true CEF."
  },
  {
    "objectID": "06_linear_model.html#population-linear-regression",
    "href": "06_linear_model.html#population-linear-regression",
    "title": "6  Linear regression",
    "section": "6.2 Population linear regression",
    "text": "6.2 Population linear regression\n\n6.2.1 Bivariate linear regression\nLet’s set aside the idea of the conditional expectation function and instead focus on finding the linear function of a single covariate \\(X_i\\) that best predicts the outcome. Remember that a linear function can be written \\(a + bX_i\\). The best linear predictor (BLP) or population linear regression of \\(Y_i\\) on \\(X_i\\) is defined as \\[\nm(x) = \\beta_0 + \\beta_1 x \\quad\\text{where, }\\quad (\\beta_{0}, \\beta_{1}) = \\argmin_{(b_{0}, b_{1}) \\in \\mathbb{R}^{2}}\\; \\E[(Y_{i} - b_{0} + b_{1}X_{i} )^{2}].\n\\] That is, the best linear predictor is the line that results in the lowest mean-squared error predictions of the outcome given the covariates, averaging over the joint distribution of the data. This function is a feature of the joint distribution of the data—the DGP—and so represents something that we would like to learn about with our sample. It is an alternative to the CEF for summarizing the relationship between the outcome and the covariate, though we will see that they will sometimes be equal to each other. We call \\((\\beta_{0}, \\beta_{1})\\) the population linear regression coefficients. Notice that \\(m(x)\\) here could be very different from the CEF \\(\\mu(x)\\) if the latter is nonlinear.\nWe can solve for the best linear predictor using standard calculus (taking the derivative with respect to each coefficient, setting those equations equal to 0, and solving the system of equations). The first-order conditions in this case are \\[\n\\begin{aligned}\n  \\frac{\\partial \\E[(Y_{i} - b_{0} + b_{1}X_{i} )^{2}]}{\\partial b_{0}} = \\E[2(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})] = 0 \\\\\n  \\frac{\\partial \\E[(Y_{i} - b_{0} + b_{1}X_{i} )^{2}]}{\\partial b_{1}} = \\E[2(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})X_{i}] = 0\n\\end{aligned}  \n\\] Given the linearity of expectations, it is easy to solve for \\(\\beta_0\\) in terms of \\(\\beta_1\\), \\[\n\\beta_{0} = \\E[Y_{i}] - \\beta_{1}\\E[X_{i}].\n\\] We can plug this into the first-order condition for \\(\\beta_1\\) to get \\[\n\\begin{aligned}\n  0 &= \\E[Y_{i}X_{i}] - (\\E[Y_{i}] - \\beta_{1}\\E[X_{i}])\\E[X_{i}] - \\beta_{1}\\E[X_{i}^{2}] \\\\\n    &= \\E[Y_{i}X_{i}] - \\E[Y_{i}]\\E[X_{i}] - \\beta_{1}(\\E[X_{i}^{2}] - \\E[X_{i}]^{2})  \\\\\n    &= \\cov(X_{i},Y_{i}) - \\beta_{1}\\V[X_{i}]\\\\\n  \\beta_{1} &= \\frac{\\cov(X_{i},Y_{i})}{\\V[X_{i}]}\n\\end{aligned}\n\\]\nThus the slope on the population linear regression of \\(Y_i\\) on \\(X_i\\) is equal to the ratio of the covariance of the two variables divided by the variance of \\(X_i\\). From this, we can immediately see that the sign of the slope will be determined by the covariance: positive covariances will lead to positive \\(\\beta_1\\) and negative covariances will lead to negative \\(\\beta_1\\). In addition, we can see if \\(Y_i\\) and \\(X_i\\) are independent, then \\(\\beta_1 = 0\\). The slope scaled this covariance by the variance of the covariate, so slopes are lower for more spread out covariates and higher for more spread out covariates. If we define the correlation between these variables as \\(\\rho_{YX}\\), then we can relate the coefficient to this quantity as \\[\n\\beta_1 = \\rho_{YX}\\sqrt{\\frac{\\V[Y_i]}{\\V[X_i]}}.\n\\]\nCollecting together our results, we can write the population linear regression as \\[\nm(x) = \\beta_0 + \\beta_1x = \\E[Y_i] + \\beta_1(x - \\E[X_i]),\n\\] which shows how we adjust our best guess about \\(Y_i\\) from the mean of the outcome using the covariate.\nIt’s important to remember that the BLP, \\(m(x)\\), and the CEF, \\(\\mu(x)\\), are distinct entities. If the CEF is nonlinear, as in Figure 6.2, there will be difference between these functions and mean that the BLP might produce subpar predictions. Below, we will derive a formal connection between the BLP and the CEF.\n\n\n\n\n\nFigure 6.2: Comparison of the CEF and the best linear predictor\n\n\n\n\n\n\n6.2.2 Linear prediction with multiple covariates\nWe now generalize the idea of a best linear predictor to a setting with multiple covariates. In this setting, remember that a linear function can be written as\n\\[\n\\bfx'\\bfbeta = x_{1}\\beta_{1} + x_{2}\\beta_{2} + \\cdots + X_{k}\\beta_{k}.\n\\] We will define the best linear predictor (BLP) to be \\[\nm(\\bfx) = \\bfx'\\bfbeta, \\quad \\text{where}\\quad \\bfbeta = \\argmin_{\\mb{b} \\in \\real^k}\\; \\E\\bigl[ \\bigl(Y_{i} - \\mb{X}_{i}'\\bfbeta \\bigr)^2\\bigr]\n\\]\nThis BLP solves the same basic optimization problem as in the bivariate case: it chooses the set of coefficients that minimizes the mean-squared error averaging over the joint distribution of the data.\n\n\n\n\n\n\n\nBest linear projection assumptions\n\n\n\nWithout some assumptions on the joint distribution of the data, The following “regularity conditions” will ensure the existence of the BLP:\n\n\\(\\E[Y^2] < \\infty\\) (outcome has finite mean/variance)\n\\(\\E\\Vert \\mb{X} \\Vert^2 < \\infty\\) (\\(\\mb{X}\\) has finite means/variances/covariances)\n\\(\\mb{Q}_{\\mb{XX}} = \\E[\\mb{XX}']\\) is positive definite (columns of \\(\\X\\) are linearly independent)\n\n\n\n\nUnder these assumption, it is possible to derive a closed-form expression for the population coefficients \\(\\bfbeta\\) using matrix calculus. To set up the optimization problem, we will find the first order condition my taking the derivative of the expectation of the squared errors. First, let’s take derivative of the squared prediction errors using the chain rule: \\[\n\\begin{aligned}\n  \\frac{\\partial}{\\partial \\mb{b}^{'}}\\left(Y_{i} - \\X_{i}'\\mb{b}\\right)^{2}\n  &= 2\\left(Y_{i} - \\X_{i}'\\mb{b}\\right)\\frac{\\partial}{\\partial \\mb{b}'}(Y_{i} - \\X_{i}'\\mb{b})  \\\\\n  &= -2\\left(Y_{i} - \\X_{i}'\\mb{b}\\right)\\X_{i} \\\\\n  &= -2\\left(\\X_{i}Y_{i} - \\X_{i}\\X_{i}'\\mb{b}\\right)\n\\end{aligned}\n\\] We can now plug this into the expectation to get the first-order condition and solve for \\(\\bfbeta\\), \\[\n\\begin{aligned}\n  0 &= -2\\E[\\X_{i}Y_{i} - \\X_{i}\\X_{i}'\\bfbeta ]  \\\\\n  \\E[\\X_{i}\\X_{i}'] \\bfbeta &= \\E[\\X_{i}Y_{i}],\n\\end{aligned}\n\\] which implies the population coefficients are \\[\n\\bfbeta = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}] = \\mb{Q}_{\\mb{XX}}^{-1}\\mb{Q}_{\\mb{X}Y}\n\\] We now have an expression for the coefficients for the population best linear predictor in terms of the joint distribution \\((Y_{i}, \\X_{i})\\). There are a couple facts that might be useful for reasoning about this expression. Recall that \\(\\mb{Q}_{\\mb{XX}} = \\E[\\X_{i}\\X_{i}']\\) is a \\(k\\times k\\) matrix and \\(\\mb{Q}_{\\X Y} = \\E[\\X_{i}Y_{i}]\\) is a \\(k\\times 1\\) column vector, which implies that \\(\\bfbeta\\) is also a \\(k \\times 1\\) column vector.\n\n\n\n\n\n\nNote\n\n\n\nIntuitively, what is happening in the expression for the population regression coefficients? It is helpful to separate out the intercept or constant term so that we have \\[\nY_{i} = \\beta_{0} + \\X'\\bfbeta + e_{i},\n\\] so \\(\\bfbeta\\) refers to just the coefficients on each of the covariates. In this case, we can write the coefficients in more interpretable way: \\[\n\\bfbeta = \\V[\\X]^{-1}\\text{Cov}(\\X, Y), \\qquad \\beta_0 = \\mu_Y - \\mb{\\mu}'_{\\mb{X}}\\bfbeta\n\\]\nThus, the population coefficients take the covariance between the outcome and the covariates and “divide” it by information about variances and covariances of the covariates. The intercept recenters the regression so that projection errors are mean zero. Thus, we can see that the coeffiicients generalizes the bivariate formula to this multiple covariate context.\n\n\nWith an expression for the population linear regression coefficients, we can write the linear projection as \\[\nm(\\X_{i}) = \\X_{i}'\\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}] = \\X_{i}'\\mb{Q}_{\\mb{XX}}^{-1}\\mb{Q}_{\\mb{X}Y}\n\\]\n\n\n6.2.3 Projection error\nThe projection error is the difference between the actual value of \\(Y_i\\) and the projection, \\[\ne_{i} = Y_{i} - m(\\X_{i}) =  Y_i - \\X_{i}'\\bfbeta,\n\\] where it is hopefully clear that we have made no assumptions about this error yet. It is simply the prediction error if we used the linear projection to predict the outcome. Rewriting this definition, we can see that we can always write the outcome as the linear projection plus the projection error, \\[\nY_{i} = \\X_{i}'\\bfbeta + e_{i}.\n\\] Notice that this looks suspiciously like we have made a linearity assumption on the CEF or on the relationship between the outcome and the covariates. But we haven’t, we have just used the definition of the projection error to write a statement that is tautological: \\[\nY_{i} = \\X_{i}'\\bfbeta + e_{i} = \\X_{i}'\\bfbeta + Y_{i} - \\X_{i}'\\bfbeta = Y_{i}.\n\\] The key difference between this representation and the usual linear model assumption is what properties \\(e_{i}\\) possesses.\nOne key property of the projection errors is that when the covariate vector includes an “intercept” or constant term, the projection errors are uncorrelated with the covariates. To see this, we first note that \\(\\E[\\X_{i}e_{i}] = 0\\) since \\[\n\\begin{aligned}\n  \\E[\\X_{i}e_{i}] &= \\E[\\X_{{i}}(Y_{i} - \\X_{i}'\\bfbeta)] \\\\\n                  &= \\E[\\X_{i}Y_{i}] - \\E[\\X_{i}\\X_{i}']\\bfbeta \\\\\n                  &= \\E[\\X_{i}Y_{i}] - \\E[\\X_{i}\\X_{i}']\\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}] \\\\\n  &= \\E[\\X_{i}Y_{i}] - \\E[\\X_{i}Y_{i}] = 0\n\\end{aligned}\n\\] Thus, for every \\(X_{ij}\\) in \\(\\X_{i}\\), we have \\(\\E[X_{ij}e_{i}] =0\\). If one of the entries in \\(\\X_i\\) is a constant 1, then this also implies that \\(\\E[e_{i}] =0\\). Together, these facts imply that the projection error is uncorrelated with each \\(X_{ij}\\), since \\[\n\\cov(X_{ij}, e_{i}) = \\E[X_{ij}e_{i}] - \\E[X_{ij}]\\E[e_{i}] = 0 - 0 = 0\n\\] Notice that we still have made no assumptions about these projection errors except some mild regularity conditions on the joint distribution of the outcome and covariates. Thus, in very general settings, we can write the linear projection model \\(Y_i = \\X_i'\\bfbeta + e_i\\) where \\(\\bfbeta = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}]\\) and conclude that \\(\\E[\\X_{i}e_{i}] = 0\\) by definition not by assumption.\nThe projection error is uncorrelated with the covariates, so does this mean that the CEF is linear? Unfortunately, no because recall that while independence implies uncorrelated, the reverse does not hold. So when we look at the CEF, we have \\[\n\\E[Y_{i} \\mid \\X_{i}] = \\X_{i}'\\bfbeta + \\E[e_{i} \\mid \\X_{i}],\n\\] and the last term \\(\\E[e_{i} \\mid \\X_{i}]\\) would only be 0 if the errors were independent of the covariates so \\(\\E[e_{i} \\mid \\X_{i}] = \\E[e_{i}] = 0\\). But nowhere in the linear projection model did we assume this. So while we can (almost) always write the outcome as \\(Y_i = \\X_i'\\bfbeta + e_i\\) and have those projections error be uncorrelated with the covariates, it will require additional assumptions to ensure that the true CEF is in fact linear \\(\\E[Y_{i} \\mid \\X_{i}] = \\X_{i}'\\bfbeta\\).\nLet’s take a step back. What have we shown here? In a nutshell, we have shown that under very general conditions, a population linear regression exists and we can write the coefficients of that population linear regression as a function of expectations of the joint distribution of the data. We did not assume that the CEF was linear nor that the projection errors were normal.\nWhy do we care about this? It turns out that the ordinary least squares estimator, the workhorse regression estimator, targets this quantity of interest in large samples, regardless of whether the true CEF is linear or not. This means that even when a linear CEF assumption is incorrect, OLS still targets a perfectly valid quantity of interest: the coefficients from this population linear projection."
  },
  {
    "objectID": "06_linear_model.html#linear-cefs-without-assumptions",
    "href": "06_linear_model.html#linear-cefs-without-assumptions",
    "title": "6  Linear regression",
    "section": "6.3 Linear CEFs without assumptions",
    "text": "6.3 Linear CEFs without assumptions\nWhat is the relationship between the best linear predictor (which we just saw exists very generally) and the CEF? To draw the connection, remember a key property of the conditional expectation: it is the function of \\(\\X_i\\) that best predicts \\(Y_{i}\\). The population regression was the best linear predictor, but the CEF is the best predictor among all functions of \\(\\X_{i}\\), linear or nonlinear, that are nicely behaved. In particular, if we label \\(L_2\\) be the set of all functions of the covariates \\(g()\\) such that they are finite squared expectation, \\(\\E[g(\\X_{i})^{2}] < \\infty\\), then we can show that the CEF has the lowest squared prediction error in this class of functions: \\[\n\\mu(\\X) = \\E[Y_{i} \\mid \\X_{i}] = \\argmin_{g(\\X_i) \\in L_2}\\; \\E\\left[(Y_{i} - f(\\X_{i}))^{2}\\right],\n\\]\nSo we have established that the CEF is the best predictor and the population linear regression \\(m(\\X_{i})\\) is the best linear predictor. These two facts allow us to connect the CEF and the population regression.\n\nTheorem 6.1 If \\(\\mu(\\X_{i})\\) is a linear function of \\(\\X_i\\), then \\(\\mu(\\X_{i}) = m(\\X_{i}) = \\X_i'\\bfbeta\\).\n\nThis theorem says that if the true CEF is linear, then it is equal to the population regression. The proof of this is straightforward: the CEF is the best predictor, so if it is linear, it must also be the best linear predictor.\nIn general, we are usually in the business of learning about the CEF so we are unlikely to know if it truly is linear or not. In some situations, however, we can show that the CEF is linear without any additional assumptions. These will be situations when the covariates take on a finite number of possible values. Suppose that we are interested in the CEF of poll wait times for Black (\\(X_i = 1\\)) vs non-Black (\\(X_i = 0\\)) voters. In this case, there are two possible values of the CEF, \\(\\mu(1) = \\E[Y_{i}\\mid X_{i}= 1]\\), the average wait time for Black voters, and \\(\\mu(0) = \\E[Y_{i}\\mid X_{i} = 0]\\), the average wait time for non-Black voters. Notice that we can write the CEF as \\[\n\\mu(x) = x \\mu(1) + (1 - x) \\mu(0) = \\mu(0) + x\\left(\\mu(1) - \\mu(0)\\right)= \\beta_0 + x\\beta_1,\n\\] which is clearly a linear function of \\(x\\). Based on this derivation, we can see that the coefficients of this linear CEF have a clear interpretation:\n\n\\(\\beta_0 = \\mu(0)\\): the expected wait time for a Black voter.\n\\(\\beta_1 = \\mu(1) - \\mu(0)\\): the difference in average wait times between Black and non-Black voters. Notice that it matters how \\(X_{i}\\) is defined here since the intercept will always be the average outcome when \\(X_i = 0\\) and the slope will always be the difference in means between the \\(X_i = 1\\) group and the \\(X_i = 0\\) group.\n\nWhat about a categorical coviarate with more than two levels? For instance, we might be interested in wait times by party identification, where \\(X_i = 1\\) indicates Democratic voters, \\(X_i = 2\\) indicates Republican voters, and \\(X_i = 3\\) indicates independent voters. How can we possible write the CEF of wait times as a linear function of this variable? That would assume that the difference between Democrats and Republicans is the same as for Independents and Republicans. With more than two levels, a categorical variable is better represented as a vector of binary variables, \\(\\X_i = (X_{i1}, X_{i2})\\), where \\[\n\\begin{aligned}\n  X_{{i1}} &= \\begin{cases}\n                1&\\text{if Republican} \\\\\n                   0 & \\text{if not Republican}\n              \\end{cases} \\\\\nX_{{i2}} &= \\begin{cases}\n                1&\\text{if independent} \\\\\n                   0 & \\text{if not independent}\n              \\end{cases} \\\\\n\\end{aligned}\n\\] Clearly, these two indicator variables encode the same information as the original three-level variable, \\(X_{i}\\). If I know the values of \\(X_{i1}\\) and \\(X_{i2}\\), I know exactly what party to which \\(i\\) belongs. Thus, the CEFs with repect to \\(X_i\\) and the pair of indicator variables, \\(\\X_i\\), is exactly the same, but the latter admits a very nice linear representation, \\[\n\\E[Y_i \\mid X_{i1}, X_{i2}] = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2},\n\\] where\n\n\\(\\beta_0 = \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 0]\\) is the average wait time for the group who does not get an indicator variable (Democrats in this case).\n\\(\\beta_1 = \\E[Y_{i} \\mid X_{i1} = 1, X_{i2} = 0] - \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 0]\\) is the difference in means between Republican voters and Democratic voters, or the difference between the first indicator group and the baseline group.\n\\(\\beta_2 = \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 1] - \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 0]\\) is the difference in means between independent voters and Democratic voters, or the difference between the second indicator group and the baseline group.\n\nThis approach generalizes easily to categorical variables with an arbitrary number of levels.\nWhat have we shown? The CEF will be linear without additional assumptions when there is a categorical covariate. We can show that this continues to hold even when we have multiple categorical variables. Suppose now that we have two binary covariates: \\(X_{i1}=1\\) indicating a Black voter and \\(X_{i2} = 1\\) indicating an urban voter. With these two binary variables, there are 4 possible values of the CEF: \\[\n\\mu(x_1, x_2) = \\begin{cases}\n\\mu_{00} & \\text{if } x_1 = 0 \\text{ and } x_2 = 0 \\text{ (non-Black, rural)} \\\\\n  \\mu_{10} & \\text{if }  x_1 = 1 \\text{ and } x_2 = 0 \\text{ (Black, rural)}\\\\\n  \\mu_{01} & \\text{if }  x_1 = 0 \\text{ and } x_2 = 1 \\text{ (non-Black, urban)}\\\\\n\\mu_{11} & \\text{if }  x_1 = 1 \\text{ and } x_2 = 1 \\text{ (Black, urban)}\n\\end{cases}\n\\] We can write this as \\[\n\\mu(x_{1}, x_{2}) = (1 - x_{1})(1 - x_{2})\\mu_{00} + x_{1}(1 -x_{2})\\mu_{10} + (1-x_{1})x_{2}\\mu_{01} + x_{1}x_{2}\\mu_{11},\n\\] which we can rewrite as \\[\n\\mu(x_1, x_2) = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3,\n\\] where\n\n\\(\\beta_0 = \\mu_{00}\\): average wait times for rural non-Black voters.\n\\(\\beta_1 = \\mu_{10} - \\mu_{00}\\): difference in means for rural Black vs rural non-Black voters..\n\\(\\beta_2 = \\mu_{01} - \\mu_{00}\\): difference in means for urban non-Black vs rural non-Black voters.\n\\(\\beta_3 = (\\mu_{11} - \\mu_{01}) - (\\mu_{10} - \\mu_{00})\\): difference in urban racial difference vs rural racial difference.\n\nThus, we can write the CEF with two binary covriataes as linear when the linear specification includes and multiplicative interaction between them (\\(x_1x_2\\)). This holds for all pairs of binary covariates and we can generalize the interpretation of the coefficients in the CEF as\n\n\\(\\beta_0 = \\mu_{00}\\): average outcome when both variables are 0.\n\\(\\beta_1 = \\mu_{10} - \\mu_{00}\\): difference in average outcomes for the first covariate when second covariate is 0.\n\\(\\beta_2 = \\mu_{01} - \\mu_{00}\\): difference in average outcomes for the second covariate when the first covariate is 0.\n\\(\\beta_3 = (\\mu_{11} - \\mu_{01}) - (\\mu_{10} - \\mu_{00})\\): change in the “effect” of the first (second) covariate when the second (first) covariate goes from 0 to 1.\n\nThis result also generalizes to an arbitrary number of binary covariates. If we have \\(p\\) binary covariates, then the CEF will be linear with all two-way interactions, \\(x_1x_2\\), all three-way interactions, \\(x_1x_2x_3\\), up to the \\(p\\)-way interaction \\(x_1\\times\\cdots\\times x_p\\). Furthermore, we can generalize to arbitrary numbers of categorical variables by expanding each categorical variable into a series of binary variables and then including all interactions between the resulting binary variables.\nWe have established that when we have a set of categorical covariates, the true CEF will be linear and we have seen the various ways to represent that CEF. Notice that when we actually use, for example, ordinary least squares, we are free to choose how to include our variables. That means that we could run a regression of \\(Y_i\\) on \\(X_{i1}\\) and \\(X_{i2}\\) without an interaction term. This model will only be correct if \\(\\beta_3\\) is actually equal to 0 and so the interaction term is irrelevant. Because of this ability to choose our models, it’s helpful to have a language for models that capture the linear CEF appropriately. We call a model saturated if there as many coefficients as there are unique values of the CEF. A saturated model by its nature can always be written as a linear function without assumptions. The above examples show how to construct saturated models in various situations."
  },
  {
    "objectID": "06_linear_model.html#interpretation-of-the-regression-coefficients",
    "href": "06_linear_model.html#interpretation-of-the-regression-coefficients",
    "title": "6  Linear regression",
    "section": "6.4 Interpretation of the regression coefficients",
    "text": "6.4 Interpretation of the regression coefficients\nWe have seen how to interpret population regression coefficients in situations where the CEF is linear without assumptions. How do we interpret the population coefficients \\(\\bfbeta\\) in other settings?\nLet’s start with simplest case, where every entry in \\(\\X_{i}\\) represents a different covariate and no covariate is any function of another (we’ll see why this caveat is important below). In this simple case, the \\(k\\)th coefficient, \\(\\beta_{k}\\) will represent the change in the predicted outcome for a one-unit change in the \\(k\\)th covariate \\(X_{ik}\\), holding all other covariates fixed. We can see this from \\[\n\\begin{aligned}\n  m(x_{1} + 1, x_{2}) & =  \\beta_{0} + \\beta_{1}(x_{1} + 1) + \\beta_{2}x_{2} \\\\\n  m(x_{1}, x_{2}) &= \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2},\n\\end{aligned}\n\\] so that the change in the predicted outcome for increasing \\(X_{i1}\\) by one unit isn’t \\[\nm(x_{1} + 1, x_{2}) -  m(x_{1}, x_{2}) = \\beta_1\n\\] Notice that nothing changes in this interpretation if we add more covariates to the vector, \\[\nm(x_{1} + 1, \\bfx_{2}) -  m(x_{1}, \\bfx_{2}) = \\beta_1,\n\\] the coefficient on a particular variable is the change in predicted outcome for a one-unit change in the covariate holding all other covariates constant. Each coefficient summarizes the “all else equal” change in predicted outcome for each covariate."
  },
  {
    "objectID": "06_linear_model.html#multiple-regression-from-bivariate-regression",
    "href": "06_linear_model.html#multiple-regression-from-bivariate-regression",
    "title": "6  Linear regression",
    "section": "6.5 Multiple regression from bivariate regression",
    "text": "6.5 Multiple regression from bivariate regression\nWhen we have a regression of an outcome on two covariates, it is useful to understand how the coefficients of one variables related to the other. For example, if we have the following best linear projection: \\[\n(\\alpha, \\beta, \\gamma) = \\argmin_{(a,b,c) \\in \\mathbb{R}^{3}} \\; \\E[(Y_{i} - (a + bX_{i} + cZ_{i}))^{2}]\n\\tag{6.1}\\] Is there some way to understand the \\(\\beta\\) coefficient here in terms of simple linear regression? As it turns out, yes. From the above results, we know that the intercept has a simple form \\[\n\\alpha = \\E[Y_i] - \\beta\\E[X_i] - \\gamma\\E[Z_i].\n\\] Let’s investigate the first order condition for \\(\\beta\\): \\[\n\\begin{aligned}\n  0 &= \\E[Y_{i}X_{i}] - \\alpha\\E[X_{i}] - \\beta\\E[X_{i}^{2}] - \\gamma\\E[X_{i}Z_{i}] \\\\\n    &=  \\E[Y_{i}X_{i}] - \\E[Y_{i}]\\E[X_{i}] + \\beta\\E[X_{i}]^{2} + \\gamma\\E[X_{i}]\\E[Z_{i}] - \\beta\\E[X_{i}^{2}] - \\gamma\\E[X_{i}Z_{i}] \\\\\n  &= \\cov(Y, X) - \\beta\\V[X_{i}] - \\gamma \\cov(X_{i}, Z_{i})\n\\end{aligned}\n\\] We can see from this that if \\(\\cov(X_{i}, Z_{i}) = 0\\), then the coefficient on \\(X_i\\) will be the same as in the simple regression case, \\(\\cov(Y_{i}, X_{i})/\\V[X_{i}]\\). In this case, when \\(X_i\\) and \\(Z_i\\) are uncorrelated, we sometimes call them orthogonal.\nTo write a simple formula for \\(\\beta\\) when the covariates are not orthogonal, we will orthogonalize \\(X_i\\) by obtaining the prediction errors from a population linear regression of \\(X_i\\) on \\(Z_i\\): \\[\n\\widetilde{X}_{i} = X_{i} - (\\delta_{0} + \\delta_{1}Z_{i}) \\quad\\text{where}\\quad (\\delta_{0}, \\delta_{1}) = \\argmin_{(d_{0},d_{1}) \\in \\mathbb{R}^{2}} \\; \\E[(X_{i} - (d_{0}  + d_{1}Z_{i}))^{2}]\n\\] Given the properties of projection errors, we know that this orthogonalized version of \\(X_{i}\\) will be uncorrelated with \\(Z_{i}\\) since \\(\\E[\\widetilde{X}_{i}Z_{i}] = 0\\). Remarkably, the coefficient on \\(X_i\\) from the “long” BLP in Equation 6.1 is the same as the regression of \\(Y_i\\) on this orthogonalized \\(\\widetilde{X}_i\\), \\[\n\\beta = \\frac{\\text{cov}(Y_{i}, \\widetilde{X}_{i})}{\\V[\\widetilde{X}_{i}]}\n\\]\nWe can expand this to idea to when there are several other covariates. Suppose now that we are interested in a regression of \\(Y_i\\) on \\(\\X_i\\) and we are interested in the coefficient on the \\(k\\)th covariate. Let \\(\\X_{i,-k}\\) be the vector of covariates omitting the \\(k\\)th entry and let \\(m_k(\\X_{i,-k})\\) represent the BLP of \\(X_{ik}\\) on these other covariates. We can define \\(\\widetilde{X}_{ik} = X_{ik} - m_{k}(\\X_{i,-k})\\) as the \\(k\\) th variable othorgonalized with respect to rest of the variables and we can write the coefficient on \\(X_{ik}\\) as \\[\n\\beta_k = \\frac{\\cov(Y_i, \\widetilde{X}_{ik})}{\\V[\\widetilde{X}_{ik}]}.\n\\] Thus, the population regression coefficient in the BLP is the same as from a bivariate regression of the outcome on the projection error for \\(X_{ik}\\) being projected on all other covariates. One interpretation of coefficients in a population multiple regression is they represent the relationship between the outcome and the covariate, after removing the linear relationships of all other variables."
  },
  {
    "objectID": "06_linear_model.html#omitted-variable-bias",
    "href": "06_linear_model.html#omitted-variable-bias",
    "title": "6  Linear regression",
    "section": "6.6 Omitted variable bias",
    "text": "6.6 Omitted variable bias\nIn many situations, we might need to choose to include a variable in a regression or not, so it can be helpful to understand how this choice might affect the population coefficients on the other variables in a the regression. In particular, suppose we are considering whether to include some variable \\(Z_i\\) in our regression where \\(\\X_i\\) represents the other covariates. Then we are deciding between the two projections \\[\nm(\\X_i, Z_i) = \\X_i'\\bfbeta + Z_i\\gamma, \\qquad m(\\X_{i}) = \\X_i'\\bs{\\delta},\n\\] where we often refer to \\(m(\\X_i, Z_i)\\) as the long regression and \\(m(\\X_i)\\) as the short regression.\nWe know from the defintion of the BLP that we can write the short coefficients as \\[\n\\bs{\\delta} = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1} \\E[\\X_{i}Y_{i}].\n\\] Letting \\(e_i = Y_i - m(\\X_{i}, Z_{i})\\) be the projection errors from the long regression, we can write this as \\[\n\\begin{aligned}\n  \\bs{\\delta} &= \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1} \\E[\\X_{i}(\\X_{i}'\\bfbeta + Z_{i}\\gamma + e_{i})] \\\\\n              &= \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}(\\E[\\X_{i}\\X_{i}']\\bfbeta + \\E[\\X_{i}Z_{i}]\\gamma + \\E[\\X_{i}e_{i}]) \\\\\n  &=    \\bfbeta + \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Z_{i}]\\gamma\n\\end{aligned}\n\\] Notice that the vector in the second term is actually the linear projection coefficients of a population linear regression of \\(Z_i\\) on the \\(\\X_i\\). If we call these coefficients \\(\\bs{\\pi}\\), then th short coefficients are \\[\n\\bs{\\delta} = \\bfbeta + \\bs{\\pi}\\gamma.\n\\]\nWe can easily rewrite this to show that the difference between the coefficients in these two projections is \\(\\bs{\\delta} - \\bfbeta= \\bs{\\pi}\\gamma\\) or the product of the coefficient on the “excluded” \\(Z_i\\) and the coefficient of the included \\(\\X_i\\) on the excluded. This difference is usually referred to as the omitted variable bias of omitting \\(Z_i\\) under the idea that \\(\\bfbeta\\) is the true target of inference. But the result is much broader than this since it just tells us how to relate the coefficients of two nested projection.\nThe last two results (multiple regressions from bivariate and omitted variable bias) are sometimes presented as results for the ordinary least squares estimator that we will present in the next chapter. We introduce them here as features of a particular population quantity, the linear projection or population linear regression."
  },
  {
    "objectID": "06_linear_model.html#drawbacks-of-the-blp",
    "href": "06_linear_model.html#drawbacks-of-the-blp",
    "title": "6  Linear regression",
    "section": "6.7 Drawbacks of the BLP",
    "text": "6.7 Drawbacks of the BLP\nThe best linear predictor is, of course, a linear approximation to the CEF and this approximation could be quite bad if the true CEF is highly nonlinear. A more subtle issue with the BLP is that it is sensitive to the marginal distribution of the covariates when the CEF is nonlinear. Let’s return to our example of voter wait times and income. In Figure 6.3, we show the true CEF and the BLP when income is restricted below $50,000 or above $100,000. We can see that the BLP can vary quite dramatically here. This is an extremely example, but the basic point will still hold when the marginal distribution of \\(X_i\\) is weighted toward one end of the spectrum versus the other.\n\n\n\n\n\nFigure 6.3: Linear projections for when income is truncated below $50k and above $100k"
  },
  {
    "objectID": "07_least_squares.html#deriving-the-ols-estimator",
    "href": "07_least_squares.html#deriving-the-ols-estimator",
    "title": "7  The mechanics of least squares",
    "section": "7.1 Deriving the OLS estimator",
    "text": "7.1 Deriving the OLS estimator\nIn the last chapter on the linear model and the best linear projection, we operated purely in the population, not samples. We derived the population regression coefficients \\(\\bfbeta\\), which represented the coefficients on the line of best fit in the population. We now take these as our quantity of interest.\n\n\n\n\n\n\nAssumption\n\n\n\nThe variables \\(\\{(Y_1, \\X_1), \\ldots, (Y_i,\\X_i), \\ldots, (Y_n, \\X_n)\\}\\) are i.i.d. draws from a common distribution \\(F\\)."
  },
  {
    "objectID": "07_least_squares.html#bivariate-ols",
    "href": "07_least_squares.html#bivariate-ols",
    "title": "7  The mechanics of least squares",
    "section": "7.2 Bivariate OLS",
    "text": "7.2 Bivariate OLS"
  },
  {
    "objectID": "07_least_squares.html#model-fit",
    "href": "07_least_squares.html#model-fit",
    "title": "7  The mechanics of least squares",
    "section": "7.3 Model fit",
    "text": "7.3 Model fit"
  },
  {
    "objectID": "07_least_squares.html#matrix-form-of-ols",
    "href": "07_least_squares.html#matrix-form-of-ols",
    "title": "7  The mechanics of least squares",
    "section": "7.4 Matrix form of OLS",
    "text": "7.4 Matrix form of OLS"
  },
  {
    "objectID": "07_least_squares.html#projection",
    "href": "07_least_squares.html#projection",
    "title": "7  The mechanics of least squares",
    "section": "7.5 Projection",
    "text": "7.5 Projection"
  },
  {
    "objectID": "07_least_squares.html#outliers-leverage-points-and-influential-observations",
    "href": "07_least_squares.html#outliers-leverage-points-and-influential-observations",
    "title": "7  The mechanics of least squares",
    "section": "7.6 Outliers, leverage points, and influential observations",
    "text": "7.6 Outliers, leverage points, and influential observations"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "$$\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\bfbeta}{\\bs{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\bs{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\n\nSenn, Stephen. 2012. “Tea for Three: Of Infusions and Inferences\nand Milk in First.” Significance 9 (6): 30–33.\nhttps://doi.org/https://doi.org/10.1111/j.1740-9713.2012.00620.x."
  }
]