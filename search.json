[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A User’s Guide to Statistical Inference and Regression",
    "section": "",
    "text": "$$\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\boldsymbol}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\b}{\\mb{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\mb{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\n\nPreface\nThis is a set of notes for Government 2002: Quantitative Social Science Methods II at Harvard University taught by Matthew Blackwell. The goal of this text is to provide a rigorous yet accessible introduction to the foundational topics in statistical inference with a special application to linear regression, a workhorse tool in the social sciences. The material is intended for first-year PhD students in political science, but it may be of interest more broadly. Much of the material has been adopted from various sources (far too many to recount now), but this book is especially indebted to the following texts:\n\nHansen, Bruce. Probability & Statistics for Economists. Princeton University Press.\nHansen, Bruce. Econometrics. Princeton University Press.\nWasserman, Larry. All of Statistics: A Concise Course in Statistical Inference. Springer.\nWooldridge, Jeffrey. Econometric Analysis of Cross Section and Panel Data\n\nYou can find the source for this book at https://github.com/mattblackwell/gov2002-book. Any typos or errors can be reported at https://github.com/mattblackwell/gov2002-book/issues. Thanks for reading.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\n\\(\\,\\)"
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "$$\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\boldsymbol}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\b}{\\mb{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\mb{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\\(\\,\\)"
  },
  {
    "objectID": "02_estimation.html#introduction",
    "href": "02_estimation.html#introduction",
    "title": "2  Estimation",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWhen studying probability, we assumed that we knew the parameter of a distribution (the mean, the variance, etc), and we used probability to understand what kind of data we would observe. Now we will put this engine in reverse and try to learn about the data generating process or some feature of it using only the observed data that we have. There are two main goals here: estimation which is how we formulate our best guess about a parameter of the DGP, and inference which is how we formalize and express uncertainty about our estimates.\nInsert two-direction diagram\n\nExample 2.1 (Randomized control trial) Suppose we are conducting a randomized experiment on framing effects. All respondents receive some factual information about recent levels of immigration, but the message for the treatment group (\\(D_i = 1\\)) has an additional framing of the positive benefits of immigration and the control group (\\(D_i = 0\\)) receives no additional framing. The outcome is a binary outcome on whether the respondent supports increasing legal immigration limits (\\(Y_i = 1\\)) or not (\\(Y_i = 0\\)). The observed data consists of \\(n\\) pairs of random variables, the outcome and the treatment assignment: \\(\\{(Y_1, D_1), \\ldots, (Y_n, D_n)\\}\\). Define the two sample means/proportions in each group as \\[\n\\Ybar_1 = \\frac{1}{n_1} \\sum_{i: D_i = 1} Y_i,  \\qquad\\qquad \\Ybar_0 = \\frac{1}{n_0} \\sum_{i: D_i = 0} Y_i,\n\\] where \\(n_1 = \\sum_{i=1}^n D_i\\) is the number of treated units and \\(n_0 = n - n_1\\) is the number of control units.\nA common estimator for the treatment effect in a study like this would be the difference in means, \\(\\Ybar_1 - \\Ybar_0\\). But this is only one possible estimator. We could also estimate the effect by taking this difference in means separately by party identification and then averaging those party-specific effects by the size of those groups. This is commonly called a poststratification estimator, but it’s unclear at first glance which of these two estimators we should prefer.\n\nWhat are the goals of studying estimators? In short, we prefer to use good estimators rather than bad estimators. But what makes an estimator good or bad? You probably have some intuitive sense that, say, an estimator that returns the value 3 is bad, but it will be helpful for us to formally define and explore some properties of estimators that will allow us to compare them and choose the good over the bad."
  },
  {
    "objectID": "02_estimation.html#samples-and-populations",
    "href": "02_estimation.html#samples-and-populations",
    "title": "2  Estimation",
    "section": "2.2 Samples and populations",
    "text": "2.2 Samples and populations\nFor most of this class, we’ll focus on a relatively simple setting where we have a random vectors \\(\\{X_1, \\ldots, X_n\\}\\) that are independent and identically distributed (iid) draws from a distribution with cumulative distribution function (cdf) \\(F\\). They are independent in the sense that the random vectors \\(X_i\\) and \\(X_j\\) are independent for all \\(i \\neq j\\), and the they are identically distributed in the sense that each of the random variables \\(X_i\\) have the same marginal distribution, \\(F\\).\nYou can think of each of these vectors, \\(X_i\\), as the rows in your data frame. Note that we’re being purposely vague about this cdf—it just represents the unknown distribution of the data, otherwise known as the data generating process (DGP). Sometimes \\(F\\) is also referred to as the population distribution or even just population, which has its roots in viewing the data as a random sample from some larger population. As a shorthand, we often say that the collection of random vectors \\(\\{X_1, \\ldots, X_n\\}\\) is a random sample from population \\(F\\) if \\(\\{X_1, \\ldots, X_n\\}\\) is iid with distribution \\(F\\). The sample size \\(n\\) is the number of units in the sample.\nThere are two metaphors that can help build intuition about the concept of viewing the data as an iid draw from \\(F\\):\n\nRandom sampling. Suppose we have a population of size \\(N\\) which is much, much larger than our sample size \\(n\\), and we take a simple random sample of size \\(n\\) from this population. Then the distribution of the data in the random sample will be iid draws from the population distribution of the variables we are sampling. For instance, suppose we take a random sample from a population of US citizens where the population proportion of Democratic party identifiers is 0.33. Then if we randomly sample \\(n = 100\\) US citizens, each data point \\(X_i\\) will be distributed Bernoulli with probability of success 0.33.\nGroundhog Day. Random sampling does not always make sense as a justification for iid data, especially when the units are not really samples at all, but rather countries, states, or subnational units. In this case, we have to appeal to thought experiment, where \\(F\\) represents the fundamental uncertainty about how the data was generated. The metaphor here is that if we could re-run history many times, like the 1993 American classic comedy Groundhog Day, data and outcomes would change slightly due to the inherently stochastic nature of the world. The iid assumption, then, is that each of the units in our data have the same DGP producing this data or the same distribution of outcomes under the Groundhog Day scenario.[1]\n\nNote that there are many, many situations where the iid assumption does not make sense. We will cover some of those later in the semester. But much of the innovation and growth in statistics over the last, say, 50 years has been in figuring out how to do statistical inference when iid is violated and often the solutions are specific to the type of iid violation you have (spatial, time-series, network, clustered, etc). As a rule of thumb, though, if you suspect a violation of iid data, your statements of uncertainty will likely be overconfident (for example, confidence intervals, which we’ll cover later, are too small)."
  },
  {
    "objectID": "02_estimation.html#point-estimation",
    "href": "02_estimation.html#point-estimation",
    "title": "2  Estimation",
    "section": "2.3 Point estimation",
    "text": "2.3 Point estimation\n\n2.3.1 Quantities of interest\nOur goal is to learn about the data generating process, represented by the cdf, \\(F\\). We might be interested in estimating the cdf at a general level or we might only be interested in estimating some feature of the distribution, like a mean or conditional expectation function. We will almost always have a particular goal in mind, but it’s useful to introduce the idea of estimation in a general way since most of the concepts about estimation, so we’ll let \\(\\theta\\) represent the quantity of interest. Point estimation is the process of providing a single “best guess” about theta whatever quantity of interest we choose, \\(\\theta\\).\n\n\n\n\n\n\nNote\n\n\n\nQuantities of interest are also referred to as parameters or estimands (that is, the target of estimation).\n\n\n\nExample 2.2 (Population mean) Suppose we wanted to know the proportion of US citizens who support increasing the level of legal immigration in the US, which we denote as \\(Y_i = 1\\). Then our quantity of interest is the mean of this random variable, \\(\\mu = \\E[Y_i]\\), which is also the probability of randomly drawing someone from the population that supports increasing legal immigration.\n\n\nExample 2.3 (Population variance) Feeling thermometer scores are a very common way to assess how a survey respondent feels about a particular person or group of people. Respondents are asked how warmly they feel about a group from 0 to 100, which we will denote \\(Y_i\\). We might be interested in how polarized views are on a group in the population and one measure of polarization could the be variance, or spread, of the distribution of \\(Y_i\\) around the mean. In this case, \\(\\sigma^2 = \\V[Y_i]\\) would be our quantity of interest.\n\n\nExample 2.4 (RCT, continued) In Example 2.1 we discussed a common estimator for an experimental study with a binary treatment. The goal of that experiment is to learn about the difference between two conditional probabilities (or expectations): the average support for increasing legal immigration in the treatment group, \\(\\mu_1 = \\E[Y_i \\mid D_i = 1]\\), and the same average in the control group, \\(\\mu_0 = \\E[Y_i \\mid D_i = 0]\\). That is, we want to know about \\(\\mu_1 - \\mu_0\\), a function of unknown features of these two conditional distributions.\n\nEach of these is a function of the (possibly joint) distribution of the data, \\(F\\). In each of these, we are not necessarily interested in the entire distribution, just summaries of it (central tendency, spread). Of course there are situations where we are also interested in the complete distribution.\n\n\n2.3.2 Estimators\nWhen our sample size is more than a few observations, it makes no sense to work with the raw data, \\(X_1, \\ldots, X_n\\), and we inevitably will need to summarize the data in some way. We can represent this summary as a function, \\(g(x_1, \\ldots, x_n)\\), which might the formula for the sample mean or sample variance. This function is just a regular function that takes in \\(n\\) numbers (or vectors) and returns a number (or vector). We can also define a random variable based on this function, \\(Y = g(X_1, \\ldots, X_n)\\), which inherits its randomness from the randomness of the data: before we see the data, we don’t know what values of \\(X_1, \\ldots, X_n\\) we will see and so we don’t know what value of \\(Y\\) we’ll see either. We call the random variable \\(Y = g(X_1, \\ldots, X_n)\\) a statistic (or sometimes sample statistics) and we refer to the probability distribution of a statistic \\(Y\\) as the sampling distribution of \\(Y\\).\n\n\n\n\n\n\nWarning\n\n\n\nThere is one potential confusion in how we talk about “statistics.” Just above we defined a statistic as a random variable, based on it being a function of random variables (ie, the data). But we also sometimes refer to the calculated value as a statistic as well, which is a specific number that you see in your R output. To be precise, we should call the latter the realized value of the statistic, but message discipline is difficult to enforce in this context. A simple example might help. Suppose that \\(X_1\\) and \\(X_2\\) are the result of a roll of two standard six-sided dice. Then the statistic \\(Y = X_1 + X_2\\) is a random variable that has a distribution over the numbers from {2, , 12} that describes our uncertainty over what the sum will be before we roll the dice. Once, we roll the dice and observed the realized values \\(X_1 = 3\\) and \\(X_2 = 4\\), we observed the realized value of the statistic, \\(Y = 7\\).\n\n\nAt their most basic, statistics are just summaries of the data without aim or ambition. Estimators are statistics with a purpose: to provide an “educated guess” about some quantity of interest.\n\nDefinition 2.1 An estimator \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\) for some parameter \\(\\theta\\), is a statistic intended as a guess about \\(\\theta\\).\n\nOne important distinction of jargon is between an estimator and an estimate, similar to issues of statistic above. The estimator is a function of data, whereas the estimate is the realized value of the estimator once we see the data. An estimate is a single number, such as 0.38, whereas the estimator is a random variable that has uncertainty over what value it will take. Formally, the estimate is \\(\\theta(x_1, \\ldots, x_n)\\) when the data is \\(\\{X_1, \\ldots, X_n\\} = \\{x_1, \\ldots, x_n\\}\\), whereas we represent the estimator as a function of random variables, \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\n\n\n\n\n\nNote\n\n\n\nIt is very common, though not universal, to use the “hat” notation to define an estimator along with its estimand. For example, \\(\\widehat{\\theta}\\) (or “theta hat”) indicates that this estimator is targeting the parameter \\(\\theta\\).\n\n\n\nExample 2.5 (Estimators for population mean) Suppose we would like to estimate the population mean of \\(F\\), which we will represent as \\(\\mu = \\E[X_i]\\). There are several estimators that we could choose from, all with different properties. \\[\n\\widehat{\\theta}_{n,1} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\quad \\widehat{\\theta}_{n,2} = X_1, \\quad \\widehat{\\theta}_{n,3} = \\text{max}(X_1,\\ldots,X_n), \\quad \\widehat{\\theta}_{n,4} = 3\n\\] The first is just the sample mean, which is an intuitive and natural estimator for the population mean. The second just uses the first observation. While this seems silly, this is a valid statistic (it’s a function of the data!). The third just takes the maximum value in the sample, and the fourth always returns 3 no matter what the data says."
  },
  {
    "objectID": "02_estimation.html#how-to-find-estimators",
    "href": "02_estimation.html#how-to-find-estimators",
    "title": "2  Estimation",
    "section": "2.4 How to find estimators",
    "text": "2.4 How to find estimators\nWhere do estimators come from? There are a couple of different methods that I’ll cover briefly here before describing the ones that will form the bulk of this class.\n\n2.4.1 Parametric models and maximum likelihood\nThe first method for generating estimators is based on parametric models, where the researcher specifies the exact distribution (up to some unknown parameters) of the DGP. Let \\(\\theta\\) be the parameters of this distribution and we then write \\(\\{X_1, \\ldots, X_n\\}\\) are iid draws from \\(F_{\\theta}\\). We should also formally state the set of possible values that the parameters can take, which we call the parameter space and usually denote as \\(\\Theta\\). Because we’re assuming we know the distribution of the data, we can write the p.d.f. as \\(f(X_i \\mid \\theta)\\) and define the likelihood function as the product of these p.d.f.s over the units as a function of the parameters: \\[\nL(\\theta) = \\prod_{i=1}^n f(X_i \\mid \\theta).\n\\] We can then define the maximum likelihood estimator (MLE) for \\(\\theta\\) as the values of the parameter that, well, maximize the likelihood: \\[\n\\widehat{\\theta}_{mle} = \\argmax_{\\theta \\in \\Theta} \\; L(\\theta)\n\\] Sometimes we can use calculus to derive a closed-form expression for the MLE, but more often we will use iterative techniques that essentially search the parameter space for the maximum.\nMaximum likelihood estimators have very nice properties, especially in large samples. Unfortunately, it also requires the correct knowledge of the parametric model, which is often difficult to justify. Do we really know if a given event count variable should be modeling as Poisson or Negative Binomial? The nice properties of MLE are only as good as our ability to get these types of choices correct.\n\n\n\n\n\n\nNo free lunch\n\n\n\nOne really important intuition to build about statistics is the assumptions-precision tradeoff. You can usually get more precise estimates if you make stronger and potentially more fragile assumptions. Conversely, if you want to weaken your assumptions, you will almost always get less precise estimates.\n\n\n\n\n2.4.2 Plug-in estimators\nThe second broad class of estimators are semiparametric in the sense that we will specify some finite-dimensional parameters of the DGP, but leave the rest of the distribution unspecified. For example, we might specify a population mean, \\(\\mu = \\E[X_i]\\), and a population variance, \\(\\sigma^2 = \\V[X_i]\\) but leave unrestricted the shape of the distribution. This is really important because our estimators will not be as dependent on correctly specifying distributions that maybe have no business specifying.\nThe basic method for constructing estimators in this setting is to use the plug-in estimator, or the estimator that replaces any population mean with a sample mean. Obviously in the case of estimating the population mean, \\(\\mu\\), this means we will use the sample mean as its estimate: \\[\n\\Xbar_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\quad \\text{estimates} \\quad \\E[X_i] = \\int_{\\mathcal{X}} x f(x)dx\n\\] What are we doing here? We are replacing the unknown population distribution \\(f(x)\\) that’s in the population mean with a discrete uniform distribution over our data points, with \\(1/n\\) probability assigned to each unit. Why do this? It encodes that if we have a random sample, our best guess about the population distribution of \\(X_i\\) is the sample distribution in our actual data. If this intuition fails, it is also fine to hold onto an analog principle: sample means of random variables are natural estimators of population means.\nWhat about estimating something more complicated like a the expected value of a function of the data, \\(\\theta = \\E[r(X_i)]\\)? The key is to see that \\(f(X_i)\\) is also a random variable. Let’s call this random variable \\(Y_i = f(X_i)\\) now we can see that \\(\\theta\\) is just the population expectation of this random variable and using the plug-in estimator, we get: \\[\n\\widehat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\nWith these facts in hand, we can describe the more general plug-in estimator. When we want to estimate some quantity of interest that is a function of population means, we can generate a plug-in estimator by replacing any population mean with a sample mean. Formally, let \\(\\alpha = g\\left(\\E[r(X_i)]\\right)\\) be a parameter that is defined as a function of the population mean of a (possibly vector-valued) function of the data. Then, we can estimate this parameter by plugging in the sample mean for the population mean to get the plug-in estimator, \\[\n\\widehat{\\alpha} = g\\left( \\frac{1}{n} \\sum_{i=1}^n r(X_i) \\right) \\quad \\text{estimates} \\quad \\alpha = g\\left(\\E[r(X_i)]\\right)\n\\] This approach to plug-in estimation with sample means is very general and will allow us to derive estimators in a large variety of settings.\n\nExample 2.6 (Estimating population variance) The population variance of a random variable is \\(\\sigma^2 = \\E[(X_i - \\E[X_i])^2]\\). To derive a plug-in estimator for this quantity, we replace the inner \\(\\E[X_i]\\) with \\(\\Xbar_n\\) and the outer expectation with another sample mean: \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2.\n\\] Note that this plug-in estimator is not the same as the usual sample variance, which divides by \\(n - 1\\) rather than \\(n\\), but this is a very minor difference that does not matter in moderate to large samples.\n\n\nExample 2.7 (Estimating population covariance) Suppose we have two variables, \\((X_i, Y_i)\\). A natural quantity of interest here is the population covariance between these variables, \\[\n\\sigma_{xy} = \\text{Cov}[X_i,Y_i] = \\E[(X_i - \\E[X_i])(Y_i-\\E[Y_i])],\n\\] which has the plug-in estimator, \\[\n\\widehat{\\sigma}_{xy} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)(Y_i - \\Ybar_n).\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nGiven the connection between the population mean and the sample mean, you will sometimes see the \\(\\E_n[\\cdot]\\) operator used as a short hand for the sample average: \\[\n\\E_n[r(X_i)] \\equiv \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\n\n\nFinally, we are describing the plug-in estimator only considering replacing population means with sample means, but the idea of plug-in estimation is actually much more broad. We can derive estimators of the population quantiles like the median with sample versions of those quantities. What unifies all of these approaches is replacing the unknown population cdf, \\(F\\), with the empirical cdf, \\[\n\\widehat{F}_n(x) = \\frac{\\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}{n}.\n\\] For a more complete and technical treatment of these ideas, see Wasserman (2004) Chapter 7."
  },
  {
    "objectID": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "href": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "title": "2  Estimation",
    "section": "2.5 The three distributions: population, empirical, and sampling",
    "text": "2.5 The three distributions: population, empirical, and sampling\nOnce we start to wade into estimation, there are several distributions to keep track of and things can quickly become confusing. There are three distributions that are all related and easy to confuse but it’s important to keep them distinct.\nThe population distribution is the distribution of the random variable, \\(X_i\\), which we have labeled \\(F\\). This is the distribution that we want to learn about. Then there is the empirical distribution, which is the distribution of the actual realizations of the random variables in our samples (that is, the numbers in our data frame), \\(X_1, \\ldots, X_n\\). Because this is a random sample from the population distribution and can serve as estimator of \\(F\\), we sometimes call this \\(\\widehat{F}_n\\).\nInsert Sampling distribution figure here\nSeparately from both of these is the sampling distribution of an estimator, which is the probability distribution of \\(\\widehat{\\theta}_n\\). It represents our uncertainty about what our estimate will be before we see the data. Remember that our estimator is itself a random variable because it is a function of random variables: the data itself. That is, we defined the estimator as \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\nExample 2.8 (Likert resposes) Suppose \\(X_i\\) is the answer to a question, “How much do you agree with the following statement: Immigrants are a net positive for the United States,” with a \\(X_i = 0\\) being “strongly disagree,” \\(X_i = 1\\) being “disgree”, \\(X_i = 2\\) being “neither agree nor disagree”, \\(X_i = 3\\) being “agree”, and \\(X_i = 4\\) being “strongly agree.”\nThe population distribution describes the probability of randomly selecting a person with each one of these values, \\(\\P(X_i = x)\\). The empirical distribution would be the actual numeric values 0-4 in our data. And the sampling distribution of the sample mean, \\(\\Xbar_n\\), would be the distribution of the sample mean across repeated samples from the population.\nSuppose that the population distribution was binomial with 4 trials and probability of success \\(p = 0.4\\). We could generate one sample with \\(n = 10\\) and thus one empirical distribution using rbinom:\n\n\n\n\nmy_samp <- rbinom(n = 10, size = 4, prob = 0.4)\nmy_samp\n\n [1] 1 2 1 3 3 0 2 3 2 1\n\ntable(my_samp)\n\nmy_samp\n0 1 2 3 \n1 3 3 3 \n\n\nAnd we can generate one draw from the sampling distribution of \\(\\Xbar_n\\) by taking the mean of this sample:\n\nmean(my_samp)\n\n[1] 1.8\n\n\nBut obviously if we had a different sample, it would have a different empirical distribution and thus give us a different estimate of the sample mean:\n\nmy_samp2 <- rbinom(n = 10, size = 4, prob = 0.4)\nmean(my_samp2) \n\n[1] 1.6\n\n\nThe sampling distribution is the distribution of these sample means across repeated sampling."
  },
  {
    "objectID": "02_estimation.html#finite-sample-properties-of-estimators",
    "href": "02_estimation.html#finite-sample-properties-of-estimators",
    "title": "2  Estimation",
    "section": "2.6 Finite-sample properties of estimators",
    "text": "2.6 Finite-sample properties of estimators\nAs we discussed when we introduced estimators, their usefulness is tied to how well they help us learn about the quantity of interest. If we get an estimate \\(\\widehat{\\theta} = 1.6\\), we would like to know that this is “close” to the true parameter \\(\\theta\\). The key to understanding how the behavior of our estimators is the sampling distribution. Intuitively, we would like the sampling distribution of \\(\\widehat{\\theta}_n\\) to be as tightly clustered around the true as \\(\\theta\\) as possible. Here, though, we run into a problem: the sampling distribution depends on the population distribution since it is about repeated samples of the data from that distribution filtered through the function \\(\\theta()\\). Since \\(F\\) is unknown, this implies that the sampling distribution will also usually be unknown.\nEven though we cannot precisely pin down the entire sampling distribution, we will be able to use assumptions to derive certain properties of the sampling distribution that will be useful in comparing estimators.\n\n2.6.1 Bias\nThe first property of the sampling distribution concerns its central tendency. In particular, we will define the bias (or estimation bias) of estimator \\(\\widehat{\\theta}\\) for parameter \\(\\theta\\) as \\[\n\\text{bias}[\\widehat{\\theta}] = \\E[\\widehat{\\theta}] - \\theta.\n\\] This is the difference between the mean of the estimator (across repeated samples) and the true parameter. All else equal, we would like estimation bias to be as small as possible. The smallest possible bias, obviously, is 0 and we define an unbiased estimator as one with with \\(\\text{bias}[\\widehat{\\theta}] = 0\\) or equivalently, \\(\\E[\\widehat{\\theta}] = \\theta\\).\nHowever, all else is not always equal and unbiasedness is not a property to become overly attached to. There are many biased estimators that have other attractive properties and many popular modern estimators are biaseed.\n\nExample 2.9 (Unbiasedness of the sample mean) We can show that the sample mean is unbiased for the population mean when the data is iid and \\(\\E|X| < \\infty\\). In particular, we simply apply the rules of expectations: \\[\\begin{aligned}\n\\E\\left[ \\Xbar_n \\right] &= \\E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] & (\\text{definition of } \\Xbar_n) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\E[X_i] & (\\text{linearity of } \\E)\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu & (X_i \\text{ identically distributed})\\\\\n&= \\mu.\n\\end{aligned}\\] Notice that we only used the “identically distributed” part of iid. Independence is not needed.\n\n\n\n\n\n\n\nWarning\n\n\n\nProperties like unbiasedness might only hold for a subset of DGPs. For example, we just showed that the sample mean is unbiased, but only when the population mean is finite. There are probability distribution like the Cauchy where the expected value diverges and is not finite. So we are dealing with a restricted class of DGPs that rules out such distributions. You may see this sometimes formalized by defining a class \\(\\mathcal{F}\\) of distributions and unbiasedness might hold in that class if it is unbiased for all \\(F \\in \\mathcal{F}\\).\n\n\n\n\n2.6.2 Estimation variance and the standard error\nIf a “good” estimator tends to be close to the truth, then we should also care how spread out the sampling distribution is. In particular, we define the sampling variance as the variance of an estimator’s sampling distribution, \\(\\V[\\widehat{\\theta}]\\). This measures how spread the estimator it is around its mean. For an unbiased estimator, smaller sampling variance implies the distribution of \\(\\widehat{\\theta}\\) is more concentrated around the truth.\n\nExample 2.10 (Sampling variance of the sample mean:) We can establish the sampling variance of the sample mean of iid data for all \\(F\\) such that \\(\\V[X_i]\\) is finite (more precisely, \\(\\E[X_i^2] < \\infty\\))\n\\[\\begin{aligned}\n  \\V\\left[ \\Xbar_n \\right] &= \\V\\left[ \\frac{1}{n} \\sum_{i=1}^n X_i \\right] & (\\text{definition of } \\Xbar_n) \\\\\n                           &=\\frac{1}{n^2} \\V\\left[ \\sum_{i=1}^n X_i \\right] & (\\text{property of } \\V)\\\\\n                           &=\\frac{1}{n^2} \\sum_{i=1}^n \\V[X_i] & (\\text{independence})\\\\\n                           &= \\frac{1}{n^2}\\sum_{i=1}^n \\sigma^2 & (X_i \\text{ identically distributed})\\\\\n                           &= \\frac{\\sigma^2}{n}\n\\end{aligned}\\]\n\nAn alternative measure of spread for any distribution is the standard deviation, which is on the same scale as the original random variable. We call the standard deviation of the sampling distribution of \\(\\widehat{\\theta}\\) the standand error of \\(\\widehat{\\theta}\\): \\(\\se(\\widehat{\\theta}) = \\sqrt{\\V[\\widehat{\\theta}]}\\).\nGiven the above derivation the standard error of the sample mean under iid sampling is \\(\\sigma / \\sqrt{n}\\)\n\n\n2.6.3 Mean squared error\nBias and sampling variance clearly got at two different aspects of being a “good” estimator. Ideally, we want the estimator to be as close as possible to the true value. One summary measure of the quality of an estimator is the mean squared error or MSE which is defined as. \\[\n\\text{MSE} = \\E[(\\widehat{\\theta}_n-\\theta)^2]\n\\] Ideally, we would have this be as small as possible!\nWe can also relate the MSE to the bias and the sampling variance (provided it is finite) with the following decomposition result: \\[\n\\text{MSE} = \\text{bias}[\\widehat{\\theta}_n]^2 + \\V[\\widehat{\\theta}_n]\n\\] This decomposition implies that, for unbiased estimators, MSE is the sampling variance. It also highlights why we might accept some bias for large reductions in variance for lower overall MSE.\n\n\n\n\n\nTwo sampling distributions\n\n\n\n\nIn this figure, we show the sampling distributions of two estimators, \\(\\widehat{\\theta}_a\\), which is unbiased (centered on the true value \\(\\theta\\)) but with a high sampling variance, and \\(\\widehat{\\theta}_b\\) which is slightly biased but with much lower sampling variance. Even though \\(\\widehat{\\theta}_b\\) is biased, the probability of drawing a value close to the truth is higher than for \\(\\widehat{\\theta}_a\\). This type of balancing between bias and variance is exactly what the MSE helps capture and indeed, in this case, \\(MSE[\\widehat{\\theta}_b] < MSE[\\widehat{\\theta}_a]\\)."
  },
  {
    "objectID": "03_asymptotics.html#introduction",
    "href": "03_asymptotics.html#introduction",
    "title": "3  Asymptotics",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn the last chapter, we defined estimators and started to investigate their finite-sample properties like unbiasedness and the sampling variance. We call these “finite-sample” properties because establishing them generally does not depend on the sample size. We saw that under iid data, the sample mean is unbiased for the population mean, but this result holds as much for \\(n = 10\\) as it does for \\(n = 1,000,000\\). But these properties are also of limited use: we only learn the center and spread of the sampling distribution of \\(\\Xbar_n\\) from these results. What about the shape of the distribution? We can often derive the shape if we are willing to make certain assumptions on the underlying data (for example, if the data is normal, then the sample means will be normal as well), but this approach is brittle: if our parametric assumption is false, we’re back to square one.\nIn this chapter, we’re going to take a different approach and see what happens to the sampling distribution of estimators as the sample size gets large. The study of the estimators as the sample size goes to infinity is called asymptotic theory, but it’s important to understand everything we do with asymptotics will be an approximation. No one ever has infinite data, but we hope that as our samples get larger, the approximations will be closer to the truth. Why work in this asymptopia, though? It turns out that many expressions are much easier to derive in the limit than in finite samples."
  },
  {
    "objectID": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "href": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "title": "3  Asymptotics",
    "section": "3.2 Why convergence with probability is hard",
    "text": "3.2 Why convergence with probability is hard\nIt’s helpful to review the basic idea of convergence in deterministic sequences from calculus:\n\nDefinition 3.1 A sequence \\(\\{a_n: n = 1, 2, \\ldots\\}\\) has the limit \\(a\\) written \\(a_n \\rightarrow a\\) as \\(n\\rightarrow \\infty\\) of \\(\\lim_{n\\rightarrow \\infty} a_n = a\\) if for all \\(\\epsilon > 0\\) there is some \\(n_{\\epsilon} < \\infty\\) such that for all \\(n \\geq n_{\\epsilon}\\), \\(|a_n - a| \\leq \\epsilon\\).\n\nWe say that \\(a_n\\) converges to \\(a\\) if \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Basically, a sequence converges to a number if the sequence gets closer and closer to that number as the sequence goes on.\nCan we apply this same idea to sequences of random variables (like estimators)? Let’s look at a few examples that might help clarify the difficult in doing so.1 Let’s say that we have a sequence of \\(a_n = a\\) for all \\(n\\) (that is, a constant sequence). Then obviously \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Now let’s say we have a sequence of random variables, \\(X_1, X_2, \\ldots\\), that are all independent with a standard normal distribution, \\(N(0,1)\\). From the analogy to the deterministic case, it is tempting to say that \\(X_n\\) converges to \\(X \\sim N(0, 1)\\), but notice that because they are all different random variables, \\(\\P(X_n = X) = 0\\). Thus, we need to be careful about saying how one variable converges to another variable.\nAnother example highlights subtle problems with a sequence of random variables converging to a single value. Suppose we have a sequence of random variables \\(X_1, X_2, \\ldots\\) where \\(X_n \\sim N(0, 1/n)\\). Clearly, \\(X_n\\) will be concentrated around 0 for large values of \\(n\\), so it is tempting to say that \\(X_n\\) converges to 0. But notice that \\(\\P(X_n = 0) = 0\\) because of the nature of continuous random variables."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "href": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "title": "3  Asymptotics",
    "section": "3.3 Convergence in probability and consistency",
    "text": "3.3 Convergence in probability and consistency\nThere are several different ways that a sequence of random variance can converge. The first type of convergence deals with sequence converging to a single value.2\n\nDefinition 3.2 A sequence of random variables, \\(X_1, X_2, \\ldots\\), is said to converge in probability to a value \\(b\\) if for every \\(\\varepsilon > 0\\), \\[\n\\P(|X_n - b| > \\varepsilon) \\rightarrow 0,\n\\] as \\(n\\rightarrow \\infty\\). We write this \\(X_n \\inprob b\\).\n\nWith deterministic sequences, we said that \\(a_n\\) converges to \\(a\\) is it gets closer and closer to \\(a\\) as \\(n\\) gets bigger. For convergence in probability, the sequence of random variables converges to \\(b\\) is the probability that random variables are far away from \\(b\\) get smaller and smaller as \\(n\\) gets big.\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will sometimes see convergence in probability written as \\(\\text{plim}(Z_n) = b\\) if \\(Z_n \\inprob b\\), \\(\\text{plim}\\) stands for “probability limit.”\n\n\nConvergence in probability is incredibly useful for evaluating estimators. While we said that unbiasedness was not the be all and end all of properties of estimators, the following property is a fairly basic and fundamental property that we would like all good estimators to have.\n\nDefinition 3.3 An estimator is consistent if \\(\\widehat{\\theta}_n \\inprob \\theta\\).\n\nConsistency of an estimator implies that the sampling distribution of this estimator “collapses” on the true value as the sample size gets large. We say an estimator is inconsistent if it converges in probability to any other value, which is obviously a very bad property of an estimator. It means that as the sample size gets large, the probability that the estimator will be close to the truth will approach 0.\nWe can also define convergence in probability for a sequence of random vectors, \\(\\X_1, \\X_2, \\ldots\\), where \\(\\X_i = (X_{i1}, \\ldots, X_{ik})\\) is a random vector of length \\(k\\). This sequence convergences in probbaility to a vector \\(\\mb{b} = (b_1, \\ldots, b_k)\\) if and only if each random variable in the vector converges to the corresponding element in \\(\\mb{b}\\), or that \\(X_{nj} \\inprob b_j\\) for all \\(j = 1, \\ldots, k\\)."
  },
  {
    "objectID": "03_asymptotics.html#useful-inequalities",
    "href": "03_asymptotics.html#useful-inequalities",
    "title": "3  Asymptotics",
    "section": "3.4 Useful inequalities",
    "text": "3.4 Useful inequalities\nAt first glance, it appears establishing consistency of an estimator will be difficult. How can we know if a distribution will collapse to a specific value without knowing the shape or family of the distribution? It turns out that there are certain relationships between the mean and variance of a random variable and certain probability statements that hold for all distributions (that have finite variance at least). This will be incredibly helpful to us.\n\nTheorem 3.1 (Markov Inequality) For any r.v. \\(X\\) and any \\(\\delta >0\\), \\[\n\\P(|X| \\geq \\delta) \\leq \\frac{\\E[|X|]}{\\delta}.\n\\]\n\n\nProof. Notice that we can let \\(Y = |X|/\\delta\\) and rewrite the statement as \\(\\P(Y \\geq 1) \\leq \\E[Y]\\) (since \\(E[|X|]/\\delta = \\E[|X|/\\delta]\\) by the properties of expectation), which is what we will show. But notice that \\[\n\\mathbb{1}(Y \\geq 1) \\leq Y.\n\\] Why does this hold? We can investigate the two possible values of the indicator function to see. If \\(Y\\) is less than 1, then the indicator function will be 0, but \\(Y\\) is non-negative so we know that it must be at least as big as 0 so that inequality holds. If \\(Y \\geq 1\\) then the indicator function 1 but we just said that \\(Y \\geq 1\\) so the inequality holds. If we take the expectation of both sides of this inequality, we obtain the result (remember the expectation of an indicator function is the probability of the event being indicated).\n\nIn words, Markov’s inequality says that the probability of a random variable being large in magnitude cannot be high if the average is not large in magnitude. Blitzstein and Hwang 2019) provide a nice intuition behind this result. Let \\(X\\) be the income of a randomly selected individual in a population and set \\(\\delta = 2\\E[X]\\), so that the inequality becomes \\(\\P(X > 2\\E[X]) < 1/2\\) (assuming that all income is nonnegative). Here, the inequality says that the share of the population that has an income twice the average must be less than 0.5, since if more than half the population was making twice the average income then the average would have to be higher.\nIt’s quite astounding how general this result is since it holds for all random variables. Of course, its generality comes at the expense of not being very informative. If \\(\\E[|X|] = 5\\), for instance, the inequality tells us that \\(\\P(|X| \\geq 1) \\leq 5\\) which is not very helpful since we already know that probabilities are less than 1! If we are willing to make some assumptions about \\(X\\), we can get tighter bounds.\n\nTheorem 3.2 (Chebyshev Inequality) Suppose that \\(X\\) is r.v. for which \\(\\V[X] < \\infty\\). Then, for every real number \\(\\delta > 0\\), \\[\n\\P(|X-\\E[X]| \\geq \\delta) \\leq \\frac{\\V[X]}{\\delta^2}.\n\\]\n\n\nProof. To prove this, we only need to square both sides of the inequality inside the probability statement and apply Markov’s inequality: \\[\n\\P\\left( |X - \\E[X]| \\geq \\delta \\right) = \\P((X-\\E[X])^2 \\geq \\delta^2) \\leq \\frac{\\E[(X - \\E[X])^2]}{\\delta^2} = \\frac{\\V[X]}{\\delta^2},\n\\] with the last equality holding by the definition of variance.\n\nThis is a straightforward extension of the Markov result: the probability of a random variable being far away from its mean (that is, \\(|X-\\E[X]|\\) being large) is limited by the variance of the random variable. If we let \\(\\delta = c\\sigma\\), where \\(\\sigma\\) is the standard deviation of \\(X\\), then we can use this result to bound the normalized: \\[\n\\P\\left(\\frac{|X - \\E[X]|}{\\sigma} > c \\right) \\leq \\frac{1}{c^2}.\n\\] This says that the probability of being, say, 2 standard deviations away from the mean must be less than 1/4 = 0.25. Notice that this bound can be quite wide. If \\(X\\) is normally distributed, then we know that just about 5% of draws will be greater than 2 SDs away from the mean, which is much lower than the 25% bound implied by Chebyshev’s inequality."
  },
  {
    "objectID": "03_asymptotics.html#the-law-of-large-numbers",
    "href": "03_asymptotics.html#the-law-of-large-numbers",
    "title": "3  Asymptotics",
    "section": "3.5 The law of large numbers",
    "text": "3.5 The law of large numbers\nWe can now use these inequalities to show how certain estimators are consistent for certain quantities of interest. Why are these inequalities useful for this purpose? Remember that convergence in probability was about the probability of an estimator being far away from a value going to zero. Chebyshev’s inequality shows that we can bound these exact probabilities.\nThe most famous consistency result has a special name.\n\nTheorem 3.3 (Weak Law of Large Numbers) Let \\(X_1, \\ldots, X_n\\) be a an i.i.d. draws from a distribution with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i] < \\infty\\). Let \\(\\Xbar_n = \\frac{1}{n} \\sum_{i =1}^n X_i\\). Then, \\(\\Xbar_n \\inprob \\mu\\).\n\n\nProof. Recall that the sample mean is unbiased, so \\(\\E[\\Xbar_n] = \\mu\\) with sampling variance \\(\\sigma^2/n\\). We can then simply apply Chebyshev to the sample mean to get \\[\n\\P(|\\Xbar_n - \\mu| \\geq \\delta) \\leq \\frac{\\sigma^2}{n\\delta^2}\n\\] An \\(n\\rightarrow\\infty\\), the right-hand side goes to 0 which means that the left-hand side also must go to 0 which is the definition of \\(\\Xbar_n\\) converging in probability to \\(\\mu\\).\n\nThe weak law of large numbers (WLLN) shows that, under general conditions, the sample mean gets closer to the population mean as \\(n\\rightarrow\\infty\\). In fact, this result holds even when the variance of the is infinite, though that’s a situation that most analysts will rarely face.\n\n\n\n\n\n\nNote\n\n\n\nThe naming of the “weak” law of large numbers seems to imply the existence of a “strong” law of large numbers (SLLN) and this is true. The SLLN states that the sample mean converges to the population mean with probability 1. This type of convergence, called almost sure convergence, is stronger than convergence in probability which only says that the probability of the sample mean being close to the population mean converges to 1. While it is nice to know that this stronger form of convergence holds for the sample mean under the same assumptions, it is very rare for folks outside of theoretical probability and statistics to need to rely on almost sure convergence.\n\n\n\nExample 3.1 It can be helpful to see how the distribution of the sample mean changes as a function of the sample size to appreciate the WLLN. We can show this by taking repeated iid samples of different sizes from an exponential rv with rate 0.5 so that \\(\\E[X_i] = 2\\). In Figure 3.1, we show the distribution of the sample mean (across repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue), and 1000 (green). What we can see is how the distribution of the sample mean is “collapsing” on the true population mean, 2. The probability of being far away from 2 becomes progressively smaller.\n\n\n\n\n\nFigure 3.1: Sampling distribution of the sample mean as a function of sample size\n\n\n\n\n\nThe WLLN also holds for random vectors in addition to random variables. Let \\((\\X_1, \\ldots, \\X_n)\\) be an iid sample of random vectors of length \\(k\\), \\(\\mb{X}_i = (X_{i1}, \\ldots, X_{ik})\\). We can define the vector sample mean as just the vector of sample means for each of the entries:\n\\[\n\\overline{\\mb{X}}_n = \\frac{1}{n} \\sum_{i=1}^n \\mb{X}_i =\n\\begin{pmatrix}\n\\Xbar_{n,1} \\\\ \\Xbar_{n,2} \\\\ \\vdots \\\\ \\Xbar_{n, k}\n\\end{pmatrix}\n\\] Since this is just a vector of sample means, each random variable in the random vector will converge in probability to the mean of that random variable. Fortunately, this is the exact definition of convergence in probability for random vectors. We formally write this in the following theorem.\n\nTheorem 3.4 If \\(\\X_i \\in \\mathbb{R}^k\\) are iid draws from a distribution with \\(\\E[X_{ij}] < \\infty\\) for all \\(j=1,\\ldots,k\\) then as \\(n\\rightarrow\\infty\\)\n\\[\n\\overline{\\mb{X}}_n \\inprob \\E[\\X]  =\n\\begin{pmatrix}\n\\E[X_{i1}] \\\\ \\E[X_{i2}] \\\\ \\vdots \\\\ \\E[X_{ik}]\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will have noticed that many of the formal results we have presented so far have “moment conditions” that certain moments are finite. For the vector WLLN, we saw that applied to the mean of each variable in the vector. Some books use a short hand for this: \\(\\E\\Vert \\X_i\\Vert < \\infty\\), where \\[\n\\Vert\\X_i\\Vert = \\left(X_{i1}^2 + X_{i2}^2 + \\ldots + X_{ik}^2\\right)^{1/2}.\n\\] This is slightly more compact notation, but why does it work? One can show that this function, called the Euclidean norm or \\(L_2\\)-norm is a convex function, so we can apply Jensen’s inequality to show that: \\[\n\\E\\Vert \\X_i\\Vert \\geq \\Vert \\E[\\X_i] \\Vert = (\\E[X_{i1}]^2 + \\ldots + \\E[X_{ik}]^2)^{1/2}.\n\\] So if \\(\\E\\Vert \\X_i\\Vert\\) is finite, it means that all the component means are finite otherwise the right-hand side of the previous equation would be infinite."
  },
  {
    "objectID": "03_asymptotics.html#consistency-of-estimators",
    "href": "03_asymptotics.html#consistency-of-estimators",
    "title": "3  Asymptotics",
    "section": "3.6 Consistency of estimators",
    "text": "3.6 Consistency of estimators\nThe WLLN shows that the sample mean of iid draws is consistent for the population mean, which is a massive result given that so many estimators can be written as sample means. What about other estimators? The proof of the WLLN points to one way to determine if an estimator is consistent: if it is unbiased and the sampling variance shrinks as the sample size grows. The next theorem\n\nTheorem 3.5 For any estimator \\(\\widehat{\\theta}_n\\), if \\(\\text{bias}[\\widehat{\\theta}_n] \\to 0\\) and \\(\\V[\\widehat{\\theta}_n] \\rightarrow 0\\) as \\(n\\rightarrow \\infty\\), then \\(\\widehat{\\theta}_n\\) is consistent.\n\nThus, if we can characterize the bias and sampling variance of an estimator, then we should be able to tell if it consistent or not. This is handy since working with the kinds of probability inequalities used for the WLLN can sometimes be quite confusing.\nWhat do we do if it is difficult or impossible to characterize the bias? Consider a plug-in estimator like \\(\\widehat{\\alpha} = \\log(\\Xbar_n)\\) where \\(X_1, \\ldots, X_n\\) are iid from a population with mean \\(\\mu\\). We know that for nonlinear functions like logarithms we have \\(\\log\\left(\\E[Z]\\right) \\neq \\E[\\log(Z)]\\), so \\(\\E[\\widehat{\\alpha}] \\neq \\log(\\E[\\Xbar_n])\\) and the plug-in estimator will be biased for \\(\\log(\\mu)\\). It will also be difficult to obtain an expression for the bias in terms of \\(n\\). Is all hope lost here? Must we give up on consistency? No, and in fact, consistency will be much simpler to show in this setting.\n\nTheorem 3.6 (Properties of convergence in probability) Let \\(X_n\\) and \\(Z_n\\) be two sequences of random variables such that \\(X_n \\inprob a\\) and \\(Z_n \\inprob b\\), and let \\(g(\\cdot)\\) be a continuous function. Then,\n\n\\(g(X_n) \\inprob g(a)\\) (continuous mapping theorem)\n\\(X_n + Z_n \\inprob a + b\\)\n\\(X_nZ_n \\inprob ab\\)\n\\(X_n/Z_n \\inprob a/b\\) if \\(b > 0\\).\n\n\nWe can now see that many of the nasty problems with expectations and nonlinear functions are made considerably easier with convergence in probability in the asymptotic setting. So while we know that \\(\\log(\\Xbar_n)\\) is biased for \\(\\log(\\mu)\\), we know that it is consistent since \\(\\log(\\Xbar_n) \\inprob \\log(\\mu)\\) because \\(\\log\\) is a continuous function.\n\nExample 3.2 Suppose we implemented a survey by randomly selecting a sample from the population of size \\(n\\), but not everyone responded to our survey. Let the data consist of pairs of random variables, \\((Y_1, R_1), \\ldots, (Y_n, R_n)\\), where \\(Y_i\\) is the question of interest and \\(R_i\\) is a binary indicator for if the respondent answered the question (\\(R_i = 1\\)) or not (\\(R_i = 0\\)). Our goal is to estimate the mean of the question for responders: \\(\\E[Y_i \\mid R_i = 1]\\). We can use the law of iterated expectation to which we can rewrite as \\[\n\\begin{aligned}\n\\E[Y_iR_i] &= \\E[Y_i \\mid R_i = 1]\\P(R_i = 1) + \\E[ 0 \\mid R_i = 0]\\P(R_i = 0) \\\\\n\\implies \\E[Y_i \\mid R_i = 1] &= \\frac{\\E[Y_iR_i]}{\\P(R_i = 1)}\n\\end{aligned}\n\\]\nThe relevant estimator for this quantity is the mean of the of the outcome among those who responded, which is slightly more complicated than a typical sample mean because the denominator is a random variable: \\[\n\\widehat{\\theta}_n = \\frac{\\sum_{i=1}^n Y_iR_i}{\\sum_{i=1}^n R_i}.\n\\] Notice that this estimator is the ratio of two random variables. The numerator has mean \\(n\\E[Y_iR_i]\\) and the denominator has mean \\(n\\P(R_i = 1)\\). It is then tempting to say that we can take the ratio of these means as the mean of \\(\\widehat{\\theta}_n\\), but expectations are not preserved in nonlinear functions like this one.\nWe can establish consistency of our estimator, though, by noting that we can rewrite the estimator as a ratio of sample means \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i},\n\\] where by the WLLN the numerator \\((1/n)\\sum_{i=1}^n Y_iR_i \\inprob \\E[Y_iR_i]\\) and the denominator \\((1/n)\\sum_{i=1}^n R_i \\inprob \\P(R_i = 1)\\). Thus, by Theorem 3.6, we have \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i} \\inprob \\frac{\\E[Y_iR_i]}{\\P[R_i = 1]} = \\E[Y_i \\mid R_i = 1]\n\\] so long as the probability of responding is greater than zero. This establishes that our sample mean among responders while biased for the conditional expectation among responders, it is consistent for that quantity.\n\nIt is very important to keep the difference between unbiased and consistent clear in your mind. There are very many silly unbiased estimators that are inconsistent. Let’s go back to our iid sample, \\(X_1, \\ldots, X_n\\) from a population with \\(E[X_i] = \\mu\\). There is nothing in the rule book against defining an estimator \\(\\widehat{\\theta}_{first} = X_1\\) that just uses the first observation as the estimate. This seems like an obviously silly estimator, but it is actually unbiased since \\(\\E[\\widehat{\\theta}_{first}] = \\E[X_1] = \\mu\\). It is inconsistent since the sampling variance of this estimator is just the variance of the population distribution, \\(\\V[\\widehat{\\theta}_{first}] = \\V[X_i] = \\sigma^2\\), which does not change as a function of the sample size. Generally speaking, we can regard “unbiased, but inconsistent” estimators as silly and not worth our time (along with bias and inconsistent estimators).\nThere are also estimators that are biased but consistent that are often much more interesting. We already saw one such estimator in Example 3.2, but there are many more. Maximum likelihood estimators, for example, are (under some regularity conditions) consistent for the parameters of a parametric model, but they are often biased.\n\nExample 3.3 (Plug-in variance estimator) Last chapter, we introduced the plug-in estimator for the population variance, \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2,\n\\] which we will now show is biased but consistent. To see the bias note that we can rewrite the sum of square deviations \\[\\sum_{i=1}^n (X_i - \\Xbar_n)^2 = \\sum_{i=1}^n X_i^2 - n\\Xbar_n. \\] Then, the expectation of the plug-in estimator is \\[\n\\begin{aligned}\n\\E[\\widehat{\\sigma}^2] & = \\E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i^2\\right] - \\E[\\Xbar_n^2] \\\\\n&= \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n \\E[X_iX_j] \\\\\n&= \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j\\neq i} \\underbrace{\\E[X_i]\\E[X_j]}_{\\text{independence}} \\\\\n&= \\E[X_i^2] - \\frac{1}{n}\\E[X_i^2] - \\frac{1}{n^2} n(n-1)\\mu^2 \\\\\n&= \\frac{n-1}{n} \\left(\\E[X_i^2] - \\mu^2\\right) \\\\\n&= \\frac{n-1}{n} \\sigma^2 = \\sigma^2 - \\frac{1}{n}\\sigma^2\n\\end{aligned}.\n\\] Thus, we can see that the bias of the plug-in estimator is \\(-(1/n)\\sigma^2\\) so it slightly underestimates the variance. Nicely, though, the bias shrinks as a function of the sample size, so according to Theorem 3.5 it will be consistent so long as the sampling variance of \\(\\widehat{\\sigma}^2\\) shrinks as a function of the sample size, which it does (though omit that proof here). Of course, simply multiplying this estimator by \\(n/(n-1)\\) will give an unbiased and consistent estimator that is also the typical sample variance estimator."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "href": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "title": "3  Asymptotics",
    "section": "3.7 Convergence in distribution and the central limit theorem",
    "text": "3.7 Convergence in distribution and the central limit theorem\nConvergence in probability and the law of large numbers are very useful for understanding how our estimators will (or will not) collapse to their estimand as the sample size increases. But what about the shape of the sampling distribution of our estimators? For the purposes of statistical inference, we would like to be able to make probability statements such as \\(\\P(a \\leq \\widehat{\\theta}_n \\leq b)\\). These types of statements will be the basis of hypothesis testing and confidence intervals. But in order make those types of statements, we need to know the entire distribution of \\(\\widehat{\\theta}_n\\), not just the mean and variance. Luckily, there are established results that will allow us to approximate the sampling distribution of a huge swath of estimators when our sample sizes are large.\nTo see how we will develop these approximations, we need to first describe a weaker form of convergence to a distribution rather than to a single value.\n\nDefinition 3.4 Let \\(X_1,X_2,\\ldots\\), be a sequence of r.v.s, and for \\(n = 1,2, \\ldots\\) let \\(F_n(x)\\) be the c.d.f. of \\(X_n\\). Then it is said that \\(X_1,X_2, \\ldots\\) converges in distribution to r.v. \\(X\\) with c.d.f. \\(F(x)\\) if \\[\n\\lim_{n\\rightarrow \\infty} F_n(x) = F(x),\n\\] for all values of \\(x\\) for which \\(F(x)\\) is continuous. We write this as \\(X_n \\indist X\\) or sometimes \\(X_n ⇝ X\\).\n\nEssentially, convergence in distribution means that as \\(n\\) gets large, the distribution of \\(X_n\\) becomes more and more similar to the distribution of \\(X\\), which we often call the asymptotic distribution of \\(X_n\\) (other names include the large-sample distribution). If we know that \\(X_n \\indist X\\), then we can use the distribution of \\(X\\) as an approximation to the distribution of \\(X_n\\) and that distribution can be fairly accurate.\nOne of the most remarkable results in probability and statistics is that a large class of estimators will converge in distribution to one particular family of distributions: the normal. This is one reason that we study the normal so much and why investing in building intuition about it will pay off across many domains of applied work. We call this broad class of results the “central limit theorem,” (CLT) but it would probably be more accurate to refer to them as “central limit theorems” since much of statistics is devoted to showing the result in different settings. We now present the simplest CLT for the sample mean.\n\nTheorem 3.7 (Central Limit Theorem) Let \\(X_1, \\ldots, X_n\\) be i.i.d. r.v.s from a distribution with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i]\\). Then if \\(\\E[X_i^2] < \\infty\\), we have \\[\n\\frac{\\Xbar_n - \\mu}{\\sqrt{\\V[\\Xbar_n]}} = \\frac{\\sqrt{n}\\left(\\Xbar_n - \\mu\\right)}{\\sigma} \\indist \\N(0, 1).\n\\]\n\nIn words: the sample mean of a random sample from a population with finite mean and variance will be approximately normally distributed in large samples. Notice how we have not made any assumptions about the distribution of the underlying random variables, \\(X_i\\). They could binary, event count, continuous, anything. This means the CLT is incredibly broadly applicable.\n\n\n\n\n\n\nNotation alert\n\n\n\nWhy do we state the CLT in terms of the sample mean after centering and scaling by its standard error? If we don’t normalize the sample mean in this way, it’s difficult to talk about convergence in distribution because we know from the WLLN that \\(\\Xbar_n \\inprob \\mu\\) so in the limit the distribution of \\(\\Xbar_n\\) is concentrated at point mass around that value. Normalizing by centering and rescaling ensures that the variance of the resulting quantity will be fixed as a function of \\(n\\), so it makes sense to talk about its distribution converging. Sometimes you will see the equivalent result as \\[\n\\sqrt{n}\\left(\\Xbar_n - \\mu\\right) \\indist \\N(0, \\sigma^2).\n\\]\n\n\nWe can use this result to state approximations that we can use when discussing estimators such as \\[\n\\Xbar_n \\overset{a}{\\sim} N(\\mu, \\sigma^2/n),\n\\] where we use \\(\\overset{a}{\\sim}\\) to be “approximately distributed as in large samples.” This allow us to say things like: “in large samples, we should expect the sample mean to between within \\(2\\sigma/\\sqrt{n}\\) of the true mean in 95% of repeated samples.” As you might guess, this will be very important for hypothesis tests and confidence intervals! Estimators so often follow the CLT that we have an expression for this property.\n\nDefinition 3.5 An estimator \\(\\widehat{\\theta}_n\\) is asymptotically normal if for some \\(\\theta\\) \\[\n\\sqrt{n}\\left( \\widehat{\\theta}_n - \\theta \\right) \\indist N\\left(0,\\V[\\widehat{\\theta}_n]\\right).\n\\]\n\n\nExample 3.4 To illustrate how the CLT works, we can simulate the sampling distribution of the (normalized) sample mean at different sample sizes. Let \\(X_1, \\ldots, X_n\\) be iid samples from a Bernoulli with probability of success 0.25. We then draw repeated samples of size \\(n=30\\) and \\(n=100\\) and calculate \\(\\sqrt{n}(\\Xbar_n - 0.25)/\\sigma\\) for each random sample. Figure 3.2 plots the density of these two sampling distributions along with a standard normal reference. We can see that even at \\(n=30\\), the rough shape of the density looks normal, with spikes and valleys due to the discrete nature of the data (the sample mean can only take on 31 possible values in this case). By \\(n=100\\), the sampling distribution is very close to the true standard normal.\n\n\n\n\n\nFigure 3.2: Sampling distributions of the normalized sample mean at n=30 and n=100.\n\n\n\n\n\nThere are several properties of convergence in distribution that are helpful to us.\n\nTheorem 3.8 (Properties of convergence in distribution) Let \\(X_n\\) be a sequence of random variables \\(X_1,X_2,\\ldots\\) that converges in distribution to some rv \\(X\\) and let \\(Y_n\\) be a sequence of random variables \\(Y_1,Y_2,\\ldots\\) that converges in probability to some number, \\(c\\). Then,\n\n\\(g(X_n) \\indist g(X)\\) for all continuous functions \\(g\\).\n\\(X_nY_n\\) converges in distribution to \\(cX\\)\n\\(X_n + Y_n\\) converges in distribution to \\(X + c\\)\n\\(X_n / Y_n\\) converges in distribution to \\(X / c\\) if \\(c \\neq 0\\)\n\n\nThe last 3 of these results are sometimes referred to as Slutsky’s theorem. These results are very commonly used when trying to determine the asymptotic distribution of an estimator.\nOne important application of Slutsky’s theorem is when we replace the (unknown) popoulation variance in the CLT with an estimate. Recall the definition of the sample variance as \\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\Xbar_n)^2,\n\\] with the sample standard deviation defined as \\(s = \\sqrt{s^2}\\). It’s easy to show that these are consistent estimators for their respective population parameters \\[\ns^2 \\inprob \\sigma^2 = \\V[X_i], \\qquad s \\inprob \\sigma,\n\\] which by Slutsky’s theorem implies that \\[\n\\frac{\\sqrt{n}\\left(\\Xbar_n - \\mu\\right)}{s} \\indist \\N(0, 1)\n\\] Comparing this result to the statement of CLT, we see that replacing the population variance with a consistent estimate of the variance (or standard deviation) does not affect the asymptotic distribution.\nLike with the WLLN, the CLT holds for random vectors of sample means, where their centered and scaled versions converge to a multivariate normal distribution with a covariance matrix equal to covariance matrix of the underlying random vectors of data, \\(\\X_i\\).\n\nTheorem 3.9 If \\(\\mb{X}_i \\in \\mathbb{R}^k\\) are i.i.d. and \\(\\E\\Vert \\mb{X}_i \\Vert^2 < \\infty\\), then as \\(n \\to \\infty\\), \\[\n\\sqrt{n}\\left( \\overline{\\mb{X}}_n - \\mb{\\mu}\\right) \\indist \\N(0, \\mb{\\Sigma}),\n\\] where \\(\\mb{\\mu} = \\E[\\mb{X}_i]\\) and \\(\\mb{\\Sigma} = \\V[\\mb{X}_i] = \\E\\left[(\\mb{X}_i-\\mb{\\mu})(\\mb{X}_i - \\mb{\\mu})'\\right]\\).\n\nHere, notice that \\(\\mb{\\mu}\\) is the vector of population means for all the random variables in \\(\\X_i\\) and \\(\\mb{\\Sigma}\\) is the variance-covariance matrix for that vector.\n\n\n\n\n\n\nNote\n\n\n\nAs with the notation alert with the WLLN, we are using a shorthand here, \\(\\E\\Vert \\mb{X}_i \\Vert^2 < \\infty\\), which implies that \\(\\E[X_{ij}^2] < \\infty\\) for all \\(j = 1,\\ldots, k\\), or equivalently, that the variances of each variable in the sample means has finite variance."
  },
  {
    "objectID": "03_asymptotics.html#delta-method",
    "href": "03_asymptotics.html#delta-method",
    "title": "3  Asymptotics",
    "section": "3.8 Delta method",
    "text": "3.8 Delta method\nSuppose that we know that an estimator follows the CLT and so we have \\[\n\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta  \\right) \\indist \\N(0, V),\n\\] but we actually want to estimate \\(h(\\theta)\\) so we use the plug-in estimator, \\(h(\\widehat{\\theta}_n)\\). It seems like we should be able to apply part 1 of Theorem 3.8, the CLT established the large-sample distribution of the centered and scaled random sequence, \\(\\sqrt{n}(\\widehat{\\theta}_n - \\theta)\\), not to the original estimator itself like we would need to investigate the asymptotic distribution of \\(h(\\widehat{\\theta}_n)\\). We can use a little bit of calculus to get an approximation to the distribution we need.\n\nTheorem 3.10 If \\(\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta\\right) \\indist \\N(0, V)\\) and \\(h(u)\\) is continuously differentiable in a neighborhood around \\(\\theta\\), then as \\(n\\to\\infty\\), \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta}_n) - h(\\theta)  \\right) \\indist \\N(0, (h'(\\theta))^2 V).\n\\]\n\nIt’s useful to understand what’s happening here since it might help give intuition as to when this might go wrong. Why do we focus on continuously differentiable functions, \\(h()\\)? These are functions that can be well-approximated with a line in a neighborhood around a given point like \\(\\theta\\). In Figure 3.3, we show this where the tangent line at \\(\\theta_0\\), which has slope \\(h'(\\theta_0)\\), is very similar to \\(h(\\theta)\\) for values close to \\(\\theta_0\\). Because of this, we can approximate the difference between \\(h(\\widehat{\\theta}_n)\\) and \\(h(\\theta_0)\\) with the what this tangent line would give us: \\[\n\\underbrace{\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right)}_{\\text{change in } y} \\approx \\underbrace{h'(\\theta_0)}_{\\text{slope}} \\underbrace{\\left(\\widehat{\\theta}_n - \\theta_0\\right)}_{\\text{change in } x},\n\\] and then multiplying both sides by the \\(\\sqrt{n}\\) gives \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right) \\approx h'(\\theta_0)\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta_0\\right).\n\\] The right-hand side of this approximation converges to \\(h'(\\theta_0)Z\\), where \\(Z\\) is a random variable variable with \\(\\N(0, V)\\). The variance of this quantity will be \\[\n\\V[h'(\\theta_0)Z] = (h'(\\theta_0))^2\\V[Z] = (h'(\\theta_0))^2V,\n\\] by the properties of variances.\n\n\n\n\n\nFigure 3.3: Linear approximation to nonlinear functions\n\n\n\n\n\nExample 3.5 Let’s return to the iid sample \\(X_1, \\ldots, X_n\\) with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i]\\). From the CLT, we know that \\(\\sqrt{n}(\\Xbar_n - \\mu) \\indist \\N(0, \\sigma^2)\\). Suppose that we want to estimate \\(\\log(\\mu)\\) so we use the plug-in estimator \\(\\log(\\Xbar_n)\\) (assuming that \\(X_i > 0\\) for all \\(i\\) so that we can actually take the log). What is the asymptotic distribution of this estimator? This is a situation where \\(\\widehat{\\theta}_n = \\Xbar_n\\) and \\(h(\\mu) = \\log(\\mu)\\). From basic calculus we know that \\[\nh'(\\mu) = \\frac{\\partial \\log(\\mu)}{\\partial \\mu} = \\frac{1}{\\mu},\n\\] so applying the delta method, we can determine that \\[\n\\sqrt{n}\\left(\\log(\\Xbar_n) - \\log(\\mu)\\right) \\indist \\N\\left(0,\\frac{\\sigma^2}{\\mu^2} \\right).\n\\]\n\n\nExample 3.6 What about if we want to estimate the \\(\\exp(\\mu)\\) with \\(\\exp(\\Xbar_n)\\)? Recall that \\[\nh'(\\mu) = \\frac{\\partial \\exp(\\mu)}{\\partial \\mu} = \\exp(\\mu)\n\\] so applying the delta method, we have \\[\n\\sqrt{n}\\left(\\exp(\\Xbar_n) - \\exp(\\mu)\\right) \\indist \\N(0, \\exp(2mu)\\sigma^2),\n\\] since \\(\\exp(\\mu)^2 = \\exp(2\\mu)\\).\n\nLike all of the results in this chapter, there is a multivariate version of the delta method that is incredibly useful in practical applications. This is because we often will take two different estimators (or two different estimated parameters) and combine them to estimate another quantity. We now let \\(\\mb{h}(\\mb{\\theta}) = (h_1(\\mb{\\theta}), \\ldots, h_m(\\mb{\\theta}))\\) map from \\(\\mathbb{R}^k \\to \\mathbb{R}^m\\) and be continuously differentiable (we make the function bold since it ). It will help us use more compact matrix notation if we introduce a \\(m \\times k\\) Jacobian matrix of all partial derivatives \\[\n\\mb{H}(\\mb{\\theta}) = \\mb{\\nabla}_{\\mb{\\theta}}\\mb{h}(\\mb{\\theta}) = \\begin{pmatrix}\n  \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_k} \\\\\n  \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_k}\n\\end{pmatrix},\n\\] which we can use to generate the equivalent multivariate linear approximation \\[\n\\left(\\mb{h}(\\widehat{\\mb{\\theta}}_n) - \\mb{h}(\\mb{\\theta}_0)\\right) \\approx \\mb{H}(\\mb{\\theta}_0)'\\left(\\widehat{\\mb{\\theta}}_n - \\mb{\\theta}_0\\right).\n\\] We can use this fact to derive the multivariate delta method.\n\nTheorem 3.11 Suppose that \\(\\sqrt{n}\\left(\\widehat{\\mb{\\theta}}_n - \\mb{\\theta}_0 \\right) \\indist \\N(0, \\mb{\\Sigma})\\), then for any function \\(\\mb{h}\\) that is continuously differentiable in a neighborhood of \\(\\mb{\\theta}_0\\), we have \\[\n\\sqrt{n}\\left(\\mb{h}(\\widehat{\\mb{\\theta}}_n) - \\mb{h}(\\mb{\\theta}_0) \\right) \\indist \\N(0, \\mb{H}\\mb{\\Sigma}\\mb{H}'),\n\\] where \\(\\mb{H} = \\mb{H}(\\mb{\\theta}_0)\\).\n\nThis result follows from the approximation above plus rules about variances of random vectors. Remember that for any compatible matrix of constants, \\(\\mb{A}\\), we have \\(\\V[\\mb{A}'\\mb{Z}] = \\mb{A}\\V[\\mb{Z}]\\mb{A}'\\). You can see that the matrix of constants appears twice here, sort of like the matrix version of “squaring the constant” rule for variance.\nThe delta method is a very useful for generating closed-form approximations for asymptotic standard errors, but the math is often quite complex for even simple estimators. For applied researchers, it is usually more straightforward to use computational tools like the bootstrap to approximate the standard errors we need. This has the trade-off of taking more computational time to implement than the delta method, but is more easily adaptable across different estimators and domains with little human thinking time."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-lady-tasting-tea",
    "href": "04_hypothesis_tests.html#the-lady-tasting-tea",
    "title": "4  Hypothesis Tests",
    "section": "4.1 The lady tasting tea",
    "text": "4.1 The lady tasting tea\nThe lady tasting tea is an example of the core ideas behind hypothesis testing due to R.A. Fisher.1 Fisher had prepared a cup of tea for his colleague, the algologist Muriel Bristol. Knowing that she preferred milk in her tea, he poured milk into a tea cup and then poured the hot tea into the milk. Bristol rejected the cup, stating that she preferred the tea to be poured first, then milk. Fisher was apparently incredulous at the idea anyone could tell the difference between a cup poured milk-first or tea-first. So he and another colleague, William Roach, devised a test to see if Bristol had the ability to distinguish the two preparation methods.\nFor this test, Fisher and Roach prepared 8 cups of tea, 4 prepared milk-first and 4 prepared tea-first. They then presented the cups to Bristol in a random order (though she knew there were 4 of each type), and she proceeded to identify all of the cups correctly. At a first glance, this seems like good evidence that she can tell the difference between the two types, but a skeptic like Fisher raised the question: “could she have just been randomly guessing and got lucky?” This led Fisher to a statistical thought experiment: what would the probability of guessing all cups correctly if she were guessing randomly?\nTo calculate the probability of Bristol’s achievement, we can note that “randomly guessing” here would mean that she were selecting a group of 4 cups to be labeled milk-first from the 8 cups available. Using basic combinatorics, there are 70 ways to choose 4 cups among 8, but only 1 of those arrangements would be correct. Thus, if randomly guessing means choosing among those 70 options with equal chance, then the probability of guessing correctly is 1/70 or \\(\\approx 0.014\\). This probability being so low implies that the hypothesis of random guessing may be implausible.\nThe story of the lady tasting tea encapsulates many of the core elements of hypothesis testing. Hypothesis testing is about taking our observed estimate (Bristol guessing all the cups correctly) and seeing how likely that observed estimate would be under some assumption or hypothesis about the data generating process (Bristol was randomly guessing). When the observed estimate is very unlikely under the maintained hypothesis, we might view this as evidence against that hypothesis. Thus, hypothesis tests help us assess evidence for particular guesses about the DGP.\n\n\n\n\n\n\nNotation alert\n\n\n\nFor the rest of this chapter, we’ll introduce the concepts following the notation in the past chapters. We’ll usually assume that we have a random (iid) sample of random variables \\(X_1, \\ldots, X_n\\) from a distribution, \\(F\\). We’ll focus on estimating some parameter, \\(\\theta\\) of this distribution (like the mean, median, variance, etc). We’ll refer to \\(\\Theta\\) as the set of possible values of \\(\\theta\\), or the parameter space."
  },
  {
    "objectID": "04_hypothesis_tests.html#hypotheses",
    "href": "04_hypothesis_tests.html#hypotheses",
    "title": "4  Hypothesis Tests",
    "section": "4.2 Hypotheses",
    "text": "4.2 Hypotheses\nIn the context of hypothesis testing, hypotheses are just statements about the population distribution. In particular, we will make statements that \\(\\theta = \\theta_0\\) where \\(\\theta_0 \\in \\Theta\\) is the hypothesized value of \\(\\theta\\). Hypotheses are ubiquitous in empirical work, but here are some examples to give you a flavor:\n\nThe population proportion of US citizens that identify as Democrats is 0.33.\nThe population difference in average voter turnout between households who received receiving get-out-the-vote mailers vs those who did not is 0.\nThe difference in average incidence of human rights abuse in countries that signed a human rights treaty vs those countries that did not sign is 0.\n\nEach of these is a statement about the true DGP. The latter two are very common: when \\(\\theta\\) represents the difference in means between two groups, then \\(\\theta = 0\\) is the hypothesis of no true difference in population means, or no treatment effect (if the causal effect is identified).\nThe goal of hypothesis testing is to adjudicate between two complementary hypotheses.\n\nDefinition 4.1 The two hypotheses in a hypothesis test are called the null hypothesis and the alternative hypothesis, denoted as \\(H_0\\) and \\(H_1\\), respectively.\n\nThese hypotheses are complementary, so that if the null hypothesis \\(H_0: \\theta \\in \\Theta_0\\), then the alternative hypothesis is \\(H_1: \\theta \\in \\Theta_0^c\\). The “null” in null hypothesis might seem odd until you realize that most hypotheses being tested are that there is no effect of some treatment or no difference in means. For example, suppose \\(\\theta\\) is the difference in mean support for expanding legal immigration between a treatment group that received a pro-immigrant message along with some facts about immigration and a control group that just received the factual information. Then, the typical null hypothesis would be no difference in means or \\(H_0: \\theta = 0\\) and the alternative would be \\(H_1: \\theta \\neq 0\\).\nThere are two types of tests that differ by the form of their null and alternative hypotheses. A two-sided test is of the form \\[\nH_0: \\theta = \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta \\neq \\theta_0,\n\\] where the “two-sided” part refers to how the alternative contains values of \\(\\theta\\) above and below the null value \\(\\theta_0\\). A one-sided test has the form \\[\nH_0: \\theta \\leq \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta > \\theta_0,\n\\] or \\[\nH_0: \\theta \\geq \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta < \\theta_0.\n\\] Two-sided tests are much more common in the social science where we want to know if there is any evidence, positive or negative, against the presumption of no treatment effect or no relationship between two variables. One-sided tests are for situations where we only want evidence in one direction, which is rarely relevant to social science research. One-sided tests also have the downside of being misused to inflate the strength of evidence against the null and so should be avoided. Unfortunately, the math of two-sided tests are also more complicated."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-procedure-of-hypothesis-testing",
    "href": "04_hypothesis_tests.html#the-procedure-of-hypothesis-testing",
    "title": "4  Hypothesis Tests",
    "section": "4.3 The procedure of hypothesis testing",
    "text": "4.3 The procedure of hypothesis testing\nAt the most basic level, a hypothesis test is a rule that specifies values of the sample data for which we will decide to reject the null hypothesis. Let \\(\\mathcal{X}_n\\) be the range of the sample—that is, all possible vectors \\((x_1, \\ldots, x_n)\\) that have positive probability of occurring. Then, a hypothesis test describes a region of this space, \\(R \\subset \\mathcal{X}_n\\), called the rejection region where when \\((X_1, \\ldots, X_n) \\in R\\) we will reject \\(H_0\\) and when the data is outside this region, \\((X_1, \\ldots, X_n) \\notin R\\) we retain, accept, or fail to reject the null hypothesis.2\nHow do we decide what the rejection region should be? Even though we define the rejection region in terms of the sample space, \\(\\mathcal{X}_n\\), it’s unwieldy to work with the entire vector of data. Instead, we often formulate the rejection region in terms of a test statistic, \\(T = T(X_1, \\ldots, X_n)\\), where the rejection region becomes \\[\nR = \\left\\{(x_1, \\ldots, x_n) : T(x_1, \\ldots, x_n) > c\\right\\},\n\\] where \\(c\\) is called the critical value. In words, this says that the rejection region are the parts of the sample space that make the test statistic sufficiently large. We reject null hypotheses when the observed data is incompatible with those hypotheses, where the test statistic should be a measure of this incompatibility. Note that the test statistic is a random variable and has a distribution—we will exploit this to understand the different properties of a hypothesis test.\n\nExample 4.1 Suppose that \\((X_1, \\ldots, X_n)\\) measure whether a sample of US citizens support the current US president (\\(X_i = 1\\)) or not (\\(X_i = 0\\)). We might be interested in the test of the null hypothesis that the president does not have the support of a majority of American citizens. Let \\(\\mu = \\E[X_i] = \\P(X_i = 1)\\). Then, a one-sided test would compare the two hypotheses \\[\nH_0: \\mu \\leq 0.5 \\quad\\text{versus}\\quad H_1: \\mu > 0.5.\n\\] In this case, we might use the sample mean as the test statistic, so that \\(T(X_1, \\ldots, X_n) = \\Xbar_n\\) and we have to find some threshold above 0.5 such that we would reject the null, \\[\nR = \\left\\{(x_1, \\ldots, x_n): \\Xbar_n > c\\right\\}.\n\\] In words, how much support should we see for the current president before we decide to reject the notion that they lack majority support. Below we are going to select the critical value, \\(c\\), to have nice statistical properties.\n\nThe structure of a reject region will depend on whether a test is one- or two-sided. One-sided tests will take the form \\(T > c\\), whereas two-sided tests will take the form \\(|T| > c\\), since we want to count deviations from either side of the null hypothesis as evidence against that null."
  },
  {
    "objectID": "04_hypothesis_tests.html#testing-errors",
    "href": "04_hypothesis_tests.html#testing-errors",
    "title": "4  Hypothesis Tests",
    "section": "4.4 Testing errors",
    "text": "4.4 Testing errors\nIf we are making a decision about whether to reject a null hypothesis or not, it is possible that we will make the incorrect decision. In particular, there are two ways for us to make errors and two ways for us to be correct in this setting, as shown in Table 4.1. The labels are confusing, but it’s helpful to remember that type I errors (said “type one”) are labelled so because they are the worse of the two types of errors. This is when we reject a null (say there is a true treatment effect or relationship) when in fact the null is true (there is no true treatment effect or relationship). Type I errors are what we see in the replication crisis: lots of “significant” effects that turn out later to be null. Type II errors (said “type two”) are considered less problematic: there is a true relationship, but we cannot detect it with our test (cannot reject the null).\n\n\nTable 4.1: Typology of testing errors\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nRetain \\(H_0\\)\nAwesome\nType II error\n\n\nReject \\(H_0\\)\nType I error\nGreat\n\n\n\n\nIdeally, we would minimize the chances that we make either a type I or type II error. Unfortunately, because the test statistic is a random variable, we cannot remove the probability of an error completely. Instead, we will try to derive tests that have some guaranteed performance in terms of minimizing the probability of type I error. To derive this, we can define the power function of a test, \\[\n\\pi(\\theta) = \\P\\left(  \\text{Reject } H_0 \\mid \\theta \\right) = \\P\\left( T \\in R \\mid \\theta \\right),\n\\] which is the probability of rejection as a function of the parameter of interest, \\(\\theta\\). This tells us, for example, how likely we are to reject the null of no treatment effect as we vary the true size of the treatment effect.\nFrom the power function we can define the probability of type I error.\n\nDefinition 4.2 The size of a hypothesis test with the null hypothesis \\(H_0: \\theta = \\theta_0\\) is \\[\n\\pi(\\theta_0) = \\P\\left( \\text{Reject } H_0 \\mid \\theta_0 \\right).\n\\]\n\nYou can think of the size of a test as the rate of false positives (or false discoveries) produced by the test. Figure 4.1 shows an example of rejection regions, size, and power for a one-sided test. In the left panel, we have the distribution of the test statistic under the null, with \\(H_0: \\theta = \\theta_0\\) and the rejection region is defined by values \\(T > c\\). The shaded grey region is the probability of rejection under this null hypothesis, or the size of the test. Sometimes by random chance we will get samples that are extreme even under the null, leading to false discoveries.3\nIn the right panel, we overlay the distribution of the test statistic under one particular alternative, \\(\\theta = \\theta_1 > \\theta_0\\). The red shaded region is the probability of rejecting the null when this alternative is true, or the power—it’s the probability of correctly rejecting the null when it is false. Intuitively, we can see that alternatives that produces test statistics closer to the rejection region will have higher power. This makes sense: it should be easier to detect big deviations from the null than small deviations from the null.\n\n\n\n\n\nFigure 4.1: Size of a test and power against an alternative\n\n\n\n\nFigure 4.1 also hints at a tradeoff between size and power. Notice that we could make the size smaller (lower the false positive rate) by increasing the critical value to \\(c' > c\\). This would make the probability of being in the rejection region smaller, \\(\\P(T > c' \\mid \\theta_0) < \\P(T > c \\mid \\theta_0)\\), leading to a lower sized test. Unfortunately, it would also reduce power in the right panel since the probability of being in the rejection region will be lower under any alternative, \\(\\P(T > c' \\mid \\theta_1) < \\P(T > c \\mid \\theta_1)\\). This means we usually cannot simultaneously reduce both types of errors at the same time."
  },
  {
    "objectID": "04_hypothesis_tests.html#determining-the-rejection-region",
    "href": "04_hypothesis_tests.html#determining-the-rejection-region",
    "title": "4  Hypothesis Tests",
    "section": "4.5 Determining the rejection region",
    "text": "4.5 Determining the rejection region\nIf we cannot simultaneously optimize both the size and power of a test, how should we determine where the reject region is? That is, how should we determine what empirical evidence will be strong enough for us to reject the null? The standard approach to this problem in hypothesis testing is to control the size of a test (that is, control the rate of false positives) and try to maximize the power of the test subject to that constraint. So we say “I’m willing to accept at most x%” of findings will be false positives and do whatever we can to maximize power subject to that constraint.\n\nDefinition 4.3 A test has significance level \\(\\alpha\\) if its size is less than or equal to \\(\\alpha\\), or \\(\\pi(\\theta_0) \\leq \\alpha\\).\n\nA test with a significance level of \\(\\alpha = 0.05\\) means that the test will have a false positive/type I error rate no larger than 0.05. This is an extremely common level for tests in the social science, though you also will \\(\\alpha = 0.01\\) or \\(\\alpha = 0.1\\). Frequentists justify this by saying this means that with \\(\\alpha = 0.05\\), there will only 5% of studies that will produce false discoveries.\nOur task, then, is to construct the rejection region so that the null distribution of the test statistic \\(G_0(t) = \\P(T \\leq t \\mid \\theta_0)\\) to have less than \\(\\alpha\\) probability in that region. One-sided tests like in Figure 4.1 are easiest to show this even though we have warned that you shouldn’t use them. We want to choose \\(c\\) that puts no more than \\(\\alpha\\) probability in the tail, or \\[\n\\P(T > c \\mid \\theta_0) = 1 - G_0(c) \\leq \\alpha.\n\\] Remembering that the smaller the value of \\(c\\) we can use will maximize power, which implies that the critical value for the maximum power while maintaining the significance level is when \\(1 - G_0(c) = \\alpha\\). We can use the quantile function of the null distribution to find the exact value of \\(c\\) we need, \\[\nc = G^{-1}_0(1 - \\alpha),\n\\] which is just fancy math to say “the value at which \\(1-\\alpha\\) of the null distribution is below.”\nThe determination of the rejection region follows the same principles with two-sided tests, but it is slightly more complicated because typically need to reject when the magnitude of the test statistic is large, \\(|T| > c\\). Figure 4.2 shows that basic setup. Notice that because there are two (disjoint) regions, we can write the size (false positive rate) as \\[\n\\pi(\\theta_0) = G_0(-c) + 1 - G_0(c)\n\\] In most cases that we will see, the null distribution for such a test will be symmetric around 0 (usually asymptotically standard normal, actually), which means that \\(G_0(-c) = 1 - G_0(c)\\), which implies that the size is \\[\n\\pi(\\theta_0) = 2(1 - G_0(c)).\n\\] Solving for the critical value that would make this \\(\\alpha\\) gives \\[\nc = G^{-1}_0(1 - \\alpha/2).\n\\] Again, this formula can seem dense, but just remember what you are doing: finding the value that puts \\(\\alpha/2\\) of the probability of the null distribution in each of the tails.\n\n\n\n\n\nFigure 4.2: Rejection regions for a two-sided test."
  },
  {
    "objectID": "04_hypothesis_tests.html#hypothesis-tests-of-the-sample-mean",
    "href": "04_hypothesis_tests.html#hypothesis-tests-of-the-sample-mean",
    "title": "4  Hypothesis Tests",
    "section": "4.6 Hypothesis tests of the sample mean",
    "text": "4.6 Hypothesis tests of the sample mean\nLet’s go through an extended example about hypothesis testing of a sample mean, sometimes called a one-sample test. Let’s say \\(X_i\\) are feeling thermometer scores about “liberals” as a group on a scale of 0 to 100, with values closer to 0 indicating cooler feelings about liberals and values closer to 100 indicating warmer feelings about liberals. We might want to know if the true population average is different from a neutral value of 50. We can write this two-sided test as \\[\nH_0: \\mu = 50 \\quad\\text{versus}\\quad H_1: \\mu \\neq 50,\n\\] where \\(\\mu = \\E[X_i]\\). The standard test statistic for this type of test is the so-called t-statistic, \\[\nT = \\frac{\\left( \\Xbar_n - \\mu_0 \\right)}{\\sqrt{s^2 / n}} =\\frac{\\left( \\Xbar_n - 50 \\right)}{\\sqrt{s^2 / n}},\n\\] where \\(\\mu_0\\) is the null value of interest and \\(s^2\\) is the sample variance. If the null hypothesis is true, then by the CLT we know that the t-statistic is asympotically normal, \\(T \\indist \\N(0, 1)\\). Thus, we know that the null distribution can be approximated with standard normal!\nLet’s say that we want to create a test with level \\(\\alpha = 0.05\\). Then we need to find the rejection region that puts \\(0.05\\) probability in the tails of the null distribution, which we just saw was \\(\\N(0,1)\\). Let \\(\\Phi()\\) be the CDF for the standard normal and let \\(\\Phi^{-1}()\\) be the quantile function for the standard normal. Drawing on what we developed above, you can find the value \\(c\\) so that \\(\\P(|T| > c \\mid \\mu_0)\\) is 0.05 with \\[\nc = \\Phi^{-1}(1 - 0.05/2) \\approx 1.96,\n\\] which means that a test where we reject when \\(|T| > 1.96\\) would have a level that will be 0.05 asymptotically."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-wald-test",
    "href": "04_hypothesis_tests.html#the-wald-test",
    "title": "4  Hypothesis Tests",
    "section": "4.7 The Wald test",
    "text": "4.7 The Wald test\nWe can generalize the hypothesis test for the sample mean to estimators more broadly. Let \\(\\widehat{\\theta}_n\\) be an estimator for some parameter \\(\\theta\\) and let \\(\\widehat{\\textsf{se}}[\\widehat{\\theta}_n]\\) be a consistent estimate of the standard error of the estimator, \\(\\textsf{se}[\\widehat{\\theta}_n] = \\sqrt{\\V[\\widehat{\\theta}_n]}\\). We consider the two-sided test \\[\nH_0: \\theta = \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta \\neq \\theta_0\n\\]\nIn many cases, our estimators will be asymptotically normal by a version of the CLT. This means that under the null hypothesis, we will have \\[\nT = \\frac{\\widehat{\\theta}_n - \\theta_0}{\\widehat{\\textsf{se}}[\\widehat{\\theta}_n]} \\indist \\N(0, 1).\n\\] The Wald test rejects \\(H_0\\) when \\(|T| > z_{\\alpha/2}\\), where \\(z_{\\alpha/2}\\) that puts \\(\\alpha/2\\) in the upper tail of the standard normal. That is, if \\(Z \\sim \\N(0, 1)\\), then \\(z_{\\alpha/2}\\) satisfies \\(\\P(Z \\geq z_{\\alpha/2}) = \\alpha/2\\).\n\n\n\n\n\n\nNote\n\n\n\nIn R, you can find the \\(z_{\\alpha/2}\\) values easily with the qnorm() function:\n\nqnorm(0.05 / 2, lower.tail = FALSE)\n\n[1] 1.959964\n\n\n\n\n\nTheorem 4.1 Asymptotically, the Wald test has size \\(\\alpha\\) such that \\[\n\\P(|T| > z_{\\alpha/2} \\mid \\theta_0) \\to \\alpha.\n\\]\n\nThis is a very general result and it means that many, many hypothesis tests based on estimators will have the same form. The main difference across estimators will be the exact way that we calculate the estimated standard error.\n\nExample 4.2 (Difference in proportions) In get-out-the-vote (GOTV) experiments, we might randomly assign a group of citizens to receive mailers that encourage them to vote, whereas a control group receives no message. We’ll define the turnout variables in the treatment group \\(Y_{1}, Y_{2}, \\ldots, Y_{n_t}\\) as iid draws from a Bernoulli distribution with success \\(p_t\\) which represents the population turnout rate among treated citizens. The outcomes in the control group \\(X_{1}, X_{2}, \\ldots, X_{n_c}\\) are iid draws from another Bernoulli distribution with success \\(p_c\\), which represents the population turnout rate among citizens not receiving a mailer.\nOur goal is to learn about the treatment effect of this treatment on whether or not the citizen votes, \\(\\tau = p_t - p_c\\) and we will use the sample difference in means/proportions as our estimator, \\(\\widehat{\\tau} = \\Ybar - \\Xbar\\). To perform a Wald test, we need to know/estimate the standard error of this estimator. Notice that because these are independent samples, the variance is \\[\n\\V[\\widehat{\\tau}_n] =  \\V[\\Ybar - \\Xbar] = \\V[\\Ybar] + \\V[\\Xbar] = \\frac{p_t(1-p_t)}{n_t} + \\frac{p_c(1-p_c)}{n_c},\n\\] where the third equality comes from the fact that the underlying outcome variables \\(Y_i\\) and \\(X_j\\) are binary. Obviously we do not know the true population proportions \\(p_t\\) and \\(p_c\\) (that’s why we’re doing the test!), but we can estimate the standard error by replacing them with their estimates \\[\n\\widehat{\\textsf{se}}[\\widehat{\\tau}] = \\sqrt{\\frac{\\Ybar(1 -\\Ybar)}{n_t} + \\frac{\\Xbar(1-\\Xbar)}{n_c}}.\n\\]\nThe typical null hypothesis test in this case is “no treatment effect” vs “some treatment effect” or \\[\nH_0: \\tau = p_t - p_c = 0 \\quad\\text{versus}\\quad H_1: \\tau \\neq 0,\n\\] which gives the following test statistic for the Wald test \\[\nT = \\frac{\\Ybar - \\Xbar}{\\sqrt{\\frac{\\Ybar(1 -\\Ybar)}{n_t} + \\frac{\\Xbar(1-\\Xbar)}{n_c}}}.\n\\] If we wanted a test with level \\(\\alpha = 0.01\\), we would reject the null when \\(|T| > 2.58\\) since\n\nqnorm(0.01/2, lower.tail = FALSE)\n\n[1] 2.575829\n\n\n\n\nExample 4.3 (Difference in means) Let’s take a similar setting to the last example with randomly assigned treatment and control groups, but now the treatment is an appeal for donations and the outcomes are continuous measures of how much a person donated to the political campaign. Now the treatment data \\(Y_1, \\ldots, Y_{n_t}\\) are iid draws from a population with mean \\(\\mu_t = \\E[Y_i]\\) and population variance \\(\\sigma^2_t = \\V[Y_i]\\). The control data \\(X_1, \\ldots, X_{n_c}\\) are iid draws (independent of the \\(Y_i\\)) from a population with mean \\(\\mu_c = \\E[X_i]\\) and population variance \\(\\sigma^2_c = \\V[X_i]\\). The parameter of interest is similar to before: the population difference in means, \\(\\tau = \\mu_t - \\mu_c\\), and we’ll form the usual hypothesis test of \\[\nH_0: \\tau = \\mu_t - \\mu_c = 0 \\quad\\text{versus}\\quad H_1: \\tau \\neq 0.\n\\]\nThe only difference between this setting and the difference in proportions is the standard error here will be different because we cannot rely on the Bernoulli. Instead, we’ll use our knowledge of the sampling variance of the sample means and independence between the samples to derive \\[  \n\\V[\\widehat{\\tau}] = \\V[\\Ybar] + \\V[\\Xbar] = \\frac{\\sigma^2_t}{n_t} + \\frac{\\sigma^2_c}{n_c},\n\\] where we can come up with an estimate of the unknown population variance with sample variances \\[  \n\\widehat{\\se}[\\widehat{\\tau}] = \\sqrt{\\frac{s^2_t}{n_t} + \\frac{s^2_c}{n_c}}.\n\\] This leads to the Wald test statistic of \\[\nT = \\frac{\\widehat{\\tau} - 0}{\\widehat{\\se}[\\widehat{\\tau}]} = \\frac{\\Ybar - \\Xbar}{\\sqrt{\\frac{s^2_t}{n_t} + \\frac{s^2_c}{n_c}}},\n\\] and if want an asymptotically level of 0.05, we can reject when \\(|T| > 1.96\\)."
  },
  {
    "objectID": "04_hypothesis_tests.html#p-values",
    "href": "04_hypothesis_tests.html#p-values",
    "title": "4  Hypothesis Tests",
    "section": "4.8 p-values",
    "text": "4.8 p-values\nThe hypothesis testing framework is clearly designed for actually making a decision in the face of uncertainty. You choose a level of wrongness you are comfortable with (rate of false positives) and then make a decision null vs alternative based firmly on the rejection region. This has the downside that when we’re not really making a decision, we are somewhat artificially discarding information about the strength of evidence. We “accept” the null if \\(T = 1.95\\) in the last example but reject it if \\(T = 1.97\\) even though these two situations are actually very similar. Just reporting the reject/retain decision also fails to give us a sense of at what other levels we might have rejected the null. Again, this makes sense if we need to make a single decision: other tests don’t matter because we carefully considered our \\(\\alpha\\) level test. But in the lower stakes world of the academic social sciences, we can afford to be more informative.\nOne alternative to reporting the reject/retain decision is to report a p-value.\n\nDefinition 4.4 The p-value of a test is the probability of observing a test statistic is at least as extreme as the observed test statistic in the direction of the alternative hypothesis.\n\nThe line about “in the direction of the alternative hypothesis” deals with the unfortunate headache of one-sided versus two-sided tests. For a one-sided test where larger values of \\(T\\) correspond to more evidence for \\(H_1\\), the p-value is \\[\n\\P(T(X_1,\\ldots,X_n) > T \\mid \\theta_0) = 1 - G_0(T),\n\\] whereas for a (symmetric) two-sided test we have \\[\n\\P(|T(X_1, \\ldots, X_n)| > |T| \\mid \\theta_0) = 2(1 - G_0(|T|)).\n\\]\nIn either case, the interpretation of the p-value is the same. It is the smallest size \\(\\alpha\\) at which a test would reject null. Presenting a p-value allows the reader to determine their own \\(\\alpha\\) level and determine quickly if the evidence would warrant rejecting \\(H_0\\) in that case. Thus, the p-value is a more continuous measure of evidence against the null, where lower values are stronger evidence against the null because the observed result is less likely under the null.\nThere is quite a bit of controversy surrounding p-values but most of it focuses on arbitrary p-value cutoffs for determining statistical significance and sometimes publication decisions. This really isn’t the fault of p-values, but rather the hyperfixation on the reject/retain decision for arbitrary test levels like \\(\\alpha = 0.05\\). It might be best to view p-values as a transformation of the test statistic onto a common scale between 0 and 1.\n\n\n\n\n\n\nWarning\n\n\n\nThere are many statistical shibboleths that people use to purportedly identify people that don’t understand statistics and they usually hinge on seemingly subtle differences in interpretation that are easy to miss. If you understand the core concepts, the statistical shibboleths tend to be overblown, but it would be malpractice not flag them for you.\nThe shibboleth with p-values is that sometimes people will interpret them as “the probability that the null hypothesis is true.” Of course, this doesn’t make sense from our definition because the p-values conditions on the null hypothesis—it cannot tell us anything about the probability of that null hypothesis. Instead, the metaphor you should always carry is that hypothesis tests are statistical thought experiments and that p-values answer the question: how likely would my data be if the null were true?"
  },
  {
    "objectID": "04_hypothesis_tests.html#power-analysis",
    "href": "04_hypothesis_tests.html#power-analysis",
    "title": "4  Hypothesis Tests",
    "section": "4.9 Power analysis",
    "text": "4.9 Power analysis\nImagine you have spent a large research budget on a big experiment to test your amazing theory and the results come back and… you fail to reject the null of no treatment effect. When this happens, there are two possible states of the world: the null is true and you correctly identified that, or the null is false but the test had lower power to detect the true effect. Because of this uncertainty after the fact, it is common for researchers to conduct power analyses prior to running studies that try to forecast what sample size is necessary to ensure you will be able to reject the null under a hypothesized effect size.\nGenerally power analyses involve calculating the power function \\(\\pi(\\theta) = \\P(T(X_1, \\ldots, X_n) \\in R \\mid \\theta)\\) for different values of \\(\\theta\\). It might also involve sample size calculations for a particular alternative, \\(\\theta_1\\). In that case, we try to find the sample size \\(n\\) that would make the power \\(\\pi(\\theta_1)\\) as close to a particular value (often 0.8) as possible. In simple one-sided tests, it is possible to solve for this sample size explicitly, but for more general situations or two-sided tests, we typically need to use numerical or simulation-based approaches to finding the optimal sample size.\nWith Wald tests, we can characterize the power function quite easily even if it does not allow us to back out sample size calculations easily.\n\nTheorem 4.2 For a Wald test with an asymptotically normal estimator, the power function for a particular alternative \\(\\theta_1 \\neq \\theta_0\\) is \\[\n\\pi(\\theta_1) = 1 - \\Phi\\left( \\frac{\\theta_0 - \\theta_1}{\\widehat{\\se}[\\widehat{\\theta}_n]} + z_{\\alpha/2} \\right) + \\Phi\\left( \\frac{\\theta_0 - \\theta_1}{\\widehat{\\se}[\\widehat{\\theta}_n]}-z_{\\alpha/2} \\right).\n\\]"
  },
  {
    "objectID": "04_hypothesis_tests.html#exact-tests-under-normal-data",
    "href": "04_hypothesis_tests.html#exact-tests-under-normal-data",
    "title": "4  Hypothesis Tests",
    "section": "4.10 Exact tests under normal data",
    "text": "4.10 Exact tests under normal data\nThe Wald test above relies on large sample approximations. In finite samples, these approximation may not be valid. Is it possible to get exact inferences at any sample size? Yes, if we make stronger assumptions about the data. In particular, assume a parametric model for the data where \\(X_1,\\ldots,X_n\\) are i.i.d. samples from \\(N(\\mu,\\sigma^2)\\). Under null of \\(H_0: \\mu = \\mu_0\\), we can show that \\[\nT_n = \\frac{\\Xbar_n - \\mu_0}{s_n/\\sqrt{n}} \\sim t_{n-1},\n\\] where $t_{n-1} is the $Student’s t-distribution with \\(n-1\\) degrees of freedom. This implies the null distribution is \\(t\\) so we use quantiles of \\(t\\) for critical values. For one-sided test \\(c = G^{-1}_0(1 - \\alpha)\\) but now \\(G_0\\) is \\(t\\) with \\(n-1\\) df and so we use qt() instead of qnorm() to calculate these critical values.\nThe critical values for the \\(t\\) distribution are always larger than the normal because the t has fatter tails as shown in Figure 4.3. As \\(n\\to\\infty\\), however, the \\(t\\) converges to the standard normal and so it is asymptotically equivalent to the Wald test but slightly more conservative in finite samples. Oddly, most software packages calculate p-values and rejection regions based on the \\(t\\) to take advantage of this conservativeness.\n\n\n\n\n\nFigure 4.3: Normal versus t distribution\n\n\n\n\n\n\n\n\nSenn, Stephen. 2012. “Tea for Three: Of Infusions and Inferences and Milk in First.” Significance 9 (6): 30–33. https://doi.org/https://doi.org/10.1111/j.1740-9713.2012.00620.x."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "$$\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\boldsymbol}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\b}{\\mb{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\mb{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\n\nSenn, Stephen. 2012. “Tea for Three: Of Infusions and Inferences\nand Milk in First.” Significance 9 (6): 30–33.\nhttps://doi.org/https://doi.org/10.1111/j.1740-9713.2012.00620.x."
  }
]