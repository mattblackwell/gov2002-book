[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A User’s Guide to Statistical Inference and Regression",
    "section": "",
    "text": "$$\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\bfbeta}{\\bs{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\bs{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\n\nPreface\nThis is a set of notes for Government 2002: Quantitative Social Science Methods II at Harvard University taught by Matthew Blackwell. The goal of this text is to provide a rigorous yet accessible introduction to the foundational topics in statistical inference with a special application to linear regression, a workhorse tool in the social sciences. The material is intended for first-year PhD students in political science, but it may be of interest more broadly. Much of the material has been adopted from various sources (far too many to recount now), but this book is especially indebted to the following texts:\n\nHansen, Bruce. Probability & Statistics for Economists. Princeton University Press.\nHansen, Bruce. Econometrics. Princeton University Press.\nWasserman, Larry. All of Statistics: A Concise Course in Statistical Inference. Springer.\nWooldridge, Jeffrey. Econometric Analysis of Cross Section and Panel Data. The MIT Press.\n\nYou can find the source for this book at https://github.com/mattblackwell/gov2002-book. Any typos or errors can be reported at https://github.com/mattblackwell/gov2002-book/issues. Thanks for reading.\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\n\\(\\,\\)\n\n\nAcknowledgements\nMuch of how I approach this material comes from Adam Glynn, for whom I was a teaching fellow during graduate school. Thanks to the students of Gov 2000 and Gov 2002 over years for helping me refine the material in this book. Also very special thanks to those who have provided valuable feedback including Zeki Akyol, Noah Dasanaike, and Jarell Cheong Tze Wen."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "$$\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\bfbeta}{\\bs{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\bs{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\\(\\,\\)\nThis book, like so many books before it, will try to teach you statistics. The field of statistics describes how we learn about the world from quantitative data. In the social sciences, the vast majority of empirical studies use statistical methods to provide evidence for their arguments. While it is possible to conduct quantitative research without understanding statistics, one must advise against it. Quantitative research involves a host of choices about what model to use, what variables to include, what tuning parameters to set, what assumptions to make, and so on. Without a deep understanding of statistics, you will find these choices bewildering and often yield to the default settings of your statistical software. The goal of this book is to give you the foundation to confidently make those choices for your specific application.\nWe will focus on two key goals in this book.\n\nUnderstand the basic ways to assess estimators With quantitative data, we often want to make statistical inferences about some unknown feature of the world. We use estimators (which are just ways of summarizing our data) to estimate these features. One major goal of this book is to show the basics of this task at a general enough level to be applicable to almost any estimator that you are likely to encounter in research. The ideas of bias, sampling variance, consistency, and asymptotic normality are common to such a large swath of (frequentist) inference that you get a tremendous return on your investment of time in these topics. Understand these core ideas and you will have a language to analyze any fancy new estimator that pops up in the next few decades.\nApply these ideas to estimation of regressions This book will apply these ideas to one particular workhorse task in the social sciences: estimating regression functions. So many methods either use regression estimators like ordinary least squares or extend it in some way. Understanding how these estimators work is vital for conducting research in the social sciences. Regression and regression estimators also provide an entry point for discussing parametric models explicitly as approximation and projections rather than as rigid assumptions about the truth of a given specification.\n\nWhy write a book on statistics and regression when so many already exist? Aside from hubris, my goal in this book is to find a level of mathematical sophistication that will challenge and push political scientists to develop stronger foundations in the material. While some textbooks at this level exist in statistics and economics, they tend to focus on applications less relevant to political science. This book attempts to correct this."
  },
  {
    "objectID": "02_estimation.html#introduction",
    "href": "02_estimation.html#introduction",
    "title": "2  Estimation",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWhen studying probability, we assumed that we knew the parameter of a distribution (the mean or the variance) and used probability theory to understand what kind of data we would observe. Estimation and inference put this engine in reverse and try to learn some aspect of the data-generating process using only our observed data. There are two main goals here: estimation, which is how we formulate our best guess about a parameter of the DGP, and inference, which is how we formalize and express uncertainty about our estimates.\n\n\nExample 2.1 (Randomized control trial) Suppose we are conducting a randomized experiment on framing effects. All respondents receive some factual information about current levels of immigration. The message for the treatment group (\\(D_i = 1\\)) has an additional framing of the positive benefits of immigration, while the control group (\\(D_i = 0\\)) receives no additional framing. The outcome is a binary outcome on whether the respondent supports increasing legal immigration limits (\\(Y_i = 1\\)) or not (\\(Y_i = 0\\)). The observed data consists of \\(n\\) pairs of random variables, the outcome, and the treatment assignment: \\(\\{(Y_1, D_1), \\ldots, (Y_n, D_n)\\}\\). Define the two sample means/proportions in each group as \\[\n\\Ybar_1 = \\frac{1}{n_1} \\sum_{i: D_i = 1} Y_i, \\qquad\\qquad \\Ybar_0 = \\frac{1}{n_0} \\sum_{i: D_i = 0} Y_i,\n\\] where \\(n_1 = \\sum_{i=1}^n D_i\\) is the number of treated units and \\(n_0 = n - n_1\\) is the number of control units.\nA standard estimator for the treatment effect in a study like this would be the difference in means, \\(\\Ybar_1 - \\Ybar_0\\). But this is only one possible estimator. We could also estimate the effect by taking this difference in means separately by party identification and then averaging those party-specific effects by the size of those groups. This estimator is commonly called a poststratification estimator, but it’s unclear at first glance which of these two estimators we should prefer.\n\nWhat are the goals of studying estimators? In short, we prefer to use good estimators rather than bad estimators. But what makes an estimator good or bad? You probably have some intuitive sense that, for example, an estimator that always returns the value 3 is bad. Still, it will be helpful for us to formally define and explore some properties of estimators that will allow us to compare them and choose the good over the bad."
  },
  {
    "objectID": "02_estimation.html#samples-and-populations",
    "href": "02_estimation.html#samples-and-populations",
    "title": "2  Estimation",
    "section": "2.2 Samples and populations",
    "text": "2.2 Samples and populations\nFor most of this class, we’ll focus on a relatively simple setting where we have a set of random vectors \\(\\{X_1, \\ldots, X_n\\}\\) that are independent and identically distributed (iid) draws from a distribution with cumulative distribution function (cdf) \\(F\\). They are independent in that information about any subset of random vectors is not informative about any other subset of random vectors, or, more formally, \\[\nF_{X_{1},\\ldots,X_{n}}(x_{1}, \\ldots, x_{n}) = F_{X_{1}}(x_{1})\\cdots F_{X_{n}}(x_{n}),\n\\] where \\(F_{X_{1},\\ldots,X_{n}}(x_{1}, \\ldots, x_{n})\\) is the joint cdf of the random vectors and \\(F_{X_{j}}(x_{j})\\) is the marginal cdf of the \\(j\\)th random vector. They are “identically distributed” in the sense that each of the random variables \\(X_i\\) have the same marginal distribution, \\(F\\).\nYou can think of each vector, \\(X_i\\), as the rows in your data frame. Note that we’re being purposely vague about this cdf—it simply represents the unknown distribution of the data, otherwise known as the data generating process (DGP). Sometimes \\(F\\) is also referred to as the population distribution or even just population, which has its roots in viewing the data as a random sample from some larger population.1 As a shorthand, we often say that the collection of random vectors \\(\\{X_1, \\ldots, X_n\\}\\) is a random sample from population \\(F\\) if \\(\\{X_1, \\ldots, X_n\\}\\) is iid with distribution \\(F\\). The sample size \\(n\\) is the number of units in the sample.\nTwo metaphors can help build intuition about the concept of viewing the data as an iid draw from \\(F\\):\n\nRandom sampling. Suppose we have a population of size \\(N\\) that is much larger than our sample size \\(n\\), and we take a random sample of size \\(n\\) from this population with replacement. Then the distribution of the data in the random sample will be iid draws from the population distribution of the variables we are sampling. For instance, suppose we take a random sample from a population of US citizens where the population proportion of Democratic party identifiers is 0.33. Then if we randomly sample \\(n = 100\\) US citizens, each data point \\(X_i\\) will be distributed Bernoulli with probability of success 0.33.\nGroundhog Day. Random sampling does not always make sense as a justification for iid data, especially when the units are not samples at all but rather countries, states, or subnational units. In this case, we have to appeal to a thought experiment where \\(F\\) represents the fundamental uncertainty in the data-generating process. The metaphor here is that if we could re-run history many times, like the 1993 American classic comedy Groundhog Day, data and outcomes would change slightly due to the inherently stochastic nature of the world. The iid assumption, then, is that each of the units in our data has the same DGP producing this data or the same distribution of outcomes under the Groundhog Day scenario. The set of all these infinite possible draws from the DGP is sometimes referred to as the superpopulation.\n\nNote that there are many situations where the iid assumption is not appropriate. We will cover some of those later in the semester. But much of the innovation and growth in statistics over the last 50 years has been figuring out how to perform statistical inference when iid does not hold. Often, the solutions are specific to the type of iid violation you have (spatial, time-series, network, or clustered). As a rule of thumb, though, if you suspect iid is incorrect, your uncertainty statements will likely be overconfident (for example, confidence intervals, which we’ll cover later, are too small)."
  },
  {
    "objectID": "02_estimation.html#point-estimation",
    "href": "02_estimation.html#point-estimation",
    "title": "2  Estimation",
    "section": "2.3 Point estimation",
    "text": "2.3 Point estimation\n\n2.3.1 Quantities of interest\nWe aim to learn about the data-generating process, represented by the cdf, \\(F\\). We might be interested in estimating the cdf at a general level or only some feature of the distribution, like a mean or conditional expectation function. We will almost always have a particular quantity in mind, but we’ll introduce estimation at a general level. So we’ll let \\(\\theta\\) represent the quantity of interest. Point estimation describes how we obtain a single “best guess” about \\(\\theta\\).\n\n\n\n\n\n\nNote\n\n\n\nSome refer to quantities of interest as parameters or estimands (that is, the target of estimation).\n\n\n\nExample 2.2 (Population mean) Suppose we wanted to know the proportion of US citizens who support increasing legal immigration in the US, which we denote as \\(Y_i = 1\\). Then our quantity of interest is the mean of this random variable, \\(\\mu = \\E[Y_i]\\), which is the probability of randomly drawing someone from the population supporting increased legal immigration.\n\n\nExample 2.3 (Population variance) Feeling thermometer scores are a prevalent way to assess how a survey respondent feels about a particular person or group. A survey asks respondents how warmly they feel about a group from 0 to 100, which we will denote \\(Y_i\\). We might be interested in how polarized views are on a group in the population, and one measure of polarization could be the variance, or spread, of the distribution of \\(Y_i\\) around the mean. In this case, \\(\\sigma^2 = \\V[Y_i]\\) would be our quantity of interest.\n\n\nExample 2.4 (RCT continued) In Example 2.1, we discussed a typical estimator for an experimental study with a binary treatment. The goal of that experiment is to learn about the difference between two conditional probabilities (or expectations): the average support for increasing legal immigration in the treatment group, \\(\\mu_1 = \\E[Y_i \\mid D_i = 1]\\), and the same average in the control group, \\(\\mu_0 = \\E[Y_i \\mid D_i = 0]\\). This difference, \\(\\mu_1 - \\mu_0\\), is a function of unknown features of these two conditional distributions.\n\nEach of these is a function of the (possibly joint) distribution of the data, \\(F\\). In each of these, we are not necessarily interested in the entire distribution, just summaries of it (central tendency, spread). Of course, there are situations where we are also interested in the complete distribution.\n\n\n2.3.2 Estimators\nWhen our sample size is more than a few observations, it makes no sense to work with the raw data, \\(X_1, \\ldots, X_n\\), and we inevitably will need to summarize the data in some way. We can represent this summary as a function, \\(g(x_1, \\ldots, x_n)\\), which might be the formula for the sample mean or sample variance. This function is just a regular function that takes in \\(n\\) numbers (or vectors) and returns a number (or vector). We can also define a random variable based on this function, \\(Y = g(X_1, \\ldots, X_n)\\), which inherits its randomness from the randomness of the data. Before we see the data, we don’t know what values of \\(X_1, \\ldots, X_n\\) we will see, so we don’t know what value of \\(Y\\) we’ll see either. We call the random variable \\(Y = g(X_1, \\ldots, X_n)\\) a statistic (or sometimes sample statistics), and we refer to the probability distribution of a statistic \\(Y\\) as the sampling distribution of \\(Y\\).\n\n\n\n\n\n\nWarning\n\n\n\nThere is one potential confusion in how we talk about “statistics.” Just above, we defined a statistic as a random variable based on it being a function of random variables (the data). But we sometimes refer to the calculated value as a statistic as well, which is a specific number that you see in your R output. To be precise, we should call the latter the realized value of the statistic, but message discipline is difficult to enforce in this context. A simple example might help. Suppose that \\(X_1\\) and \\(X_2\\) are the results of a roll of two standard six-sided dice. Then the statistic \\(Y = X_1 + X_2\\) is a random variable that has a distribution over the numbers from \\(\\{2, \\ldots, 12\\}\\) that describes our uncertainty over what the sum will be before we roll the dice. Once we have rolled the dice and observed the realized values \\(X_1 = 3\\) and \\(X_2 = 4\\), we observed the realized value of the statistic, \\(Y = 7\\).\n\n\nAt their most basic, statistics are just data summaries without aim or ambition. Estimators are statistics with a purpose: to provide an “educated guess” about some quantity of interest.\n\nDefinition 2.1 An estimator \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\) for some parameter \\(\\theta\\), is a statistic intended as a guess about \\(\\theta\\).\n\nOne important distinction of jargon is between an estimator and an estimate, similar to the issues with “statistic” described above. The estimator is a function of the data, whereas the estimate is the realized value of the estimator once we see the data. An estimate is a single number, such as 0.38, whereas the estimator is a random variable that has uncertainty over what value it will take. Formally, the estimate is \\(\\theta(x_1, \\ldots, x_n)\\) when the data is \\(\\{X_1, \\ldots, X_n\\} = \\{x_1, \\ldots, x_n\\}\\), whereas we represent the estimator as a function of random variables, \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\n\n\n\n\n\nNote\n\n\n\nIt is widespread, though not universal, to use the “hat” notation to define an estimator and its estimand. For example, \\(\\widehat{\\theta}\\) (or “theta hat”) indicates that this estimator is targeting the parameter \\(\\theta\\).\n\n\n\nExample 2.5 (Estimators for the population mean) Suppose we would like to estimate the population mean of \\(F\\), which we will represent as \\(\\mu = \\E[X_i]\\). We could choose from several estimators, all with different properties. \\[\n\\widehat{\\theta}_{n,1} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\quad \\widehat{\\theta}_{n,2} = X_1, \\quad \\widehat{\\theta}_{n,3} = \\text{max}(X_1,\\ldots,X_n), \\quad \\widehat{\\theta}_{n,4} = 3\n\\] The first is just the sample mean, which is an intuitive and natural estimator for the population mean. The second just uses the first observation. While this seems silly, this is a valid statistic (it’s a function of the data!). The third takes the maximum value in the sample, and the fourth always returns three, regardless of the data."
  },
  {
    "objectID": "02_estimation.html#how-to-find-estimators",
    "href": "02_estimation.html#how-to-find-estimators",
    "title": "2  Estimation",
    "section": "2.4 How to find estimators",
    "text": "2.4 How to find estimators\nWhere do estimators come from? There are a couple of different methods that I’ll cover briefly here before describing the ones that will form the bulk of this class.\n\n2.4.1 Parametric models and maximum likelihood\nThe first method for generating estimators relies on parametric models, where the researcher specifies the exact distribution (up to some unknown parameters) of the DGP. Let \\(\\theta\\) be the parameters of this distribution and we then write \\(\\{X_1, \\ldots, X_n\\}\\) are iid draws from \\(F_{\\theta}\\). We should also formally state the set of possible values the parameters can take, which we call the parameter space and usually denote as \\(\\Theta\\). Because we’re assuming we know the distribution of the data, we can write the p.d.f. as \\(f(X_i \\mid \\theta)\\) and define the likelihood function as the product of these p.d.f.s over the units as a function of the parameters: \\[\nL(\\theta) = \\prod_{i=1}^n f(X_i \\mid \\theta).\n\\] We can then define the maximum likelihood estimator (MLE) for \\(\\theta\\) as the values of the parameter that, well, maximize the likelihood: \\[\n\\widehat{\\theta}_{mle} = \\argmax_{\\theta \\in \\Theta} \\; L(\\theta)\n\\] Sometimes we can use calculus to derive a closed-form expression for the MLE. Still, we often use iterative techniques that search the parameter space for the maximum.\nMaximum likelihood estimators have very nice properties, especially in large samples. Unfortunately, they also require the correct knowledge of the parametric model, which is often difficult to justify. Do we really know if we should model a given event count variable as Poisson or Negative Binomial? The attractive properties of MLE are only as good as our ability to specify the parametric model.\n\n\n\n\n\n\nNo free lunch\n\n\n\nOne essential intuition to build about statistics is the assumptions-precision tradeoff. You can usually get more precise estimates if you make stronger and potentially more fragile assumptions. Conversely, you will almost always get less accurate estimates if you weaken your assumptions.\n\n\n\n\n2.4.2 Plug-in estimators\nThe second broad class of estimators is semiparametric in that we will specify some finite-dimensional parameters of the DGP but leave the rest of the distribution unspecified. For example, we might define a population mean, \\(\\mu = \\E[X_i]\\), and a population variance, \\(\\sigma^2 = \\V[X_i]\\) but leave unrestricted the shape of the distribution. This approach ensures that our estimators will be less dependent on correctly specifying distributions we have little intuition about.\nThe primary method for constructing estimators in this setting is to use the plug-in estimator, or the estimator that replaces any population mean with a sample mean. Obviously, in the case of estimating the population mean, \\(\\mu\\), this means we will use the sample mean as its estimate: \\[\n\\Xbar_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\quad \\text{estimates} \\quad \\E[X_i] = \\int_{\\mathcal{X}} x f(x)dx\n\\] What are we doing here? We are replacing the unknown population distribution \\(f(x)\\) in the population mean with a discrete uniform distribution over our data points, with \\(1/n\\) probability assigned to each unit. Why do this? It encodes that if we have a random sample, our best guess about the population distribution of \\(X_i\\) is the sample distribution in our actual data. If this intuition fails, you can hold onto an analog principle: sample means of random variables are natural estimators of population means.\nWhat about estimating something more complicated, like the expected value of a function of the data, \\(\\theta = \\E[r(X_i)]\\)? The key is to see that \\(f(X_i)\\) is also a random variable. Let’s call this random variable \\(Y_i = f(X_i)\\). Now we can see that \\(\\theta\\) is just the population expectation of this random variable, and using the plug-in estimator, we get: \\[\n\\widehat{\\theta} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\nWith these facts in hand, we can describe the more general plug-in estimator. When we want to estimate some quantity of interest that is a function of population means, we can generate a plug-in estimator by replacing any population mean with a sample mean. Formally, let \\(\\alpha = g\\left(\\E[r(X_i)]\\right)\\) be a parameter that is defined as a function of the population mean of a (possibly vector-valued) function of the data. Then, we can estimate this parameter by plugging in the sample mean for the population mean to get the plug-in estimator, \\[\n\\widehat{\\alpha} = g\\left( \\frac{1}{n} \\sum_{i=1}^n r(X_i) \\right) \\quad \\text{estimates} \\quad \\alpha = g\\left(\\E[r(X_i)]\\right)\n\\] This approach to plug-in estimation with sample means is very general and will allow us to derive estimators in various settings.\n\nExample 2.6 (Estimating population variance) The population variance of a random variable is \\(\\sigma^2 = \\E[(X_i - \\E[X_i])^2]\\). To derive a plug-in estimator for this quantity, we replace the inner \\(\\E[X_i]\\) with \\(\\Xbar_n\\) and the outer expectation with another sample mean: \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2.\n\\] This plug-in estimator differs from the standard sample variance, which divides by \\(n - 1\\) rather than \\(n\\). This minor difference does not matter in moderate to large samples.\n\n\nExample 2.7 (Estimating population covariance) Suppose we have two variables, \\((X_i, Y_i)\\). A natural quantity of interest here is the population covariance between these variables, \\[\n\\sigma_{xy} = \\text{Cov}[X_i,Y_i] = \\E[(X_i - \\E[X_i])(Y_i-\\E[Y_i])],\n\\] which has the plug-in estimator, \\[\n\\widehat{\\sigma}_{xy} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)(Y_i - \\Ybar_n).\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nGiven the connection between the population mean and the sample mean, you will sometimes see the \\(\\E_n[\\cdot]\\) operator used as a shorthand for the sample average: \\[\n\\E_n[r(X_i)] \\equiv \\frac{1}{n} \\sum_{i=1}^n r(X_i).\n\\]\n\n\nFinally, plug-in estimation goes beyond just replacing population means with sample means. We can derive estimators of the population quantiles like the median with sample versions of those quantities. What unifies all of these approaches is replacing the unknown population cdf, \\(F\\), with the empirical cdf, \\[\n\\widehat{F}_n(x) = \\frac{\\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}{n},\n\\] where \\(\\mathbb{I}(A)\\) is an indicator function that take the value 1 if the event \\(A\\) occurs and 0 otherwise. For a more complete and technical treatment of these ideas, see Wasserman (2004) Chapter 7."
  },
  {
    "objectID": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "href": "02_estimation.html#the-three-distributions-population-empirical-and-sampling",
    "title": "2  Estimation",
    "section": "2.5 The three distributions: population, empirical, and sampling",
    "text": "2.5 The three distributions: population, empirical, and sampling\nOnce we start to wade into estimation, there are several distributions to keep track of, and things can quickly become confusing. Three specific distributions are all related and easy to confuse, but keeping them distinct is crucial.\nThe population distribution is the distribution of the random variable, \\(X_i\\), which we have labeled \\(F\\) and is our target of inference. Then there is the empirical distribution, which is the distribution of the actual realizations of the random variables in our samples (that is, the numbers in our data frame), \\(X_1, \\ldots, X_n\\). Because this is a random sample from the population distribution and can serve as an estimator of \\(F\\), we sometimes call this \\(\\widehat{F}_n\\).\n\nSeparately from both is the sampling distribution of an estimator, which is the probability distribution of \\(\\widehat{\\theta}_n\\). It represents our uncertainty about our estimate before we see the data. Remember that our estimator is itself a random variable because it is a function of random variables: the data itself. That is, we defined the estimator as \\(\\widehat{\\theta}_n = \\theta(X_1, \\ldots, X_n)\\).\n\nExample 2.8 (Likert responses) Suppose \\(X_i\\) is the answer to a question, “How much do you agree with the following statement: Immigrants are a net positive for the United States,” with a \\(X_i = 0\\) being “strongly disagree,” \\(X_i = 1\\) being “disagree,” \\(X_i = 2\\) being “neither agree nor disagree,” \\(X_i = 3\\) being “agree,” and \\(X_i = 4\\) being “strongly agree.”\nThe population distribution describes the probability of randomly selecting a person with each one of these values, \\(\\P(X_i = x)\\). The empirical distribution would be the fraction of our data taking each value. And the sampling distribution of the sample mean, \\(\\Xbar_n\\), would be the distribution of the sample mean across repeated samples from the population.\nSuppose the population distribution was binomial with four trials and probability of success \\(p = 0.4\\). We could generate one sample with \\(n = 10\\) and thus one empirical distribution using rbinom():\n\nmy_samp &lt;- rbinom(n = 10, size = 5, prob = 0.4)\nmy_samp\n\n [1] 1 3 2 3 4 0 2 3 2 2\n\ntable(my_samp)\n\nmy_samp\n0 1 2 3 4 \n1 1 4 3 1 \n\n\nAnd we can generate one draw from the sampling distribution of \\(\\Xbar_n\\) by taking the mean of this sample:\n\nmean(my_samp)\n\n[1] 2.2\n\n\nBut, if we had a different sample, it would have a different empirical distribution and thus give us a different estimate of the sample mean:\n\nmy_samp2 &lt;- rbinom(n = 10, size = 5, prob = 0.4)\nmean(my_samp2) \n\n[1] 2\n\n\nThe sampling distribution is the distribution of these sample means across repeated sampling."
  },
  {
    "objectID": "02_estimation.html#finite-sample-properties-of-estimators",
    "href": "02_estimation.html#finite-sample-properties-of-estimators",
    "title": "2  Estimation",
    "section": "2.6 Finite-sample properties of estimators",
    "text": "2.6 Finite-sample properties of estimators\nAs we discussed when we introduced estimators, their usefulness depends on how well they help us learn about the quantity of interest. If we get an estimate \\(\\widehat{\\theta} = 1.6\\), we would like to know that this is “close” to the true parameter \\(\\theta\\). The sampling distribution is the key to answering these questions. Intuitively, we would like the sampling distribution of \\(\\widehat{\\theta}_n\\) to be as tightly clustered around the true as \\(\\theta\\) as possible. Here, though, we run into a problem: the sampling distribution depends on the population distribution since it is about repeated samples of the data from that distribution filtered through the function \\(\\theta()\\). Since \\(F\\) is unknown, this implies that the sampling distribution will also usually be unknown.\nEven though we cannot precisely pin down the entire sampling distribution, we can use assumptions to derive specific properties of the sampling distribution that will be useful in comparing estimators.\n\n2.6.1 Bias\nThe first property of the sampling distribution concerns its central tendency. In particular, we will define the bias (or estimation bias) of estimator \\(\\widehat{\\theta}\\) for parameter \\(\\theta\\) as \\[\n\\text{bias}[\\widehat{\\theta}] = \\E[\\widehat{\\theta}] - \\theta,\n\\] which is the difference between the mean of the estimator (across repeated samples) and the true parameter. All else equal, we would like estimation bias to be as small as possible. The smallest possible bias, obviously, is 0, and we define an unbiased estimator as one with \\(\\text{bias}[\\widehat{\\theta}] = 0\\) or equivalently, \\(\\E[\\widehat{\\theta}] = \\theta\\).\nHowever, all else is not always equal, and unbiasedness is not a property to become overly attached to. Many biased estimators have other attractive properties, and many popular modern estimators are biased.\n\nExample 2.9 (Unbiasedness of the sample mean) We can show that the sample mean is unbiased for the population mean when the data is iid and \\(\\E|X| &lt; \\infty\\). In particular, we simply apply the rules of expectations: \\[\\begin{aligned}\n\\E\\left[ \\Xbar_n \\right] &= \\E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] & (\\text{definition of } \\Xbar_n) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\E[X_i] & (\\text{linearity of } \\E)\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mu & (X_i \\text{ identically distributed})\\\\\n&= \\mu.\n\\end{aligned}\\] Notice that we only used the “identically distributed” part of iid. Independence is not needed.\n\n\n\n\n\n\n\nWarning\n\n\n\nProperties like unbiasedness might only hold for a subset of DGPs. For example, we just showed that the sample mean is unbiased, but only when the population mean is finite. There are probability distributions like the Cauchy where the expected value diverges and is not finite. So we are dealing with a restricted class of DGPs that rules out such distributions. You may see this sometimes formalized by defining a class \\(\\mathcal{F}\\) of distributions, and unbiasedness might hold in that class if it is unbiased for all \\(F \\in \\mathcal{F}\\).\n\n\n\n\n2.6.2 Estimation variance and the standard error\nIf a “good” estimator tends to be close to the truth, we should also care about the spread of the sampling distribution. In particular, we define the sampling variance as the variance of an estimator’s sampling distribution, \\(\\V[\\widehat{\\theta}]\\), which measures how spread out the estimator is around its mean. For an unbiased estimator, lower sampling variance implies the distribution of \\(\\widehat{\\theta}\\) is more concentrated around the truth.\n\nExample 2.10 (Sampling variance of the sample mean) We can establish the sampling variance of the sample mean of iid data for all \\(F\\) such that \\(\\V[X_i]\\) is finite (more precisely, \\(\\E[X_i^2] &lt; \\infty\\))\n\\[\\begin{aligned}\n  \\V\\left[ \\Xbar_n \\right] &= \\V\\left[ \\frac{1}{n} \\sum_{i=1}^n X_i \\right] & (\\text{definition of } \\Xbar_n) \\\\\n                           &= \\frac{1}{n^2} \\V\\left[ \\sum_{i=1}^n X_i \\right] & (\\text{property of } \\V) \\\\\n                           &= \\frac{1}{n^2} \\sum_{i=1}^n \\V[X_i] & (\\text{independence}) \\\\\n                           &= \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 & (X_i \\text{ identically distributed}) \\\\\n                           &= \\frac{\\sigma^2}{n}\n\\end{aligned}\\]\n\nAn alternative measure of spread for any distribution is the standard deviation, which is on the same scale as the original random variable. We call the standard deviation of the sampling distribution of \\(\\widehat{\\theta}\\) the standard error of \\(\\widehat{\\theta}\\): \\(\\se(\\widehat{\\theta}) = \\sqrt{\\V[\\widehat{\\theta}]}\\).\nGiven the above derivation, the standard error of the sample mean under iid sampling is \\(\\sigma / \\sqrt{n}\\).\n\n\n2.6.3 Mean squared error\nBias and sampling variance measure two different aspects of being a “good” estimator. Ideally, we want the estimator to be as close as possible to the true value. One summary measure of the quality of an estimator is the mean squared error or MSE, which is\n\\[\n\\text{MSE} = \\E[(\\widehat{\\theta}_n-\\theta)^2].\n\\] Ideally, we would have this be as small as possible!\nWe can also relate the MSE to the bias and the sampling variance (provided it is finite) with the following decomposition result: \\[\n\\text{MSE} = \\text{bias}[\\widehat{\\theta}_n]^2 + \\V[\\widehat{\\theta}_n]\n\\] This decomposition implies that, for unbiased estimators, MSE is the sampling variance. It also highlights why we might accept some bias for significant reductions in variance for lower overall MSE.\n\n\n\n\n\nTwo sampling distributions\n\n\n\n\nIn this figure, we show the sampling distributions of two estimators, \\(\\widehat{\\theta}_a\\), which is unbiased (centered on the true value \\(\\theta\\)) but with a high sampling variance, and \\(\\widehat{\\theta}_b\\) which is slightly biased but with much lower sampling variance. Even though \\(\\widehat{\\theta}_b\\) is biased, the probability of drawing a value close to the truth is higher than for \\(\\widehat{\\theta}_a\\). This balancing between bias and variance is precisely what the MSE helps capture and, indeed, in this case, \\(MSE[\\widehat{\\theta}_b] &lt; MSE[\\widehat{\\theta}_a]\\)."
  },
  {
    "objectID": "02_estimation.html#footnotes",
    "href": "02_estimation.html#footnotes",
    "title": "2  Estimation",
    "section": "",
    "text": "This approach to inference is often called a model-based approach since we are assuming a probability model in the cdf, \\(F\\). This is usually in contrast to a design-based approach to inference that views the population of interest as a finite group with fixed traits and the only randomness comes from the random sampling procedure.↩︎"
  },
  {
    "objectID": "03_asymptotics.html#introduction",
    "href": "03_asymptotics.html#introduction",
    "title": "3  Asymptotics",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn the last chapter, we defined estimators and started investigating their finite-sample properties like unbiasedness and the sampling variance. We call these “finite-sample” properties since they hold at any sample size. We saw that under iid data, the sample mean is unbiased for the population mean, but this result holds as much for \\(n = 10\\) as it does for \\(n = 1,000,000\\). But these properties are also of limited use: we only learn the center and spread of the sampling distribution of \\(\\Xbar_n\\) from these results. What about the shape of the distribution? We can often derive the shape if we are willing to make certain assumptions on the underlying data (for example, if the data is normal, then the sample means will also be normal). Still, this approach is brittle: if our parametric assumption is false, we’re back to square one.\nIn this chapter, we will take a different approach and see what happens to the sampling distribution of estimators as the sample size gets large, which we refer to as asymptotic theory. While asymptotics will often simplify our derivations, it is essential to understand everything we do with asymptotics will be an approximation. No one ever has infinite data, but we hope that the approximations will be closer to the truth as our samples get larger."
  },
  {
    "objectID": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "href": "03_asymptotics.html#why-convergence-with-probability-is-hard",
    "title": "3  Asymptotics",
    "section": "3.2 Why convergence with probability is hard",
    "text": "3.2 Why convergence with probability is hard\nIt’s helpful to review the basic idea of convergence in deterministic sequences from calculus:\n\nDefinition 3.1 A sequence \\(\\{a_n: n = 1, 2, \\ldots\\}\\) has the limit \\(a\\) written \\(a_n \\rightarrow a\\) as \\(n\\rightarrow \\infty\\) or \\(\\lim_{n\\rightarrow \\infty} a_n = a\\) if for all \\(\\epsilon &gt; 0\\) there is some \\(n_{\\epsilon} &lt; \\infty\\) such that for all \\(n \\geq n_{\\epsilon}\\), \\(|a_n - a| \\leq \\epsilon\\).\n\nWe say that \\(a_n\\) converges to \\(a\\) if \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Basically, a sequence converges to a number if the sequence gets closer and closer to that number as the sequence goes on.\nCan we apply this same idea to sequences of random variables (like estimators)? Let’s look at a few examples that help clarify why this might be difficult.1 Let’s say that we have a sequence of \\(a_n = a\\) for all \\(n\\) (that is, a constant sequence). Then obviously \\(\\lim_{n\\rightarrow\\infty} a_n = a\\). Now let’s say we have a sequence of random variables, \\(X_1, X_2, \\ldots\\), that are all independent with a standard normal distribution, \\(N(0,1)\\). From the analogy to the deterministic case, it is tempting to say that \\(X_n\\) converges to \\(X \\sim N(0, 1)\\), but notice that because they are all different random variables, \\(\\P(X_n = X) = 0\\). Thus, we must be careful about saying how one variable converges to another variable.\nAnother example highlights subtle problems with a sequence of random variables converging to a single value. Suppose we have a sequence of random variables \\(X_1, X_2, \\ldots\\) where \\(X_n \\sim N(0, 1/n)\\). Clearly, the distribution of \\(X_n\\) will concentrate around 0 for large values of \\(n\\), so it is tempting to say that \\(X_n\\) converges to 0. But notice that \\(\\P(X_n = 0) = 0\\) because of the nature of continuous random variables."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "href": "03_asymptotics.html#convergence-in-probability-and-consistency",
    "title": "3  Asymptotics",
    "section": "3.3 Convergence in probability and consistency",
    "text": "3.3 Convergence in probability and consistency\nThere are several different ways that a sequence of random variance can converge. The first type of convergence deals with sequences converging to a single value.2\n\nDefinition 3.2 A sequence of random variables, \\(X_1, X_2, \\ldots\\), is said to converge in probability to a value \\(b\\) if for every \\(\\varepsilon &gt; 0\\), \\[\n\\P(|X_n - b| &gt; \\varepsilon) \\rightarrow 0,\n\\] as \\(n\\rightarrow \\infty\\). We write this \\(X_n \\inprob b\\).\n\nWith deterministic sequences, we said that \\(a_n\\) converges to \\(a\\) as it gets closer and closer to \\(a\\) as \\(n\\) gets bigger. For convergence in probability, the sequence of random variables converges to \\(b\\) if the probability that random variables are far away from \\(b\\) gets smaller and smaller as \\(n\\) gets big.\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will sometimes see convergence in probability written as \\(\\text{plim}(Z_n) = b\\) if \\(Z_n \\inprob b\\), \\(\\text{plim}\\) stands for “probability limit.”\n\n\nConvergence in probability is crucial for evaluating estimators. While we said that unbiasedness was not the be-all and end-all of properties of estimators, the following property is an essential and fundamental property that we would like all good estimators to have.\n\nDefinition 3.3 An estimator is consistent if \\(\\widehat{\\theta}_n \\inprob \\theta\\).\n\nConsistency of an estimator implies that the sampling distribution of this estimator “collapses” on the true value as the sample size gets large. We say an estimator is inconsistent if it converges in probability to any other value, which is obviously a terrible property of an estimator. As the sample size gets large, the probability that an inconsistent estimator will be close to the truth will approach 0.\nWe can also define convergence in probability for a sequence of random vectors, \\(\\X_1, \\X_2, \\ldots\\), where \\(\\X_i = (X_{i1}, \\ldots, X_{ik})\\) is a random vector of length \\(k\\). This sequence convergences in probability to a vector \\(\\mb{b} = (b_1, \\ldots, b_k)\\) if and only if each random variable in the vector converges to the corresponding element in \\(\\mb{b}\\), or that \\(X_{nj} \\inprob b_j\\) for all \\(j = 1, \\ldots, k\\)."
  },
  {
    "objectID": "03_asymptotics.html#useful-inequalities",
    "href": "03_asymptotics.html#useful-inequalities",
    "title": "3  Asymptotics",
    "section": "3.4 Useful inequalities",
    "text": "3.4 Useful inequalities\nAt first glance, establishing an estimator’s consistency will be difficult. How can we know if a distribution will collapse to a specific value without knowing the shape or family of the distribution? It turns out that there are certain relationships between the mean and variance of a random variable and certain probability statements that hold for all distributions (that have finite variance, at least). These relationships will be crucial to establishing results that do not depend on a specific distribution.\n\nTheorem 3.1 (Markov Inequality) For any r.v. \\(X\\) and any \\(\\delta &gt;0\\), \\[\n\\P(|X| \\geq \\delta) \\leq \\frac{\\E[|X|]}{\\delta}.\n\\]\n\n\nProof. Notice that we can let \\(Y = |X|/\\delta\\) and rewrite the statement as \\(\\P(Y \\geq 1) \\leq \\E[Y]\\) (since \\(\\E[|X|]/\\delta = \\E[|X|/\\delta]\\) by the properties of expectation), which is what we will show. But notice that \\[\n\\mathbb{I}(Y \\geq 1) \\leq Y.\n\\] Why does this hold? We can investigate the two possible values of the indicator function to see. If \\(Y\\) is less than 1, then the indicator function will be 0, but recall that \\(Y\\) is nonnegative, so we know that it must be at least as big as 0 so that inequality holds. If \\(Y \\geq 1\\), then the indicator function will take the value one, but we just said that \\(Y \\geq 1\\), so the inequality holds. If we take the expectation of both sides of this inequality, we obtain the result (remember, the expectation of an indicator function is the probability of the event).\n\nIn words, Markov’s inequality says that the probability of a random variable being large in magnitude cannot be high if the average is not large in magnitude. Blitzstein and Hwang (2019) provide an excellent intuition behind this result. Let \\(X\\) be the income of a randomly selected individual in a population and set \\(\\delta = 2\\E[X]\\) so that the inequality becomes \\(\\P(X &gt; 2\\E[X]) &lt; 1/2\\) (assuming that all income is nonnegative). Here, the inequality says that the share of the population with an income twice the average must be less than 0.5 since if more than half the population were making twice the average income, then the average would have to be higher.\nIt’s pretty astounding how general this result is since it holds for all random variables. Of course, its generality comes at the expense of not being very informative. If \\(\\E[|X|] = 5\\), for instance, the inequality tells us that \\(\\P(|X| \\geq 1) \\leq 5\\), which is not very helpful since we already know that probabilities are less than 1! We can get tighter bounds if we are willing to make some assumptions about \\(X\\).\n\nTheorem 3.2 (Chebyshev Inequality) Suppose that \\(X\\) is r.v. for which \\(\\V[X] &lt; \\infty\\). Then, for every real number \\(\\delta &gt; 0\\), \\[\n\\P(|X-\\E[X]| \\geq \\delta) \\leq \\frac{\\V[X]}{\\delta^2}.\n\\]\n\n\nProof. To prove this, we only need to square both sides of the inequality inside the probability statement and apply Markov’s inequality: \\[\n\\P\\left( |X - \\E[X]| \\geq \\delta \\right) = \\P((X-\\E[X])^2 \\geq \\delta^2) \\leq \\frac{\\E[(X - \\E[X])^2]}{\\delta^2} = \\frac{\\V[X]}{\\delta^2},\n\\] with the last equality holding by the definition of variance.\n\nChebyshev’s inequality is a straightforward extension of the Markov result: the probability of a random variable being far from its mean (that is, \\(|X-\\E[X]|\\) being large) is limited by the variance of the random variable. If we let \\(\\delta = c\\sigma\\), where \\(\\sigma\\) is the standard deviation of \\(X\\), then we can use this result to bound the normalized: \\[\n\\P\\left(\\frac{|X - \\E[X]|}{\\sigma} &gt; c \\right) \\leq \\frac{1}{c^2}.\n\\] This statement says the probability of being two standard deviations away from the mean must be less than 1/4 = 0.25. Notice that this bound can be fairly wide. If \\(X\\) has a normal distribution, we know that about 5% of draws will be greater than 2 SDs away from the mean, much lower than the 25% bound implied by Chebyshev’s inequality."
  },
  {
    "objectID": "03_asymptotics.html#the-law-of-large-numbers",
    "href": "03_asymptotics.html#the-law-of-large-numbers",
    "title": "3  Asymptotics",
    "section": "3.5 The law of large numbers",
    "text": "3.5 The law of large numbers\nWe can now use these inequalities to show how estimators can be consistent for their target quantities of interest. Why are these inequalities helpful for this purpose? Remember that convergence in probability was about the probability of an estimator being far away from a value going to zero. Chebyshev’s inequality shows that we can bound these exact probabilities.\nThe most famous consistency result has a special name.\n\nTheorem 3.3 (Weak Law of Large Numbers) Let \\(X_1, \\ldots, X_n\\) be i.i.d. draws from a distribution with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i] &lt; \\infty\\). Let \\(\\Xbar_n = \\frac{1}{n} \\sum_{i =1}^n X_i\\). Then, \\(\\Xbar_n \\inprob \\mu\\).\n\n\nProof. Recall that the sample mean is unbiased, so \\(\\E[\\Xbar_n] = \\mu\\) with sampling variance \\(\\sigma^2/n\\). We can then apply Chebyshev to the sample mean to get \\[\n\\P(|\\Xbar_n - \\mu| \\geq \\delta) \\leq \\frac{\\sigma^2}{n\\delta^2}\n\\] An \\(n\\rightarrow\\infty\\), the right-hand side goes to 0, which means that the left-hand side also must go to 0, which is the definition of \\(\\Xbar_n\\) converging in probability to \\(\\mu\\).\n\nThe weak law of large numbers (WLLN) shows that, under general conditions, the sample mean gets closer to the population mean as \\(n\\rightarrow\\infty\\). This result holds even when the variance of the data is infinite, though that’s a situation that most analysts will rarely face.\n\n\n\n\n\n\nNote\n\n\n\nThe naming of the “weak” law of large numbers seems to imply the existence of a “strong” law of large numbers (SLLN), and this is true. The SLLN states that the sample mean converges to the population mean with probability 1. This type of convergence, called almost sure convergence, is stronger than convergence in probability which only says that the probability of the sample mean being close to the population mean converges to 1. While it is nice to know that this stronger form of convergence holds for the sample mean under the same assumptions, it is rare for folks outside of theoretical probability and statistics to rely on almost sure convergence.\n\n\n\nExample 3.1 It can be helpful to see how the distribution of the sample mean changes as a function of the sample size to appreciate the WLLN. We can show this by taking repeated iid samples of different sizes from an exponential random variable with rate parameter 0.5 so that \\(\\E[X_i] = 2\\). In Figure 3.1, we show the distribution of the sample mean (across repeated samples) when the sample size is 15 (black), 30 (violet), 100 (blue), and 1000 (green). We can see how the sample mean distribution is “collapsing” on the true population mean, 2. The probability of being far away from 2 becomes progressively smaller.\n\n\n\n\n\nFigure 3.1: Sampling distribution of the sample mean as a function of sample size.\n\n\n\n\n\nThe WLLN also holds for random vectors in addition to random variables. Let \\((\\X_1, \\ldots, \\X_n)\\) be an iid sample of random vectors of length \\(k\\), \\(\\mb{X}_i = (X_{i1}, \\ldots, X_{ik})\\). We can define the vector sample mean as just the vector of sample means for each of the entries:\n\\[\n\\overline{\\mb{X}}_n = \\frac{1}{n} \\sum_{i=1}^n \\mb{X}_i =\n\\begin{pmatrix}\n\\Xbar_{n,1} \\\\ \\Xbar_{n,2} \\\\ \\vdots \\\\ \\Xbar_{n, k}\n\\end{pmatrix}\n\\] Since this is just a vector of sample means, each random variable in the random vector will converge in probability to the mean of that random variable. Fortunately, this is the exact definition of convergence in probability for random vectors. We formally write this in the following theorem.\n\nTheorem 3.4 If \\(\\X_i \\in \\mathbb{R}^k\\) are iid draws from a distribution with \\(\\E[X_{ij}] &lt; \\infty\\) for all \\(j=1,\\ldots,k\\) then as \\(n\\rightarrow\\infty\\)\n\\[\n\\overline{\\mb{X}}_n \\inprob \\E[\\X] =\n\\begin{pmatrix}\n\\E[X_{i1}] \\\\ \\E[X_{i2}] \\\\ \\vdots \\\\ \\E[X_{ik}]\n\\end{pmatrix}.\n\\]\n\n\n\n\n\n\n\nNotation alert\n\n\n\nYou will have noticed that many of the formal results we have presented so far have “moment conditions” that certain moments are finite. For the vector WLLN, we saw that applied to the mean of each variable in the vector. Some books use a shorthand for this: \\(\\E\\Vert \\X_i\\Vert &lt; \\infty\\), where \\[\n\\Vert\\X_i\\Vert = \\left(X_{i1}^2 + X_{i2}^2 + \\ldots + X_{ik}^2\\right)^{1/2}.\n\\] This expression has slightly more compact notation, but why does it work? One can show that this function, called the Euclidean norm or \\(L_2\\)-norm, is a convex function, so we can apply Jensen’s inequality to show that: \\[\n\\E\\Vert \\X_i\\Vert \\geq \\Vert \\E[\\X_i] \\Vert = (\\E[X_{i1}]^2 + \\ldots + \\E[X_{ik}]^2)^{1/2}.\n\\] So if \\(\\E\\Vert \\X_i\\Vert\\) is finite, all the component means are finite. Otherwise, the right-hand side of the previous equation would be infinite."
  },
  {
    "objectID": "03_asymptotics.html#consistency-of-estimators",
    "href": "03_asymptotics.html#consistency-of-estimators",
    "title": "3  Asymptotics",
    "section": "3.6 Consistency of estimators",
    "text": "3.6 Consistency of estimators\nThe WLLN shows that the sample mean of iid draws is consistent for the population mean, which is a massive result given that so many estimators are sample means of potentially complicated functions of the data. What about other estimators? The proof of the WLLN points to one way to determine if an estimator is consistent: if it is unbiased and the sampling variance shrinks as the sample size grows.\n\nTheorem 3.5 For any estimator \\(\\widehat{\\theta}_n\\), if \\(\\text{bias}[\\widehat{\\theta}_n] = 0\\) and \\(\\V[\\widehat{\\theta}_n] \\rightarrow 0\\) as \\(n\\rightarrow \\infty\\), then \\(\\widehat{\\theta}_n\\) is consistent.\n\nThus, for unbiased estimators, if we can characterize its sampling variance, we should be able to tell if it is consistent. This result is handy since working with the probability statements used for the WLLN can sometimes be quite confusing.\nWhat about biased estimators? Consider a plug-in estimator like \\(\\widehat{\\alpha} = \\log(\\Xbar_n)\\) where \\(X_1, \\ldots, X_n\\) are iid from a population with mean \\(\\mu\\). We know that for nonlinear functions like logarithms we have \\(\\log\\left(\\E[Z]\\right) \\neq \\E[\\log(Z)]\\), so \\(\\E[\\widehat{\\alpha}] \\neq \\log(\\E[\\Xbar_n])\\) and the plug-in estimator will be biased for \\(\\log(\\mu)\\). It will also be difficult to obtain an expression for the bias in terms of \\(n\\). Is all hope lost here? Must we give up on consistency? No, and in fact, consistency will be much simpler to show in this setting.\n\nTheorem 3.6 (Properties of convergence in probability) Let \\(X_n\\) and \\(Z_n\\) be two sequences of random variables such that \\(X_n \\inprob a\\) and \\(Z_n \\inprob b\\), and let \\(g(\\cdot)\\) be a continuous function. Then,\n\n\\(g(X_n) \\inprob g(a)\\) (continuous mapping theorem)\n\\(X_n + Z_n \\inprob a + b\\)\n\\(X_nZ_n \\inprob ab\\)\n\\(X_n/Z_n \\inprob a/b\\) if \\(b &gt; 0\\).\n\n\nWe can now see that many of the nasty problems with expectations and nonlinear functions are made considerably easier with convergence in probability in the asymptotic setting. So while we know that \\(\\log(\\Xbar_n)\\) is biased for \\(\\log(\\mu)\\), we know that it is consistent since \\(\\log(\\Xbar_n) \\inprob \\log(\\mu)\\) because \\(\\log\\) is a continuous function.\n\nExample 3.2 Suppose we implemented a survey by randomly selecting a sample from the population of size \\(n\\), but not everyone responded to our survey. Let the data consist of pairs of random variables, \\((Y_1, R_1), \\ldots, (Y_n, R_n)\\), where \\(Y_i\\) is the question of interest and \\(R_i\\) is a binary indicator for if the respondent answered the question (\\(R_i = 1\\)) or not (\\(R_i = 0\\)). Our goal is to estimate the mean of the question for responders: \\(\\E[Y_i \\mid R_i = 1]\\). We can use the law of iterated expectation to obtain \\[\n\\begin{aligned}\n\\E[Y_iR_i] &= \\E[Y_i \\mid R_i = 1]\\P(R_i = 1) + \\E[ 0 \\mid R_i = 0]\\P(R_i = 0) \\\\\n\\implies \\E[Y_i \\mid R_i = 1] &= \\frac{\\E[Y_iR_i]}{\\P(R_i = 1)}\n\\end{aligned}\n\\]\nThe relevant estimator for this quantity is the mean of the outcome among those who responded, which is slightly more complicated than a typical sample mean because the denominator is a random variable: \\[\n\\widehat{\\theta}_n = \\frac{\\sum_{i=1}^n Y_iR_i}{\\sum_{i=1}^n R_i}.\n\\] Notice that this estimator is the ratio of two random variables. The numerator has mean \\(n\\E[Y_iR_i]\\) and the denominator has mean \\(n\\P(R_i = 1)\\). It is then tempting to say that we can take the ratio of these means as the mean of \\(\\widehat{\\theta}_n\\), but expectations are not preserved in nonlinear functions like this.\nWe can establish consistency of our estimator, though, by noting that we can rewrite the estimator as a ratio of sample means \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i},\n\\] where by the WLLN the numerator \\((1/n)\\sum_{i=1}^n Y_iR_i \\inprob \\E[Y_iR_i]\\) and the denominator \\((1/n)\\sum_{i=1}^n R_i \\inprob \\P(R_i = 1)\\). Thus, by Theorem 3.6, we have \\[\n\\widehat{\\theta}_n = \\frac{(1/n)\\sum_{i=1}^n Y_iR_i}{(1/n)\\sum_{i=1}^n R_i} \\inprob \\frac{\\E[Y_iR_i]}{\\P[R_i = 1]} = \\E[Y_i \\mid R_i = 1],\n\\] so long as the probability of responding is greater than zero. This establishes that our sample mean among responders, while biased for the conditional expectation among responders, is consistent for that quantity.\n\nKeeping the difference between unbiased and consistent clear in your mind is essential. You can easily create ridiculous unbiased estimators that are inconsistent. Let’s return to our iid sample, \\(X_1, \\ldots, X_n\\), from a population with \\(E[X_i] = \\mu\\). There is nothing in the rule book against defining an estimator \\(\\widehat{\\theta}_{first} = X_1\\) that uses the first observation as the estimate. This estimator is silly, but it is unbiased since \\(\\E[\\widehat{\\theta}_{first}] = \\E[X_1] = \\mu\\). It is inconsistent since the sampling variance of this estimator is just the variance of the population distribution, \\(\\V[\\widehat{\\theta}_{first}] = \\V[X_i] = \\sigma^2\\), which does not change as a function of the sample size. Generally speaking, we can regard “unbiased but inconsistent” estimators as silly and not worth our time (along with biased and inconsistent estimators).\nSome estimators are biased but consistent that are often much more interesting. We already saw one such estimator in Example 3.2, but there are many more. Maximum likelihood estimators, for example, are (under some regularity conditions) consistent for the parameters of a parametric model but are often biased.\nTo study these estimator, we can broaden Theorem 3.5 to the class of asymptotically unbiased estimators that have bias that vanishes as the sample size grows.\n\nTheorem 3.7 For any estimator \\(\\widehat{\\theta}_n\\), if \\(\\text{bias}[\\widehat{\\theta}_n] \\to 0\\) and \\(\\V[\\widehat{\\theta}_n] \\rightarrow 0\\) as \\(n\\rightarrow \\infty\\), then \\(\\widehat{\\theta}_n\\) is consistent.\n\n\nExample 3.3 (Plug-in variance estimator) In the last chapter, we introduced the plug-in estimator for the population variance, \\[\n\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\Xbar_n)^2,\n\\] which we will now show is biased but consistent. To see the bias note that we can rewrite the sum of square deviations \\[\\sum_{i=1}^n (X_i - \\Xbar_n)^2 = \\sum_{i=1}^n X_i^2 - n\\Xbar_n^2. \\] Then, the expectation of the plug-in estimator is \\[\n\\begin{aligned}\n\\E[\\widehat{\\sigma}^2] & = \\E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i^2\\right] - \\E[\\Xbar_n^2] \\\\\n&= \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n \\E[X_iX_j] \\\\\n&= \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\E[X_i^2] - \\frac{1}{n^2}\\sum_{i=1}^n \\sum_{j\\neq i} \\underbrace{\\E[X_i]\\E[X_j]}_{\\text{independence}} \\\\\n&= \\E[X_i^2] - \\frac{1}{n}\\E[X_i^2] - \\frac{1}{n^2} n(n-1)\\mu^2 \\\\\n&= \\frac{n-1}{n} \\left(\\E[X_i^2] - \\mu^2\\right) \\\\\n&= \\frac{n-1}{n} \\sigma^2 = \\sigma^2 - \\frac{1}{n}\\sigma^2\n\\end{aligned}.\n\\] Thus, we can see that the bias of the plug-in estimator is \\(-(1/n)\\sigma^2\\), so it slightly underestimates the variance. Nicely, though, the bias shrinks as a function of the sample size, so according to Theorem 3.7, it will be consistent so long as the sampling variance of \\(\\widehat{\\sigma}^2\\) shrinks as a function of the sample size, which it does (though omit that proof here). Of course, simply multiplying this estimator by \\(n/(n-1)\\) will give an unbiased and consistent estimator that is also the typical sample variance estimator."
  },
  {
    "objectID": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "href": "03_asymptotics.html#convergence-in-distribution-and-the-central-limit-theorem",
    "title": "3  Asymptotics",
    "section": "3.7 Convergence in distribution and the central limit theorem",
    "text": "3.7 Convergence in distribution and the central limit theorem\nConvergence in probability and the law of large numbers are beneficial for understanding how our estimators will (or will not) collapse to their estimand as the sample size increases. But what about the shape of the sampling distribution of our estimators? For statistical inference, we would like to be able to make probability statements such as \\(\\P(a \\leq \\widehat{\\theta}_n \\leq b)\\). These statements will be the basis of hypothesis testing and confidence intervals. But to make those types of statements, we need to know the entire distribution of \\(\\widehat{\\theta}_n\\), not just the mean and variance. Luckily, established results will allow us to approximate the sampling distribution of a vast swath of estimators when our sample sizes are large.\nWe need first to describe a weaker form of convergence to see how we will develop these approximations.\n\nDefinition 3.4 Let \\(X_1,X_2,\\ldots\\), be a sequence of r.v.s, and for \\(n = 1,2, \\ldots\\) let \\(F_n(x)\\) be the c.d.f. of \\(X_n\\). Then it is said that \\(X_1, X_2, \\ldots\\) converges in distribution to r.v. \\(X\\) with c.d.f. \\(F(x)\\) if \\[\n\\lim_{n\\rightarrow \\infty} F_n(x) = F(x),\n\\] for all values of \\(x\\) for which \\(F(x)\\) is continuous. We write this as \\(X_n \\indist X\\) or sometimes \\(X_n ⇝ X\\).\n\nEssentially, convergence in distribution means that as \\(n\\) gets large, the distribution of \\(X_n\\) becomes more and more similar to the distribution of \\(X\\), which we often call the asymptotic distribution of \\(X_n\\) (other names include the large-sample distribution). If we know that \\(X_n \\indist X\\), then we can use the distribution of \\(X\\) as an approximation to the distribution of \\(X_n\\), and that distribution can be reasonably accurate.\nOne of the most remarkable results in probability and statistics is that a large class of estimators will converge in distribution to one particular family of distributions: the normal. This result is one reason we study the normal so much and why investing in building intuition about it will pay off across many domains of applied work. We call this broad class of results the “central limit theorem” (CLT), but it would probably be more accurate to refer to them as “central limit theorems” since much of statistics is devoted to showing the result in different settings. We now present the simplest CLT for the sample mean.\n\nTheorem 3.8 (Central Limit Theorem) Let \\(X_1, \\ldots, X_n\\) be i.i.d. r.v.s from a distribution with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i]\\). Then if \\(\\E[X_i^2] &lt; \\infty\\), we have \\[\n\\frac{\\Xbar_n - \\mu}{\\sqrt{\\V[\\Xbar_n]}} = \\frac{\\sqrt{n}\\left(\\Xbar_n - \\mu\\right)}{\\sigma} \\indist \\N(0, 1).\n\\]\n\nIn words: the sample mean of a random sample from a population with finite mean and variance will be approximately normally distributed in large samples. Notice how we have not made any assumptions about the distribution of the underlying random variables, \\(X_i\\). They could be binary, event count, continuous, or anything. The CLT is incredibly broadly applicable.\n\n\n\n\n\n\nNotation alert\n\n\n\nWhy do we state the CLT in terms of the sample mean after centering and scaling by its standard error? Suppose we don’t normalize the sample mean in this way. In that case, it isn’t easy to talk about convergence in distribution because we know from the WLLN that \\(\\Xbar_n \\inprob \\mu\\), so in the limit, the distribution of \\(\\Xbar_n\\) is concentrated at point mass around that value. Normalizing by centering and rescaling ensures that the variance of the resulting quantity will not depend on \\(n\\), so it makes sense to talk about its distribution converging. Sometimes you will see the equivalent result as \\[\n\\sqrt{n}\\left(\\Xbar_n - \\mu\\right) \\indist \\N(0, \\sigma^2).\n\\]\n\n\nWe can use this result to state approximations that we can use when discussing estimators such as \\[\n\\Xbar_n \\overset{a}{\\sim} N(\\mu, \\sigma^2/n),\n\\] where we use \\(\\overset{a}{\\sim}\\) to be “approximately distributed as in large samples.” This approximation allows us to say things like: “in large samples, we should expect the sample mean to between within \\(2\\sigma/\\sqrt{n}\\) of the true mean in 95% of repeated samples.” These statements will be essential for hypothesis tests and confidence intervals! Estimators so often follow the CLT that we have an expression for this property.\n\nDefinition 3.5 An estimator \\(\\widehat{\\theta}_n\\) is asymptotically normal if for some \\(\\theta\\) \\[\n\\sqrt{n}\\left( \\widehat{\\theta}_n - \\theta \\right) \\indist N\\left(0,\\V_{\\theta}\\right).\n\\]\n\n\nExample 3.4 To illustrate how the CLT works, we can simulate the sampling distribution of the (normalized) sample mean at different sample sizes. Let \\(X_1, \\ldots, X_n\\) be iid samples from a Bernoulli with probability of success 0.25. We then draw repeated samples of size \\(n=30\\) and \\(n=100\\) and calculate \\(\\sqrt{n}(\\Xbar_n - 0.25)/\\sigma\\) for each random sample. Figure 3.2 plots the density of these two sampling distributions along with a standard normal reference. We can see that even at \\(n=30\\), the rough shape of the density looks normal, with spikes and valleys due to the discrete nature of the data (the sample mean can only take on 31 possible values in this case). By \\(n=100\\), the sampling distribution is very close to the true standard normal.\n\n\n\n\n\nFigure 3.2: Sampling distributions of the normalized sample mean at n=30 and n=100.\n\n\n\n\n\nThere are several properties of convergence in distribution that are helpful to us.\n\nTheorem 3.9 (Properties of convergence in distribution) Let \\(X_n\\) be a sequence of random variables \\(X_1, X_2,\\ldots\\) that converges in distribution to some rv \\(X\\) and let \\(Y_n\\) be a sequence of random variables \\(Y_1, Y_2,\\ldots\\) that converges in probability to some number, \\(c\\). Then,\n\n\\(g(X_n) \\indist g(X)\\) for all continuous functions \\(g\\).\n\\(X_nY_n\\) converges in distribution to \\(cX\\)\n\\(X_n + Y_n\\) converges in distribution to \\(X + c\\)\n\\(X_n / Y_n\\) converges in distribution to \\(X / c\\) if \\(c \\neq 0\\)\n\n\nWe refer to the last three results as Slutsky’s theorem. These results are often crucial for determining an estimator’s asymptotic distribution.\nA critical application of Slutsky’s theorem is when we replace the (unknown) population variance in the CLT with an estimate. Recall the definition of the sample variance as \\[\nS_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\Xbar_n)^2,\n\\] with the sample standard deviation defined as \\(S_{n} = \\sqrt{S_{n}^2}\\). It’s easy to show that these are consistent estimators for their respective population parameters \\[\nS_{n}^2 \\inprob \\sigma^2 = \\V[X_i], \\qquad S_{n} \\inprob \\sigma,\n\\] which, by Slutsky’s theorem, implies that \\[\n\\frac{\\sqrt{n}\\left(\\Xbar_n - \\mu\\right)}{S_n} \\indist \\N(0, 1)\n\\] Comparing this result to the statement of CLT, we see that replacing the population variance with a consistent estimate of the variance (or standard deviation) does not affect the asymptotic distribution.\nLike with the WLLN, the CLT holds for random vectors of sample means, where their centered and scaled versions converge to a multivariate normal distribution with a covariance matrix equal to the covariance matrix of the underlying random vectors of data, \\(\\X_i\\).\n\nTheorem 3.10 If \\(\\mb{X}_i \\in \\mathbb{R}^k\\) are i.i.d. and \\(\\E\\Vert \\mb{X}_i \\Vert^2 &lt; \\infty\\), then as \\(n \\to \\infty\\), \\[\n\\sqrt{n}\\left( \\overline{\\mb{X}}_n - \\mb{\\mu}\\right) \\indist \\N(0, \\mb{\\Sigma}),\n\\] where \\(\\mb{\\mu} = \\E[\\mb{X}_i]\\) and \\(\\mb{\\Sigma} = \\V[\\mb{X}_i] = \\E\\left[(\\mb{X}_i-\\mb{\\mu})(\\mb{X}_i - \\mb{\\mu})'\\right]\\).\n\nNotice that \\(\\mb{\\mu}\\) is the vector of population means for all the random variables in \\(\\X_i\\) and \\(\\mb{\\Sigma}\\) is the variance-covariance matrix for that vector.\n\n\n\n\n\n\nNote\n\n\n\nAs with the notation alert with the WLLN, we are using shorthand here, \\(\\E\\Vert \\mb{X}_i \\Vert^2 &lt; \\infty\\), which implies that \\(\\E[X_{ij}^2] &lt; \\infty\\) for all \\(j = 1,\\ldots, k\\), or equivalently, that the variances of each variable in the sample means has finite variance."
  },
  {
    "objectID": "03_asymptotics.html#confidence-intervals",
    "href": "03_asymptotics.html#confidence-intervals",
    "title": "3  Asymptotics",
    "section": "3.8 Confidence intervals",
    "text": "3.8 Confidence intervals\nWe now turn to an essential application of the central limit theorem: confidence intervals.\nYou have run your experiment and presented your readers with your single best guess about the treatment effect with the difference in sample means. You may have also presented the estimated standard error of this estimate to give readers a sense of how variable the estimate is. But none of these approaches answer a fairly compelling question: what range of values of the treatment effect is plausible given the data we observe?\nA point estimate typically has 0 probability of being the exact true value, but intuitively we hope that the true value is close to this estimate. Confidence intervals make this kind of intuition more formal by instead estimating ranges of values with a fixed percentage of these ranges containing the actual parameter value.\nWe begin with the basic definition of a confidence interval.\n\nDefinition 3.6 A \\(1-\\alpha\\) confidence interval for a real-valued parameter \\(\\theta\\) is a pair of statistics \\(L= L(X_1, \\ldots, X_n)\\) and \\(U = U(X_1, \\ldots, X_n)\\) such that \\(L &lt; U\\) for all values of the sample and such that \\[\n\\P(L \\leq \\theta \\leq U \\mid \\theta) \\geq 1-\\alpha, \\quad \\forall \\theta \\in \\Theta.\n\\]\n\nWe say that a \\(1-\\alpha\\) confidence interval covers (contains, captures, traps, etc.) the true value at least \\(100(1-\\alpha)\\%\\) of the time, and we refer to \\(1-\\alpha\\) as the coverage probability or simply coverage. Typical confidence intervals include 95% percent (\\(\\alpha = 0.05\\)) and 90% (\\(\\alpha = 0.1\\)).\nSo a confidence interval is a random interval with a particular guarantee about how often it will contain the true value. It’s important to remember what is random and what is fixed in this setup. The interval varies from sample to sample, but the true value of the parameter stays fixed, and the coverage is how often we should expect the interval to contain that true value. The “repeating my sample over and over again” analogy can break down very quickly, so it’s sometimes helpful to interpret it as giving guarantees across confidence intervals across different experiments. In particular, suppose that a journal publishes 100 quantitative articles annually, each producing a single 95% confidence interval for their quantity of interest. Then, if the confidence intervals are valid, we should expect 95 of those confidence intervals to contain the true value.\n\n\n\n\n\n\nWarning\n\n\n\nSuppose you calculate a 95% confidence interval, \\([0.1, 0.4]\\). It’s tempting to make a probability statement like \\(\\P(0.1 \\leq \\theta \\leq 0.4 \\mid \\theta) = 0.95\\) or that there’s a 95% chance that the parameter is in \\([0.1, 0.4]\\). But looking at the probability statement, everything on the left-hand side of the conditioning bar is fixed, so the probability either has to be 0 (\\(\\theta\\) is outside the interval) or 1 (\\(\\theta\\) is in the interval). The coverage probability of a confidence interval refers to its status as a pair of random variables, \\((L, U)\\), not any particular realization of those variables like \\((0.1, 0.4)\\). As an analogy, consider if calculated the sample mean as \\(0.25\\) and then try to say that \\(0.25\\) is unbiased for the population mean. This statement doesn’t make sense because unbiasedness refers to how the sample mean varies from sample to sample.\n\n\nIn most cases, we will not be able to derive exact confidence intervals but rather confidence intervals that are asymptotically valid, which means that if we write the interval as a function of the sample size, \\((L_n, U_n)\\), they would have asymptotic coverage \\[\n\\lim_{n\\to\\infty} \\P(L_n \\leq \\theta \\leq U_n) \\geq 1-\\alpha \\quad\\forall\\theta\\in\\Theta.\n\\]\nAsymptotic coverage is the property we can show for most confidence intervals since we usually rely on large sample approximations based on the central limit theorem.\n\n3.8.1 Deriving confidence intervals\nIf you have taken any statistics before, you probably have seen the standard formula for the 95% confidence interval of the sample mean, \\[\n\\left[\\Xbar_n - 1.96\\frac{s}{\\sqrt{n}},\\; \\Xbar_n + 1.96\\frac{s}{\\sqrt{n}}\\right],\n\\] where you can recall that \\(s\\) is the sample standard deviation and \\(s/\\sqrt{n}\\) is the estimate of the standard error of the sample mean. If this is a 95% confidence interval, then the probability that it contains the population mean \\(\\mu\\) should be 0.95, but how can we derive this? We can justify this logic using the central limit theorem, and the argument will hold for any asymptotically normal estimator.\nLet’s say that we have an estimator, \\(\\widehat{\\theta}_n\\) for the parameter \\(\\theta\\) with estimated standard error \\(\\widehat{\\se}[\\widehat{\\theta}_n]\\). If the estimator is asymptotically normal, then in large samples, we know that \\[\n\\frac{\\widehat{\\theta}_n - \\theta}{\\widehat{\\se}[\\widehat{\\theta}_n]} \\sim \\N(0, 1).\n\\] We can then use our knowledge of the standard normal and the empirical rule to find \\[\n\\P\\left( -1.96 \\leq \\frac{\\widehat{\\theta}_n - \\theta}{\\widehat{\\se}[\\widehat{\\theta}_n]} \\leq 1.96\\right) = 0.95\n\\] and by multiplying each part of the inequality by \\(\\widehat{\\se}[\\widehat{\\theta}_n]\\), we get \\[\n\\P\\left( -1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\leq \\widehat{\\theta}_n - \\theta \\leq 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n]\\right) = 0.95,\n\\] We then subtract all parts by the estimator to get \\[\n\\P\\left(-\\widehat{\\theta}_n - 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\leq - \\theta \\leq -\\widehat{\\theta}_n + 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n]\\right) = 0.95,\n\\] and finally we multiply all parts by \\(-1\\) (and flipping the inequalities) to arrive at \\[\n\\P\\left(\\widehat{\\theta}_n - 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\leq \\theta \\leq \\widehat{\\theta}_n + 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n]\\right) = 0.95.\n\\] To connect back to the definition of the confidence interval, we have now shown that the random interval \\([L, U]\\) where \\[\n\\begin{aligned}\n  L = L(X_1, \\ldots, X_n) &= \\widehat{\\theta}_n - 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n] \\\\\n  U = U(X_1, \\ldots, X_n) &= \\widehat{\\theta}_n + 1.96\\,\\widehat{\\se}[\\widehat{\\theta}_n],\n\\end{aligned}\n\\] is an asymptotically valid estimator.3 Replacing \\(\\Xbar_n\\) for \\(\\widehat{\\theta}_n\\) and \\(s/\\sqrt{n}\\) for \\(\\widehat{\\se}[\\widehat{\\theta}_n]\\) establishes how the standard 95% confidence interval for the sample mean above is asymptotically valid.\n\n\n\n\n\nFigure 3.3: Critical values for the standard normal.\n\n\n\n\nHow can we generalize this to \\(1-\\alpha\\) confidence intervals? For a standard normal rv, \\(Z\\), we know that \\[\n\\P(-z_{\\alpha/2} \\leq Z \\leq z_{\\alpha/2}) = 1-\\alpha\n\\] which implies that we can obtain a \\(1-\\alpha\\) asymptotic confidence intervals by using the interval \\([L, U]\\), where \\[\nL = \\widehat{\\theta}_{n} - z_{\\alpha/2} \\widehat{\\se}[\\widehat{\\theta}_{n}], \\quad U = \\widehat{\\theta}_{n} + z_{\\alpha/2} \\widehat{\\se}[\\widehat{\\theta}_{n}].\n\\] This is sometimes shortened to \\(\\widehat{\\theta}_n \\pm z_{\\alpha/2} \\widehat{\\se}[\\widehat{\\theta}_{n}]\\). Remember that we can obtain the values of \\(z_{\\alpha/2}\\) easily from R:\n\n## alpha = 0.1 for 90% CI\nqnorm(0.1 / 2, lower.tail = FALSE)\n\n[1] 1.644854\n\n\nAs a concrete example, then, we could derive a 90% asymptotic confidence interval for the sample mean as \\[\n\\left[\\Xbar_{n} - 1.64 \\frac{\\widehat{\\sigma}}{\\sqrt{n}}, \\Xbar_{n} + 1.64 \\frac{\\widehat{\\sigma}}{\\sqrt{n}}\\right]\n\\]\n\n\n3.8.2 Interpreting confidence intervals\nRemember that the interpretation of confidence is how the random interval performs over repeated samples. A valid 95% confidence interval is a random interval containing the true value in 95% of samples. Simulating repeated samples helps clarify this.\n\nExample 3.5 Suppose we are taking samples of size \\(n=500\\) of random variables where \\(X_i \\sim \\N(1, 10)\\), and we want to estimate the population mean \\(\\E[X] = 1\\). To do so, we repeat the following steps:\n\nDraw a sample of \\(n=500\\) from \\(\\N(1, 10)\\).\nCalculate the 95% confidence interval sample mean \\(\\Xbar_n \\pm 1.96\\widehat{\\sigma}/\\sqrt{n}\\).\nPlot the intervals along the x-axis and color them blue if they contain the truth (1) and red if not.\n\nFigure 3.4 shows 100 iterations of these steps. We see that, as expected, most calculated CIs contain the true value. Five random samples produce intervals that fail to include 1, an exact coverage rate of 95%. Of course, this is just one simulation, and a different set of 100 random samples might have produced a slightly different coverage rate. The guarantee of the 95% confidence intervals is that if we were to continue to take these repeated samples, the long-run frequency of intervals covering the truth would approach 0.95.\n\n\n\n\n\nFigure 3.4: 95% confidence intervals from 100 random samples. Intervals are blue if they contain the truth and red if they do not."
  },
  {
    "objectID": "03_asymptotics.html#sec-delta-method",
    "href": "03_asymptotics.html#sec-delta-method",
    "title": "3  Asymptotics",
    "section": "3.9 Delta method",
    "text": "3.9 Delta method\nSuppose that we know that an estimator follows the CLT, and so we have \\[\n\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta \\right) \\indist \\N(0, V),\n\\] but we actually want to estimate \\(h(\\theta)\\) so we use the plug-in estimator, \\(h(\\widehat{\\theta}_n)\\). It seems like we should be able to apply part 1 of Theorem 3.9. Still, the CLT established the large-sample distribution of the centered and scaled random sequence, \\(\\sqrt{n}(\\widehat{\\theta}_n - \\theta)\\), not to the original estimator itself like we would need to investigate the asymptotic distribution of \\(h(\\widehat{\\theta}_n)\\). We can use a little bit of calculus to get an approximation of the distribution we need.\n\nTheorem 3.11 If \\(\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta\\right) \\indist \\N(0, V)\\) and \\(h(u)\\) is continuously differentiable in a neighborhood around \\(\\theta\\), then as \\(n\\to\\infty\\), \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta}_n) - h(\\theta) \\right) \\indist \\N(0, (h'(\\theta))^2 V).\n\\]\n\nUnderstanding what’s happening here is useful since it might help give intuition as to when this might go wrong. Why do we focus on continuously differentiable functions, \\(h()\\)? These functions can be well-approximated with a line in a neighborhood around a given point like \\(\\theta\\). In Figure 3.5, we show this where the tangent line at \\(\\theta_0\\), which has slope \\(h'(\\theta_0)\\), is very similar to \\(h(\\theta)\\) for values close to \\(\\theta_0\\). Because of this, we can approximate the difference between \\(h(\\widehat{\\theta}_n)\\) and \\(h(\\theta_0)\\) with the what this tangent line would give us: \\[\n\\underbrace{\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right)}_{\\text{change in } y} \\approx \\underbrace{h'(\\theta_0)}_{\\text{slope}} \\underbrace{\\left(\\widehat{\\theta}_n - \\theta_0\\right)}_{\\text{change in } x},\n\\] and then multiplying both sides by the \\(\\sqrt{n}\\) gives \\[\n\\sqrt{n}\\left(h(\\widehat{\\theta_n}) - h(\\theta_0)\\right) \\approx h'(\\theta_0)\\sqrt{n}\\left(\\widehat{\\theta}_n - \\theta_0\\right).\n\\] The right-hand side of this approximation converges to \\(h'(\\theta_0)Z\\), where \\(Z\\) is a random variable with \\(\\N(0, V)\\). The variance of this quantity will be \\[\n\\V[h'(\\theta_0)Z] = (h'(\\theta_0))^2\\V[Z] = (h'(\\theta_0))^2V,\n\\] by the properties of variances.\n\n\n\n\n\nFigure 3.5: Linear approximation to nonlinear functions.\n\n\n\n\n\nExample 3.6 Let’s return to the iid sample \\(X_1, \\ldots, X_n\\) with mean \\(\\mu = \\E[X_i]\\) and variance \\(\\sigma^2 = \\V[X_i]\\). From the CLT, we know that \\(\\sqrt{n}(\\Xbar_n - \\mu) \\indist \\N(0, \\sigma^2)\\). Suppose that we want to estimate \\(\\log(\\mu)\\), so we use the plug-in estimator \\(\\log(\\Xbar_n)\\) (assuming that \\(X_i &gt; 0\\) for all \\(i\\) so that we can take the log). What is the asymptotic distribution of this estimator? This is a situation where \\(\\widehat{\\theta}_n = \\Xbar_n\\) and \\(h(\\mu) = \\log(\\mu)\\). From basic calculus, we know that \\[\nh'(\\mu) = \\frac{\\partial \\log(\\mu)}{\\partial \\mu} = \\frac{1}{\\mu},\n\\] so applying the delta method, we can determine that \\[\n\\sqrt{n}\\left(\\log(\\Xbar_n) - \\log(\\mu)\\right) \\indist \\N\\left(0,\\frac{\\sigma^2}{\\mu^2} \\right).\n\\]\n\n\nExample 3.7 What about estimating the \\(\\exp(\\mu)\\) with \\(\\exp(\\Xbar_n)\\)? Recall that \\[\nh'(\\mu) = \\frac{\\partial \\exp(\\mu)}{\\partial \\mu} = \\exp(\\mu)\n\\] so applying the delta method, we have \\[\n\\sqrt{n}\\left(\\exp(\\Xbar_n) - \\exp(\\mu)\\right) \\indist \\N(0, \\exp(2\\mu)\\sigma^2),\n\\] since \\(\\exp(\\mu)^2 = \\exp(2\\mu)\\).\n\nLike all of the results in this chapter, there is a multivariate version of the delta method that is incredibly useful in practical applications. We often will combine two different estimators (or two different estimated parameters) to estimate another quantity. We now let \\(\\mb{h}(\\mb{\\theta}) = (h_1(\\mb{\\theta}), \\ldots, h_m(\\mb{\\theta}))\\) map from \\(\\mathbb{R}^k \\to \\mathbb{R}^m\\) and be continuously differentiable (we make the function bold since it returns an \\(m\\)-dimensional vector). It will help us to use more compact matrix notation if we introduce a \\(m \\times k\\) Jacobian matrix of all partial derivatives \\[\n\\mb{H}(\\mb{\\theta}) = \\mb{\\nabla}_{\\mb{\\theta}}\\mb{h}(\\mb{\\theta}) = \\begin{pmatrix}\n  \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_1(\\mb{\\theta})}{\\partial \\theta_k} \\\\\n  \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_2(\\mb{\\theta})}{\\partial \\theta_k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_1} & \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_2} & \\cdots & \\frac{\\partial h_m(\\mb{\\theta})}{\\partial \\theta_k}\n\\end{pmatrix},\n\\] which we can use to generate the equivalent multivariate linear approximation \\[\n\\left(\\mb{h}(\\widehat{\\mb{\\theta}}_n) - \\mb{h}(\\mb{\\theta}_0)\\right) \\approx \\mb{H}(\\mb{\\theta}_0)'\\left(\\widehat{\\mb{\\theta}}_n - \\mb{\\theta}_0\\right).\n\\] We can use this fact to derive the multivariate delta method.\n\nTheorem 3.12 Suppose that \\(\\sqrt{n}\\left(\\widehat{\\mb{\\theta}}_n - \\mb{\\theta}_0 \\right) \\indist \\N(0, \\mb{\\Sigma})\\), then for any function \\(\\mb{h}\\) that is continuously differentiable in a neighborhood of \\(\\mb{\\theta}_0\\), we have \\[\n\\sqrt{n}\\left(\\mb{h}(\\widehat{\\mb{\\theta}}_n) - \\mb{h}(\\mb{\\theta}_0) \\right) \\indist \\N(0, \\mb{H}\\mb{\\Sigma}\\mb{H}'),\n\\] where \\(\\mb{H} = \\mb{H}(\\mb{\\theta}_0)\\).\n\nThis result follows from the approximation above plus rules about variances of random vectors. Remember that for any compatible matrix of constants, \\(\\mb{A}\\), we have \\(\\V[\\mb{A}'\\mb{Z}] = \\mb{A}\\V[\\mb{Z}]\\mb{A}'\\). You can see that the matrix of constants appears twice here, like the matrix version of the “squaring the constant” rule for variance.\nThe delta method is handy for generating closed-form approximations for asymptotic standard errors, but the math is often quite complex for even simple estimators. It is usually more straightforward for applied researchers to use computational tools like the bootstrap to approximate the standard errors we need. The bootstrap has the trade-off of taking more computational time to implement than the delta method. Still, it is more easily adaptable across different estimators and domains with little human thinking time."
  },
  {
    "objectID": "03_asymptotics.html#footnotes",
    "href": "03_asymptotics.html#footnotes",
    "title": "3  Asymptotics",
    "section": "",
    "text": "Due to Wasserman (2004), Chapter 5.↩︎\nTechnically, a sequence can also converge in probability to another random variable, but the use case of converging to a single number is much more common in evaluating estimators.↩︎\nImplicit in this analysis is that the standard error estimate is consistent.↩︎"
  },
  {
    "objectID": "04_hypothesis_tests.html#the-lady-tasting-tea",
    "href": "04_hypothesis_tests.html#the-lady-tasting-tea",
    "title": "4  Hypothesis tests",
    "section": "4.1 The lady tasting tea",
    "text": "4.1 The lady tasting tea\nThe lady tasting tea exemplifies the core ideas behind hypothesis testing due to R.A. Fisher.1 Fisher had prepared tea for his colleague, the algologist Muriel Bristol. Knowing that she preferred milk in her tea, he poured milk into a tea cup and then poured the hot tea into the milk. Bristol rejected the cup, stating that she preferred pouring the tea first, then milk. Fisher was skeptical at the idea anyone could tell the difference between a cup poured milk-first or tea-first. So he and another colleague, William Roach, devised a test to see if Bristol could distinguish the two preparation methods.\nFisher and Roach prepared 8 cups of tea, four milk-first and four tea-first. They then presented the cups to Bristol in a random order (though she knew there were 4 of each type), and she proceeded to identify all of the cups correctly. At first glance, this seems like good evidence that she can tell the difference between the two types, but a skeptic like Fisher raised the question: “could she have just been randomly guessing and got lucky?” This led Fisher to a statistical thought experiment: what would the probability of guessing the correct cups be if she were guessing randomly?\nTo calculate the probability of Bristol’s achievement, we can note that “randomly guessing” here would mean that she was selecting a group of 4 cups to be labeled milk-first from the 8 cups available. Using basic combinatorics, we can calculate there are 70 ways to choose 4 cups among 8, but only 1 of those arrangements would be correct. Thus, if randomly guessing means choosing among those 70 options with equal chance, then the probability of guessing the right set of cups is 1/70 or \\(\\approx 0.014\\). The low probability implies that the hypothesis of random guessing may be implausible.\nThe story of the lady tasting tea encapsulates many of the core elements of hypothesis testing. Hypothesis testing is about taking our observed estimate (Bristol guessing all the cups correctly) and seeing how likely that observed estimate would be under some assumption or hypothesis about the data-generating process (Bristol was randomly guessing). When the observed estimate is unlikely under the maintained hypothesis, we might view this as evidence against that hypothesis. Thus, hypothesis tests help us assess evidence for particular guesses about the DGP.\n\n\n\n\n\n\nNotation alert\n\n\n\nFor the rest of this chapter, we’ll introduce the concepts following the notation in the past chapters. We’ll usually assume that we have a random (iid) sample of random variables \\(X_1, \\ldots, X_n\\) from a distribution, \\(F\\). We’ll focus on estimating some parameter, \\(\\theta\\), of this distribution (like the mean, median, variance, etc.). We’ll refer to \\(\\Theta\\) as the set of possible values of \\(\\theta\\) or the parameter space."
  },
  {
    "objectID": "04_hypothesis_tests.html#hypotheses",
    "href": "04_hypothesis_tests.html#hypotheses",
    "title": "4  Hypothesis tests",
    "section": "4.2 Hypotheses",
    "text": "4.2 Hypotheses\nIn the context of hypothesis testing, hypotheses are just statements about the population distribution. In particular, we will make statements that \\(\\theta = \\theta_0\\) where \\(\\theta_0 \\in \\Theta\\) is the hypothesized value of \\(\\theta\\). Hypotheses are ubiquitous in empirical work, but here are some examples to give you a flavor:\n\nThe population proportion of US citizens that identify as Democrats is 0.33.\nThe population difference in average voter turnout between households who received get-out-the-vote mailers vs. those who did not is 0.\nThe difference in the average incidence of human rights abuse in countries that signed a human rights treaty vs. those countries that did not sign is 0.\n\nEach of these is a statement about the true DGP. The latter two are very common: when \\(\\theta\\) represents the difference in means between two groups, then \\(\\theta = 0\\) is the hypothesis of no actual difference in population means or no treatment effect (if the causal effect is identified).\nThe goal of hypothesis testing is to adjudicate between two complementary hypotheses.\n\nDefinition 4.1 The two hypotheses in a hypothesis test are called the null hypothesis and the alternative hypothesis, denoted as \\(H_0\\) and \\(H_1\\), respectively.\n\nThese hypotheses are complementary, so if the null hypothesis \\(H_0: \\theta \\in \\Theta_0\\), then the alternative hypothesis is \\(H_1: \\theta \\in \\Theta_0^c\\). The “null” in null hypothesis might seem odd until you realize that most null hypotheses are that there is no effect of some treatment or no difference in means. For example, suppose \\(\\theta\\) is the difference in mean support for expanding legal immigration between a treatment group that received a pro-immigrant message and some facts about immigration and a control group that just received the factual information. Then, the typical null hypothesis would be no difference in means or \\(H_0: \\theta = 0\\), and the alternative would be \\(H_1: \\theta \\neq 0\\).\nThere are two types of tests that differ in the form of their null and alternative hypotheses. A two-sided test is of the form \\[\nH_0: \\theta = \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta \\neq \\theta_0,\n\\] where the “two-sided” part refers to how the alternative contains values of \\(\\theta\\) above and below the null value \\(\\theta_0\\). A one-sided test has the form \\[\nH_0: \\theta \\leq \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta &gt; \\theta_0,\n\\] or \\[\nH_0: \\theta \\geq \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta &lt; \\theta_0.\n\\] Two-sided tests are much more common in the social sciences, where we want to know if there is any evidence, positive or negative, against the presumption of no treatment effect or no relationship between two variables. One-sided tests are for situations where we only want evidence in one direction, which is rarely relevant to social science research. One-sided tests also have the downside of being misused to inflate the strength of evidence against the null and should be avoided. Unfortunately, the math of two-sided tests is also more complicated."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-procedure-of-hypothesis-testing",
    "href": "04_hypothesis_tests.html#the-procedure-of-hypothesis-testing",
    "title": "4  Hypothesis tests",
    "section": "4.3 The procedure of hypothesis testing",
    "text": "4.3 The procedure of hypothesis testing\nAt the most basic level, a hypothesis test is a rule that specifies values of the sample data for which we will decide to reject the null hypothesis. Let \\(\\mathcal{X}_n\\) be the range of the sample—that is, all possible vectors \\((x_1, \\ldots, x_n)\\) that have a positive probability of occurring. Then, a hypothesis test describes a region of this space, \\(R \\subset \\mathcal{X}_n\\), called the rejection region where when \\((X_1, \\ldots, X_n) \\in R\\) we will reject \\(H_0\\) and when the data is outside this region, \\((X_1, \\ldots, X_n) \\notin R\\) we retain, accept, or fail to reject the null hypothesis.2\nHow do we decide what the rejection region should be? Even though we define the rejection region in terms of the sample space, \\(\\mathcal{X}_n\\), it’s unwieldy to work with the entire vector of data. Instead, we often formulate the rejection region in terms of a test statistic, \\(T = T(X_1, \\ldots, X_n)\\), where the rejection region becomes \\[\nR = \\left\\{(x_1, \\ldots, x_n) : T(x_1, \\ldots, x_n) &gt; c\\right\\},\n\\] where \\(c\\) is called the critical value. This expression says that the rejection region is the part of the sample space that makes the test statistic sufficiently large. We reject null hypotheses when the observed data is incompatible with those hypotheses, where the test statistic should be a measure of this incompatibility. Note that the test statistic is a random variable and has a distribution—we will exploit this to understand the different properties of a hypothesis test.\n\nExample 4.1 Suppose that \\((X_1, \\ldots, X_n)\\) represents a sample of US citizens where \\(X_i = 1\\) indicates support for the current US president and \\(X_i = 0\\) means no support. We might be interested in the test of the null hypothesis that the president does not have the support of a majority of American citizens. Let \\(\\mu = \\E[X_i] = \\P(X_i = 1)\\). Then, a one-sided test would compare the two hypotheses: \\[\nH_0: \\mu \\leq 0.5 \\quad\\text{versus}\\quad H_1: \\mu &gt; 0.5.\n\\] In this case, we might use the sample mean as the test statistic, so that \\(T(X_1, \\ldots, X_n) = \\Xbar_n\\) and we have to find some threshold above 0.5 such that we would reject the null, \\[\nR = \\left\\{(x_1, \\ldots, x_n): \\Xbar_n &gt; c\\right\\}.\n\\] In words, how much support should we see for the current president before we reject the notion that they lack majority support? Below we will select the critical value, \\(c\\), to have beneficial statistical properties.\n\nThe structure of a reject region will depend on whether a test is one- or two-sided. One-sided tests will take the form \\(T &gt; c\\), whereas two-sided tests will take the form \\(|T| &gt; c\\) since we want to count deviations from either side of the null hypothesis as evidence against that null."
  },
  {
    "objectID": "04_hypothesis_tests.html#testing-errors",
    "href": "04_hypothesis_tests.html#testing-errors",
    "title": "4  Hypothesis tests",
    "section": "4.4 Testing errors",
    "text": "4.4 Testing errors\nHypothesis tests end with a decision to reject the null hypothesis or not, but this might be an incorrect decision. In particular, there are two ways to make errors and two ways to be correct in this setting, as shown in Table 4.1. The labels are confusing, but it’s helpful to remember that type I errors (said “type one”) are labeled so because they are the worse of the two types of errors. These errors occur when we reject a null (say there is a true treatment effect or relationship) when the null is true (there is no true treatment effect or relationship). Type I errors are what we see in the replication crisis: lots of “significant” effects that turn out later to be null. Type II errors (said “type two”) are considered less problematic: there is a true relationship, but we cannot detect it with our test (we cannot reject the null).\n\n\nTable 4.1: Typology of testing errors\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nRetain \\(H_0\\)\nAwesome\nType II error\n\n\nReject \\(H_0\\)\nType I error\nGreat\n\n\n\n\nIdeally, we would minimize the chances of making either a type I or type II error. Unfortunately, because the test statistic is a random variable, we cannot remove the probability of an error altogether. Instead, we will derive tests with some guaranteed performance to minimize the probability of type I error. To derive this, we can define the power function of a test, \\[\n\\pi(\\theta) = \\P\\left( \\text{Reject } H_0 \\mid \\theta \\right) = \\P\\left( T \\in R \\mid \\theta \\right),\n\\] which is the probability of rejection as a function of the parameter of interest, \\(\\theta\\). The power function tells us, for example, how likely we are to reject the null of no treatment effect as we vary the actual size of the treatment effect.\nWe can define the probability of type I error from the power function.\n\nDefinition 4.2 The size of a hypothesis test with the null hypothesis \\(H_0: \\theta = \\theta_0\\) is \\[\n\\pi(\\theta_0) = \\P\\left( \\text{Reject } H_0 \\mid \\theta_0 \\right).\n\\]\n\nYou can think of the size of a test as the rate of false positives (or false discoveries) produced by the test. Figure 4.1 shows an example of rejection regions, size, and power for a one-sided test. In the left panel, we have the distribution of the test statistic under the null, with \\(H_0: \\theta = \\theta_0\\), and the rejection region is defined by values \\(T &gt; c\\). The shaded grey region is the probability of rejection under this null hypothesis or the size of the test. Sometimes, we will get extreme samples by random chance, even under the null, leading to false discoveries.3\nIn the right panel, we overlay the distribution of the test statistic under one particular alternative, \\(\\theta = \\theta_1 &gt; \\theta_0\\). The red-shaded region is the probability of rejecting the null when this alternative is true for the power—it’s the probability of correctly rejecting the null when it is false. Intuitively, we can see that alternatives that produce test statistics closer to the rejection region will have higher power. This makes sense: detecting big deviations from the null should be easier than detecting minor ones.\n\n\n\n\n\nFigure 4.1: Size of a test and power against an alternative.\n\n\n\n\nFigure 4.1 also hints at a tradeoff between size and power. Notice that we could make the size smaller (lower the false positive rate) by increasing the critical value to \\(c' &gt; c\\). This would make the probability of being in the rejection region smaller, \\(\\P(T &gt; c' \\mid \\theta_0) &lt; \\P(T &gt; c \\mid \\theta_0)\\), leading to a lower-sized test. Unfortunately, it would also reduce power in the right panel since the probability of being in the rejection region will be lower under any alternative, \\(\\P(T &gt; c' \\mid \\theta_1) &lt; \\P(T &gt; c \\mid \\theta_1)\\). This means we usually cannot simultaneously reduce both types of errors."
  },
  {
    "objectID": "04_hypothesis_tests.html#determining-the-rejection-region",
    "href": "04_hypothesis_tests.html#determining-the-rejection-region",
    "title": "4  Hypothesis tests",
    "section": "4.5 Determining the rejection region",
    "text": "4.5 Determining the rejection region\nIf we cannot simultaneously optimize a test’s size and power, how should we determine where the rejection region is? That is, how should we decide what empirical evidence will be strong enough for us to reject the null? The standard approach to this problem in hypothesis testing is to control the size of a test (that is, control the rate of false positives) and try to maximize the power of the test subject to that constraint. So we say, “I’m willing to accept at most x%” of findings will be false positives and do whatever we can to maximize power subject to that constraint.\n\nDefinition 4.3 A test has significance level \\(\\alpha\\) if its size is less than or equal to \\(\\alpha\\), or \\(\\pi(\\theta_0) \\leq \\alpha\\).\n\nA test with a significance level of \\(\\alpha = 0.05\\) will have a false positive/type I error rate no larger than 0.05. This level is widespread in the social sciences, though you also will see \\(\\alpha = 0.01\\) or \\(\\alpha = 0.1\\). Frequentists justify this by saying this means that with \\(\\alpha = 0.05\\), there will only be at most 5% of studies that will produce false discoveries.\nOur task is to construct the rejection region so that the null distribution of the test statistic \\(G_0(t) = \\P(T \\leq t \\mid \\theta_0)\\) has less than \\(\\alpha\\) probability in that region. One-sided tests like in Figure 4.1 are the easiest to show, even though we warned you not to use them. We want to choose \\(c\\) that puts no more than \\(\\alpha\\) probability in the tail, or \\[\n\\P(T &gt; c \\mid \\theta_0) = 1 - G_0(c) \\leq \\alpha.\n\\] Remember that the smaller the value of \\(c\\) we can use will maximize power, which implies that the critical value for the maximum power while maintaining the significance level is when \\(1 - G_0(c) = \\alpha\\). We can use the quantile function of the null distribution to find the exact value of \\(c\\) we need, \\[\nc = G^{-1}_0(1 - \\alpha),\n\\] which is just fancy math to say, “the value at which \\(1-\\alpha\\) of the null distribution is below.”\nThe determination of the rejection region follows the same principles for two-sided tests, but it is slightly more complicated because we reject when the magnitude of the test statistic is large, \\(|T| &gt; c\\). Figure 4.2 shows that basic setup. Notice that because there are two (disjoint) regions, we can write the size (false positive rate) as \\[\n\\pi(\\theta_0) = G_0(-c) + 1 - G_0(c).\n\\] In most cases that we will see, the null distribution for such a test will be symmetric around 0 (usually asymptotically standard normal, actually), which means that \\(G_0(-c) = 1 - G_0(c)\\), which implies that the size is \\[\n\\pi(\\theta_0) = 2(1 - G_0(c)).\n\\] Solving for the critical value that would make this \\(\\alpha\\) gives \\[\nc = G^{-1}_0(1 - \\alpha/2).\n\\] Again, this formula can seem dense, but remember what you are doing: finding the value that puts \\(\\alpha/2\\) of the probability of the null distribution in each tail.\n\n\n\n\n\nFigure 4.2: Rejection regions for a two-sided test."
  },
  {
    "objectID": "04_hypothesis_tests.html#hypothesis-tests-of-the-sample-mean",
    "href": "04_hypothesis_tests.html#hypothesis-tests-of-the-sample-mean",
    "title": "4  Hypothesis tests",
    "section": "4.6 Hypothesis tests of the sample mean",
    "text": "4.6 Hypothesis tests of the sample mean\nLet’s go through an extended example about hypothesis testing of a sample mean, sometimes called a one-sample test. Let’s say \\(X_i\\) are feeling thermometer scores about “liberals” as a group on a scale of 0 to 100, with values closer to 0 indicating cooler feelings about liberals and values closer to 100 indicating warmer feelings about liberals. We want to know if the population average differs from a neutral value of 50. We can write this two-sided test as \\[\nH_0: \\mu = 50 \\quad\\text{versus}\\quad H_1: \\mu \\neq 50,\n\\] where \\(\\mu = \\E[X_i]\\). The standard test statistic for this type of test is the so-called t-statistic, \\[\nT = \\frac{\\left( \\Xbar_n - \\mu_0 \\right)}{\\sqrt{s^2 / n}} =\\frac{\\left( \\Xbar_n - 50 \\right)}{\\sqrt{s^2 / n}},\n\\] where \\(\\mu_0\\) is the null value of interest and \\(s^2\\) is the sample variance. If the null hypothesis is true, then by the CLT, we know that the t-statistic is asymptotically normal, \\(T \\indist \\N(0, 1)\\). Thus, we can approximate the null distribution with the standard normal!\nLet’s create a test with level \\(\\alpha = 0.05\\). Then we need to find the rejection region that puts \\(0.05\\) probability in the tails of the null distribution, which we just saw was \\(\\N(0,1)\\). Let \\(\\Phi()\\) be the CDF for the standard normal and let \\(\\Phi^{-1}()\\) be the quantile function for the standard normal. Drawing on what we developed above, you can find the value \\(c\\) so that \\(\\P(|T| &gt; c \\mid \\mu_0)\\) is 0.05 with \\[\nc = \\Phi^{-1}(1 - 0.05/2) \\approx 1.96,\n\\] which means that a test where we reject when \\(|T| &gt; 1.96\\) would have a level of 0.05 asymptotically."
  },
  {
    "objectID": "04_hypothesis_tests.html#the-wald-test",
    "href": "04_hypothesis_tests.html#the-wald-test",
    "title": "4  Hypothesis tests",
    "section": "4.7 The Wald test",
    "text": "4.7 The Wald test\nWe can generalize the hypothesis test for the sample mean to estimators more broadly. Let \\(\\widehat{\\theta}_n\\) be an estimator for some parameter \\(\\theta\\) and let \\(\\widehat{\\textsf{se}}[\\widehat{\\theta}_n]\\) be a consistent estimate of the standard error of the estimator, \\(\\textsf{se}[\\widehat{\\theta}_n] = \\sqrt{\\V[\\widehat{\\theta}_n]}\\). We consider the two-sided test \\[\nH_0: \\theta = \\theta_0 \\quad\\text{versus}\\quad H_1: \\theta \\neq \\theta_0.\n\\]\nIn many cases, our estimators will be asymptotically normal by a version of the CLT so that under the null hypothesis, we have \\[\nT = \\frac{\\widehat{\\theta}_n - \\theta_0}{\\widehat{\\textsf{se}}[\\widehat{\\theta}_n]} \\indist \\N(0, 1).\n\\] The Wald test rejects \\(H_0\\) when \\(|T| &gt; z_{\\alpha/2}\\), with \\(z_{\\alpha/2}\\) that puts \\(\\alpha/2\\) in the upper tail of the standard normal. That is, if \\(Z \\sim \\N(0, 1)\\), then \\(z_{\\alpha/2}\\) satisfies \\(\\P(Z \\geq z_{\\alpha/2}) = \\alpha/2\\).\n\n\n\n\n\n\nNote\n\n\n\nIn R, you can find the \\(z_{\\alpha/2}\\) values easily with the qnorm() function:\n\nqnorm(0.05 / 2, lower.tail = FALSE)\n\n[1] 1.959964\n\n\n\n\n\nTheorem 4.1 Asymptotically, the Wald test has size \\(\\alpha\\) such that \\[\n\\P(|T| &gt; z_{\\alpha/2} \\mid \\theta_0) \\to \\alpha.\n\\]\n\nThis result is very general, and it means that many, many hypothesis tests based on estimators will have the same form. The main difference across estimators will be how we calculate the estimated standard error.\n\nExample 4.2 (Difference in proportions) In get-out-the-vote (GOTV) experiments, we might randomly assign a group of citizens to receive mailers encouraging them to vote, whereas a control group receives no message. We’ll define the turnout variables in the treatment group \\(Y_{1}, Y_{2}, \\ldots, Y_{n_t}\\) as iid draws from a Bernoulli distribution with success \\(p_t\\), which represents the population turnout rate among treated citizens. The outcomes in the control group \\(X_{1}, X_{2}, \\ldots, X_{n_c}\\) are iid draws from another Bernoulli distribution with success \\(p_c\\), which represents the population turnout rate among citizens not receiving a mailer.\nOur goal is to learn about the treatment effect of this treatment on whether or not the citizen votes, \\(\\tau = p_t - p_c\\), and we will use the sample difference in means/proportions as our estimator, \\(\\widehat{\\tau} = \\Ybar - \\Xbar\\). To perform a Wald test, we need to know/estimate the standard error of this estimator. Notice that because these are independent samples, the variance is \\[\n\\V[\\widehat{\\tau}_n] = \\V[\\Ybar - \\Xbar] = \\V[\\Ybar] + \\V[\\Xbar] = \\frac{p_t(1-p_t)}{n_t} + \\frac{p_c(1-p_c)}{n_c},\n\\] where the third equality comes from the fact that the underlying outcome variables \\(Y_i\\) and \\(X_j\\) are binary. Obviously, we do not know the true population proportions \\(p_t\\) and \\(p_c\\) (that’s why we’re doing the test!), but we can estimate the standard error by replacing them with their estimates \\[\n\\widehat{\\textsf{se}}[\\widehat{\\tau}] = \\sqrt{\\frac{\\Ybar(1 -\\Ybar)}{n_t} + \\frac{\\Xbar(1-\\Xbar)}{n_c}}.\n\\]\nThe typical null hypothesis test, in this case, is “no treatment effect” vs. “some treatment effect” or \\[\nH_0: \\tau = p_t - p_c = 0 \\quad\\text{versus}\\quad H_1: \\tau \\neq 0,\n\\] which gives the following test statistic for the Wald test \\[\nT = \\frac{\\Ybar - \\Xbar}{\\sqrt{\\frac{\\Ybar(1 -\\Ybar)}{n_t} + \\frac{\\Xbar(1-\\Xbar)}{n_c}}}.\n\\] If we wanted a test with level \\(\\alpha = 0.01\\), we would reject the null when \\(|T| &gt; 2.58\\) since\n\nqnorm(0.01/2, lower.tail = FALSE)\n\n[1] 2.575829\n\n\n\n\nExample 4.3 (Difference in means) Let’s take a similar setting to the last example with randomly assigned treatment and control groups, but now the treatment is an appeal for donations, and the outcomes are continuous measures of how much a person donated to the political campaign. Now the treatment data \\(Y_1, \\ldots, Y_{n_t}\\) are iid draws from a population with mean \\(\\mu_t = \\E[Y_i]\\) and population variance \\(\\sigma^2_t = \\V[Y_i]\\). The control data \\(X_1, \\ldots, X_{n_c}\\) are iid draws (independent of the \\(Y_i\\)) from a population with mean \\(\\mu_c = \\E[X_i]\\) and population variance \\(\\sigma^2_c = \\V[X_i]\\). The parameter of interest is similar to before: the population difference in means, \\(\\tau = \\mu_t - \\mu_c\\), and we’ll form the usual hypothesis test of \\[\nH_0: \\tau = \\mu_t - \\mu_c = 0 \\quad\\text{versus}\\quad H_1: \\tau \\neq 0.\n\\]\nThe only difference between this setting and the difference in proportions is the standard error here will be different because we cannot rely on the Bernoulli. Instead, we’ll use our knowledge of the sampling variance of the sample means and independence between the samples to derive \\[\n\\V[\\widehat{\\tau}] = \\V[\\Ybar] + \\V[\\Xbar] = \\frac{\\sigma^2_t}{n_t} + \\frac{\\sigma^2_c}{n_c},\n\\] where we can come up with an estimate of the unknown population variance with sample variances \\[\n\\widehat{\\se}[\\widehat{\\tau}] = \\sqrt{\\frac{s^2_t}{n_t} + \\frac{s^2_c}{n_c}}.\n\\] We can use this estimator to derive the Wald test statistic of \\[\nT = \\frac{\\widehat{\\tau} - 0}{\\widehat{\\se}[\\widehat{\\tau}]} = \\frac{\\Ybar - \\Xbar}{\\sqrt{\\frac{s^2_t}{n_t} + \\frac{s^2_c}{n_c}}},\n\\] and if we want an asymptotic level of 0.05, we can reject when \\(|T| &gt; 1.96\\)."
  },
  {
    "objectID": "04_hypothesis_tests.html#p-values",
    "href": "04_hypothesis_tests.html#p-values",
    "title": "4  Hypothesis tests",
    "section": "4.8 p-values",
    "text": "4.8 p-values\nThe hypothesis testing framework focuses on actually making a decision in the face of uncertainty. You choose a level of wrongness you are comfortable with (rate of false positives) and then decide null vs. alternative based firmly on the rejection region. When we’re not making a decision, we are somewhat artificially discarding information about the strength of evidence. We “accept” the null if \\(T = 1.95\\) in the last example but reject it if \\(T = 1.97\\) even though these two situations are actually very similar. Just reporting the reject/retain decision also fails to give us a sense of at what other levels we might have rejected the null. Again, this makes sense if we need to make a single decision: other tests don’t matter because we carefully considered our \\(\\alpha\\) level test. But in the lower-stakes world of the academic social sciences, we can afford to be more informative.\nOne alternative to reporting the reject/retain decision is to report a p-value.\n\nDefinition 4.4 The p-value of a test is the probability of observing a test statistic at least as extreme as the observed test statistic in the direction of the alternative hypothesis.\n\nThe line “in the direction of the alternative hypothesis” deals with the unfortunate headache of one-sided versus two-sided tests. For a one-sided test where larger values of \\(T\\) correspond to more evidence for \\(H_1\\), the p-value is \\[\n\\P(T(X_1,\\ldots,X_n) &gt; T \\mid \\theta_0) = 1 - G_0(T),\n\\] whereas for a (symmetric) two-sided test, we have \\[\n\\P(|T(X_1, \\ldots, X_n)| &gt; |T| \\mid \\theta_0) = 2(1 - G_0(|T|)).\n\\]\nIn either case, the interpretation of the p-value is the same. It is the smallest size \\(\\alpha\\) at which a test would reject null. Presenting a p-value allows the reader to determine their own \\(\\alpha\\) level and determine quickly if the evidence would warrant rejecting \\(H_0\\) in that case. Thus, the p-value is a more continuous measure of evidence against the null, where lower values are stronger evidence against the null because the observed result is less likely under the null.\nThere is a lot of controversy surrounding p-values but most of it focuses on arbitrary p-value cutoffs for determining statistical significance and sometimes publication decisions. These problems are not the fault of p-values but rather the hyperfixation on the reject/retain decision for arbitrary test levels like \\(\\alpha = 0.05\\). It might be best to view p-values as a transformation of the test statistic onto a common scale between 0 and 1.\n\n\n\n\n\n\nWarning\n\n\n\nPeople use many statistical shibboleths to purportedly identify people who don’t understand statistics and usually hinge on seemingly subtle differences in interpretation that are easy to miss. If you know the core concepts, the statistical shibboleths tend to be overblown, but it would be malpractice not to flag them for you.\nThe shibboleth with p-values is that sometimes people interpret them as “the probability that the null hypothesis is true.” Of course, this doesn’t make sense from our definition because the p-value conditions on the null hypothesis—it cannot tell us anything about the probability of that null hypothesis. Instead, the metaphor you should always carry is that hypothesis tests are statistical thought experiments and that p-values answer the question: how likely would my data be if the null were true?"
  },
  {
    "objectID": "04_hypothesis_tests.html#power-analysis",
    "href": "04_hypothesis_tests.html#power-analysis",
    "title": "4  Hypothesis tests",
    "section": "4.9 Power analysis",
    "text": "4.9 Power analysis\nImagine you have spent a large research budget on a big experiment to test your amazing theory, and the results come back and… you fail to reject the null of no treatment effect. When this happens, there are two possible states of the world: the null is true, and you correctly identified that, or the null is false but the test had lower power to detect the true effect. Because of this uncertainty after the fact, it is common for researchers to conduct power analyses before running studies that try to forecast what sample size is necessary to ensure you can reject the null under a hypothesized effect size.\nGenerally power analyses involve calculating the power function \\(\\pi(\\theta) = \\P(T(X_1, \\ldots, X_n) \\in R \\mid \\theta)\\) for different values of \\(\\theta\\). It might also involve sample size calculations for a particular alternative, \\(\\theta_1\\). In that case, we try to find the sample size \\(n\\) to make the power \\(\\pi(\\theta_1)\\) as close to a particular value (often 0.8) as possible. It is possible to solve for this sample size in simple one-sided tests explicitly. Still, for more general situations or two-sided tests, we typically need numerical or simulation-based approaches to find the optimal sample size.\nWith Wald tests, we can characterize the power function quite easily, even if it does not allow us to back out sample size calculations easily.\n\nTheorem 4.2 For a Wald test with an asymptotically normal estimator, the power function for a particular alternative \\(\\theta_1 \\neq \\theta_0\\) is \\[\n\\pi(\\theta_1) = 1 - \\Phi\\left( \\frac{\\theta_0 - \\theta_1}{\\widehat{\\se}[\\widehat{\\theta}_n]} + z_{\\alpha/2} \\right) + \\Phi\\left( \\frac{\\theta_0 - \\theta_1}{\\widehat{\\se}[\\widehat{\\theta}_n]}-z_{\\alpha/2} \\right).\n\\]"
  },
  {
    "objectID": "04_hypothesis_tests.html#exact-tests-under-normal-data",
    "href": "04_hypothesis_tests.html#exact-tests-under-normal-data",
    "title": "4  Hypothesis tests",
    "section": "4.10 Exact tests under normal data",
    "text": "4.10 Exact tests under normal data\nThe Wald test above relies on large sample approximations. In finite samples, these approximations may not be valid. Can we get exact inferences at any sample size? Yes, if we make stronger assumptions about the data. In particular, assume a parametric model for the data where \\(X_1,\\ldots,X_n\\) are iid samples from \\(N(\\mu,\\sigma^2)\\). Under a null of \\(H_0: \\mu = \\mu_0\\), we can show that \\[\nT_n = \\frac{\\Xbar_n - \\mu_0}{s_n/\\sqrt{n}} \\sim t_{n-1},\n\\] where \\(t_{n-1}\\) is the Student’s t-distribution with \\(n-1\\) degrees of freedom. This result implies the null distribution is \\(t\\), so we use quantiles of \\(t\\) for critical values. For a one-sided test, \\(c = G^{-1}_0(1 - \\alpha)\\), but now \\(G_0\\) is \\(t\\) with \\(n-1\\) df and so we use qt() instead of qnorm() to calculate these critical values.\nThe critical values for the \\(t\\) distribution are always larger than the normal because the t has fatter tails, as shown in Figure 4.3. As \\(n\\to\\infty\\), however, the \\(t\\) converges to the standard normal, and so it is asymptotically equivalent to the Wald test but slightly more conservative in finite samples. Oddly, most software packages calculate p-values and rejection regions based on the \\(t\\) to exploit this conservativeness.\n\n\n\n\n\nFigure 4.3: Normal versus t distribution."
  },
  {
    "objectID": "04_hypothesis_tests.html#confidence-intervals-and-hypothesis-tests",
    "href": "04_hypothesis_tests.html#confidence-intervals-and-hypothesis-tests",
    "title": "4  Hypothesis tests",
    "section": "4.11 Confidence intervals and hypothesis tests",
    "text": "4.11 Confidence intervals and hypothesis tests\nAt first glance, we may seem sloppy in using \\(\\alpha\\) in deriving a \\(1 - \\alpha\\) confidence interval in the last chapter and an \\(\\alpha\\)-level test in this chapter. In reality, we were foreshadowing the deep connection between the two: every \\(1-\\alpha\\) confidence interval contains all null hypotheses that we would not reject with an \\(\\alpha\\)-level test.\nThis connection is easiest to see with an asymptotically normal estimator, \\(\\widehat{\\theta}_n\\). Consider the hypothesis test of \\[\nH_0: \\theta = \\theta_0 \\quad \\text{vs.}\\quad H_1: \\theta \\neq \\theta_0,\n\\] using the test statistic, \\[\nT = \\frac{\\widehat{\\theta}_{n} - \\theta_{0}}{\\widehat{\\se}[\\widehat{\\theta}_{n}]}.\n\\] As we discussed earlier, an \\(\\alpha = 0.05\\) test would reject this null when \\(|T| &gt; 1.96\\), or when \\[\n|\\widehat{\\theta}_{n} - \\theta_{0}| &gt; 1.96 \\widehat{\\se}[\\widehat{\\theta}_{n}].\n\\] Notice that will be true when \\[\n\\theta_{0} &lt; \\widehat{\\theta}_{n} - 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}]\\quad \\text{ or }\\quad \\widehat{\\theta}_{n} + 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}] &lt; \\theta_{0}\n\\] or, equivalently, that null hypothesis is outside of the 95% confidence interval, \\[\\theta_0 \\notin \\left[\\widehat{\\theta}_{n} - 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}], \\widehat{\\theta}_{n} + 1.96\\widehat{\\se}[\\widehat{\\theta}_{n}]\\right].\\] Of course, our choice of the null hypothesis was arbitrary, which means that any null hypothesis outside the 95% confidence interval would be rejected by a \\(\\alpha = 0.05\\) level test of that null. And any null hypothesis inside the confidence interval is a null hypothesis that we would not reject.\nThis relationship holds more broadly. Any \\(1-\\alpha\\) confidence interval contains all possible parameter values that would not be rejected as the null hypothesis of an \\(\\alpha\\)-level hypothesis test. This connection can be handy for two reasons:\n\nWe can quickly determine if we would reject a null hypothesis at some level by inspecting if it falls in a confidence interval.\nIn some situations, determining a confidence interval might be difficult, but performing a hypothesis test is straightforward. Then, we can find the rejection region for the test and determine what null hypotheses would not be rejected at level \\(\\alpha\\) to formulate the \\(1-\\alpha\\) confidence interval. We call this process inverting a test. A critical application of this method is for formulating confidence intervals for treatment effects based on randomization inference in the finite population analysis of experiments.\n\n\n\n\n\n\nSenn, Stephen. 2012. “Tea for Three: Of Infusions and Inferences and Milk in First.” Significance 9 (6): 30–33. https://doi.org/https://doi.org/10.1111/j.1740-9713.2012.00620.x."
  },
  {
    "objectID": "04_hypothesis_tests.html#footnotes",
    "href": "04_hypothesis_tests.html#footnotes",
    "title": "4  Hypothesis tests",
    "section": "",
    "text": "The analysis here largely comes from Senn (2012).↩︎\nDifferent people and different textbooks describe what to do when we do not reject the null hypothesis in different ways. The terminology is not so important so long as you understand that rejecting the null does not mean the null is logically false, and “accepting” the null does not mean the null is logically true.↩︎\nEagle-eyed readers will notice that the null tested here is a point, while we previously defined the null in a one-sided test as a region \\(H_0: \\theta \\leq \\theta_0\\). Technically, the size of the test will vary based on which of these nulls we pick. In this example, notice that any null to the left of \\(\\theta_0\\) will result in a lower size. And so, the null at the boundary, \\(\\theta_0\\), will maximize the size of the test, making it the most “conservative” null to investigate. Technically, we should define the size of a test as \\(\\alpha = \\sup_{\\theta \\in \\Theta_0} \\pi(\\theta)\\).↩︎"
  },
  {
    "objectID": "06_linear_model.html#why-do-we-need-models",
    "href": "06_linear_model.html#why-do-we-need-models",
    "title": "5  Linear regression",
    "section": "5.1 Why do we need models?",
    "text": "5.1 Why do we need models?\nAt first glance, the connection between the CEF and parametric models might be hazy. For example, imagine we are interested in estimating the average poll wait times (\\(Y_i\\)) for Black voters (\\(X_i = 1\\)) versus non-Black voters (\\(X_i=0\\)). In that case, there are two parameters to estimate, \\[\n\\mu(1) = \\E[Y_i \\mid X_i = 1] \\quad \\text{and}\\quad \\mu(0) = \\E[Y_i \\mid X_i = 0],\n\\] which we could estimate by using the plug-in estimators that replace the population averages with their sample counterparts, \\[\n\\widehat{\\mu}(1) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 1)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 1)} \\qquad \\widehat{\\mu}(0) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 0)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 0)}.\n\\] These are just the sample averages of the wait times for Black and non-Black voters, respectively. And because the race variable here is discrete, we are simply estimating sample means within subpopulations defined by race. The same logic would apply if we had \\(k\\) racial categories: we would have \\(k\\) conditional expectations to estimate and \\(k\\) (conditional) sample means.\nNow imagine that we want to know how the average poll wait time varies as a function of income so that \\(X_i\\) is (essentially) continuous. Now we have a different conditional expectation for every possible dollar amount from 0 to Bill Gates’s income. Imagine we pick a particular income, $42,238, and so we are interested in the conditional expectation \\(\\mu(42,238)= \\E[Y_{i}\\mid X_{i} = 42,238]\\). We could use the same plug-in estimator in the discrete case, \\[\n\\widehat{\\mu}(42,238) = \\frac{\\sum_{i=1}^{n} Y_{i}\\mathbb{1}(X_{i} = 42,238)}{\\sum_{i=1}^{n}\\mathbb{1}(X_{i} = 42,238)}.\n\\] What is the problem with this estimator? In all likelihood, no units in any particular dataset have that exact income, meaning this estimator is undefined (we would be dividing by zero).\nOne solution to this problem is to use subclassification, turn the continuous variable into a discrete one, and proceed with the discrete approach above. We might group incomes into $25,000 bins and then calculate the average wait times of anyone between, say, $25,000 and $50,000 income. When we make this estimator switch for practical purposes, we need to connect it back to the DGP of interest. We could assume that the CEF of interest only depends on these binned means, which would mean we have:\n\\[\n\\mu(x) =\n\\begin{cases}\n  \\E[Y_{i} \\mid 0 \\leq X_{i} &lt; 25,000] &\\text{if } 0 \\leq x &lt; 25,000 \\\\\n  \\E[Y_{i} \\mid 25,000 \\leq X_{i} &lt; 50,000] &\\text{if } 25,000 \\leq x &lt; 50,000\\\\\n  \\E[Y_{i} \\mid 50,000 \\leq X_{i} &lt; 100,000] &\\text{if } 50,000 \\leq x &lt; 100,000\\\\\n  \\vdots \\\\\n  \\E[Y_{i} \\mid 200,000 \\leq X_{i}] &\\text{if } 200,000 \\leq x\\\\\n\\end{cases}\n\\] This approach assumes, perhaps incorrectly, that the average wait time does not vary within the bins. Figure 5.1 shows a hypothetical joint distribution between income and wait times with the true CEF, \\(\\mu(x)\\), shown in red. The figure also shows the bins created by subclassification and the implied CEF if we assume bin-constant means in blue. We can see that the blue function approximates the true CEF but deviates from it close to the bin edges. The trade-off is that once we make the assumption, we only have to estimate one mean for every bin rather than an infinite number of means for each possible income.\n\n\n\n\n\nFigure 5.1: Hypothetical joint distribution of income and poll wait times (contour plot), conditional expectation function (red), and the conditional expectation of the binned income (blue).\n\n\n\n\nSimilarly, we could assume that the CEF follows a simple functional form like a line, \\[\n\\mu(x) = \\E[Y_{i}\\mid X_{i} = x] = \\beta_{0} + \\beta_{1} x.\n\\] This assumption reduces our infinite number of unknowns (the conditional mean at every possible income) to just two unknowns: the slope and intercept. As we will see, we can use the standard ordinary least squares to estimate these parameters. Notice again that if the true CEF is nonlinear, this assumption is incorrect, and any estimate based on this assumption might be biased or even inconsistent.\nWe call the binning and linear assumptions on \\(\\mu(x)\\) functional form assumptions because they restrict the class of functions that \\(\\mu(x)\\) can take. While powerful, these types of assumptions can muddy the roles of defining the quantity of interest and estimation. If our estimator \\(\\widehat{\\mu}(x)\\) performs poorly, it will be difficult to tell if this is because the estimator is flawed or our functional form assumptions are incorrect.\nTo help clarify these issues, we will pursue a different approach: understanding what linear regression can estimate under minimal assumptions and then investigating how well this estimand approximates the true CEF."
  },
  {
    "objectID": "06_linear_model.html#sec-linear-projection",
    "href": "06_linear_model.html#sec-linear-projection",
    "title": "5  Linear regression",
    "section": "5.2 Population linear regression",
    "text": "5.2 Population linear regression\n\n5.2.1 Bivariate linear regression\nLet’s set aside the idea of the conditional expectation function and instead focus on finding the linear function of a single covariate \\(X_i\\) that best predicts the outcome. Remember that linear functions have the form \\(a + bX_i\\). The best linear predictor (BLP) or population linear regression of \\(Y_i\\) on \\(X_i\\) is defined as \\[\nm(x) = \\beta_0 + \\beta_1 x \\quad\\text{where, }\\quad (\\beta_{0}, \\beta_{1}) = \\argmin_{(b_{0}, b_{1}) \\in \\mathbb{R}^{2}}\\; \\E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}].\n\\] That is, the best linear predictor is the line that results in the lowest mean-squared error predictions of the outcome given the covariates, averaging over the joint distribution of the data. This function is a feature of the joint distribution of the data—the DGP—and so represents something that we would like to learn about with our sample. It is an alternative to the CEF for summarizing the relationship between the outcome and the covariate, though we will see that they will sometimes be equal. We call \\((\\beta_{0}, \\beta_{1})\\) the population linear regression coefficients. Notice that \\(m(x)\\) could differ greatly from the CEF \\(\\mu(x)\\) if the latter is nonlinear.\nWe can solve for the best linear predictor using standard calculus (taking the derivative with respect to each coefficient, setting those equations equal to 0, and solving the system of equations). The first-order conditions, in this case, are \\[\n\\begin{aligned}\n  \\frac{\\partial \\E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]}{\\partial b_{0}} = \\E[-2(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})] = 0 \\\\\n  \\frac{\\partial \\E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]}{\\partial b_{1}} = \\E[-2(Y_{i} - \\beta_{0} - \\beta_{1}X_{i})X_{i}] = 0\n\\end{aligned}  \n\\] Given the linearity of expectations, it is easy to solve for \\(\\beta_0\\) in terms of \\(\\beta_1\\), \\[\n\\beta_{0} = \\E[Y_{i}] - \\beta_{1}\\E[X_{i}].\n\\] We can plug this into the first-order condition for \\(\\beta_1\\) to get \\[\n\\begin{aligned}\n  0 &= \\E[Y_{i}X_{i}] - (\\E[Y_{i}] - \\beta_{1}\\E[X_{i}])\\E[X_{i}] - \\beta_{1}\\E[X_{i}^{2}] \\\\\n    &= \\E[Y_{i}X_{i}] - \\E[Y_{i}]\\E[X_{i}] - \\beta_{1}(\\E[X_{i}^{2}] - \\E[X_{i}]^{2}) \\\\\n    &= \\cov(X_{i},Y_{i}) - \\beta_{1}\\V[X_{i}]\\\\\n  \\beta_{1} &= \\frac{\\cov(X_{i},Y_{i})}{\\V[X_{i}]}\n\\end{aligned}\n\\]\nThus the slope on the population linear regression of \\(Y_i\\) on \\(X_i\\) is equal to the ratio of the covariance of the two variables divided by the variance of \\(X_i\\). From this, we can immediately see that the covariance will determine the sign of the slope: positive covariances will lead to positive \\(\\beta_1\\) and negative covariances will lead to negative \\(\\beta_1\\). In addition, we can see that if \\(Y_i\\) and \\(X_i\\) are independent, \\(\\beta_1 = 0\\). The slope scales this covariance by the variance of the covariate, so slopes are lower for more spread-out covariates and higher for more spread-out covariates. If we define the correlation between these variables as \\(\\rho_{YX}\\), then we can relate the coefficient to this quantity as \\[\n\\beta_1 = \\rho_{YX}\\sqrt{\\frac{\\V[Y_i]}{\\V[X_i]}}.\n\\]\nCollecting together our results, we can write the population linear regression as \\[\nm(x) = \\beta_0 + \\beta_1x = \\E[Y_i] + \\beta_1(x - \\E[X_i]),\n\\] which shows how we adjust our best guess about \\(Y_i\\) from the mean of the outcome using the covariate.\nIt’s important to remember that the BLP, \\(m(x)\\), and the CEF, \\(\\mu(x)\\), are distinct entities. If the CEF is nonlinear, as in Figure 5.2, there will be a difference between these functions, meaning that the BLP might produce subpar predictions. Below, we will derive a formal connection between the BLP and the CEF.\n\n\n\n\n\nFigure 5.2: Comparison of the CEF and the best linear predictor.\n\n\n\n\n\n\n5.2.2 Beyond linear approximations\nThe linear part of the best linear predictor is less restrictive than at first glance. We can easily modify the minimum MSE problem to find the best quadratic, cubic, or general polynomial function of \\(X_i\\) that predicts \\(Y_i\\). For example, the quadratic function of \\(X_i\\) that best predicts \\(Y_i\\) would be \\[\nm(X_i, X_i^2) = \\beta_0 + \\beta_1X_i + \\beta_2X_i^2 \\quad\\text{where}\\quad \\argmin_{(b_0,b_1,b_2) \\in \\mathbb{R}^3}\\;\\E[(Y_{i} - b_{0} - b_{1}X_{i} - b_{2}X_{i}^{2})^{2}].\n\\] This equation is now a quadratic function of the covariates, but it is still a linear function of the unknown parameters \\((\\beta_{0}, \\beta_{1}, \\beta_{2})\\) so we will call this a best linear predictor.\nWe could include higher order terms of \\(X_i\\) in the same manner, and as we include more polynomial terms, \\(X_i^p\\), the more flexible the function of \\(X_i\\) we will capture with the BLP. When we estimate the BLP, however, we usually will pay for this flexibility in terms of overfitting and high variance in our estimates.\n\n\n5.2.3 Linear prediction with multiple covariates\nWe now generalize the idea of a best linear predictor to a setting with an arbitrary number of covariates. In this setting, remember that the linear function will be\n\\[\n\\bfx'\\bfbeta = x_{1}\\beta_{1} + x_{2}\\beta_{2} + \\cdots + x_{k}\\beta_{k}.\n\\] We will define the best linear predictor (BLP) to be \\[\nm(\\bfx) = \\bfx'\\bfbeta, \\quad \\text{where}\\quad \\bfbeta = \\argmin_{\\mb{b} \\in \\real^k}\\; \\E\\bigl[ \\bigl(Y_{i} - \\mb{X}_{i}'\\mb{b} \\bigr)^2\\bigr]\n\\]\nThis BLP solves the same fundamental optimization problem as in the bivariate case: it chooses the set of coefficients that minimizes the mean-squared error averaging over the joint distribution of the data.\n\n\n\n\n\n\n\nBest linear projection assumptions\n\n\n\nWithout some assumptions on the joint distribution of the data, the following “regularity conditions” will ensure the existence of the BLP:\n\n\\(\\E[Y^2] &lt; \\infty\\) (outcome has finite mean/variance)\n\\(\\E\\Vert \\mb{X} \\Vert^2 &lt; \\infty\\) (\\(\\mb{X}\\) has finite means/variances/covariances)\n\\(\\mb{Q}_{\\mb{XX}} = \\E[\\mb{XX}']\\) is positive definite (columns of \\(\\X\\) are linearly independent)\n\n\n\n\nUnder these assumptions, it is possible to derive a closed-form expression for the population coefficients \\(\\bfbeta\\) using matrix calculus. To set up the optimization problem, we will find the first-order condition by taking the derivative of the expectation of the squared errors. First, let’s take the derivative of the squared prediction errors using the chain rule: \\[\n\\begin{aligned}\n  \\frac{\\partial}{\\partial \\mb{b}}\\left(Y_{i} - \\X_{i}'\\mb{b}\\right)^{2}\n  &= 2\\left(Y_{i} - \\X_{i}'\\mb{b}\\right)\\frac{\\partial}{\\partial \\mb{b}}(Y_{i} - \\X_{i}'\\mb{b}) \\\\\n  &= -2\\left(Y_{i} - \\X_{i}'\\mb{b}\\right)\\X_{i} \\\\\n  &= -2\\X_{i}\\left(Y_{i} - \\X_{i}'\\mb{b}\\right) \\\\\n  &= -2\\left(\\X_{i}Y_{i} - \\X_{i}\\X_{i}'\\mb{b}\\right),\n\\end{aligned}\n\\] where the third equality comes from the fact that \\((Y_{i} - \\X_{i}'\\bfbeta)\\) is a scalar. We can now plug this into the expectation to get the first-order condition and solve for \\(\\bfbeta\\), \\[\n\\begin{aligned}\n  0 &= -2\\E[\\X_{i}Y_{i} - \\X_{i}\\X_{i}'\\bfbeta ] \\\\\n  \\E[\\X_{i}\\X_{i}'] \\bfbeta &= \\E[\\X_{i}Y_{i}],\n\\end{aligned}\n\\] which implies the population coefficients are \\[\n\\bfbeta = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}] = \\mb{Q}_{\\mb{XX}}^{-1}\\mb{Q}_{\\mb{X}Y}\n\\] We now have an expression for the coefficients for the population best linear predictor in terms of the joint distribution \\((Y_{i}, \\X_{i})\\). A couple of facts might be useful for reasoning this expression. Recall that \\(\\mb{Q}_{\\mb{XX}} = \\E[\\X_{i}\\X_{i}']\\) is a \\(k\\times k\\) matrix and \\(\\mb{Q}_{\\X Y} = \\E[\\X_{i}Y_{i}]\\) is a \\(k\\times 1\\) column vector, which implies that \\(\\bfbeta\\) is also a \\(k \\times 1\\) column vector.\n\n\n\n\n\n\nNote\n\n\n\nIntuitively, what is happening in the expression for the population regression coefficients? It is helpful to separate the intercept or constant term so that we have \\[\nY_{i} = \\beta_{0} + \\X'\\bfbeta + e_{i},\n\\] so \\(\\bfbeta\\) refers to just the vector of coefficients for the covariates. In this case, we can write the coefficients in a more interpretable way: \\[\n\\bfbeta = \\V[\\X]^{-1}\\text{Cov}(\\X, Y), \\qquad \\beta_0 = \\mu_Y - \\mb{\\mu}'_{\\mb{X}}\\bfbeta\n\\]\nThus, the population coefficients take the covariance between the outcome and the covariates and “divide” it by information about variances and covariances of the covariates. The intercept recenters the regression so that projection errors are mean zero. Thus, we can see that these coefficients generalize the bivariate formula to this multiple covariate context.\n\n\nWith an expression for the population linear regression coefficients, we can write the linear projection as \\[\nm(\\X_{i}) = \\X_{i}'\\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}] = \\X_{i}'\\mb{Q}_{\\mb{XX}}^{-1}\\mb{Q}_{\\mb{X}Y}\n\\]\n\n\n5.2.4 Projection error\nThe projection error is the difference between the actual value of \\(Y_i\\) and the projection, \\[\ne_{i} = Y_{i} - m(\\X_{i}) = Y_i - \\X_{i}'\\bfbeta,\n\\] where we have made no assumptions about this error yet. The projection error is simply the prediction error of the best linear prediction. Rewriting this definition, we can see that we can always write the outcome as the linear projection plus the projection error, \\[\nY_{i} = \\X_{i}'\\bfbeta + e_{i}.\n\\] Notice that this looks suspiciously similar to a linearity assumption on the CEF, but we haven’t made any assumptions here. Instead, we have just used the definition of the projection error to write a tautological statement: \\[\nY_{i} = \\X_{i}'\\bfbeta + e_{i} = \\X_{i}'\\bfbeta + Y_{i} - \\X_{i}'\\bfbeta = Y_{i}.\n\\] The critical difference between this representation and the usual linear model assumption is what properties \\(e_{i}\\) possesses.\nOne key property of the projection errors is that when the covariate vector includes an “intercept” or constant term, the projection errors are uncorrelated with the covariates. To see this, we first note that \\(\\E[\\X_{i}e_{i}] = 0\\) since \\[\n\\begin{aligned}\n  \\E[\\X_{i}e_{i}] &= \\E[\\X_{{i}}(Y_{i} - \\X_{i}'\\bfbeta)] \\\\\n                  &= \\E[\\X_{i}Y_{i}] - \\E[\\X_{i}\\X_{i}']\\bfbeta \\\\\n                  &= \\E[\\X_{i}Y_{i}] - \\E[\\X_{i}\\X_{i}']\\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}] \\\\\n  &= \\E[\\X_{i}Y_{i}] - \\E[\\X_{i}Y_{i}] = 0\n\\end{aligned}\n\\] Thus, for every \\(X_{ij}\\) in \\(\\X_{i}\\), we have \\(\\E[X_{ij}e_{i}] = 0\\). If one of the entries in \\(\\X_i\\) is a constant 1, then this also implies that \\(\\E[e_{i}] = 0\\). Together, these facts imply that the projection error is uncorrelated with each \\(X_{ij}\\), since \\[\n\\cov(X_{ij}, e_{i}) = \\E[X_{ij}e_{i}] - \\E[X_{ij}]\\E[e_{i}] = 0 - 0 = 0\n\\] Notice that we still have made no assumptions about these projection errors except for some mild regularity conditions on the joint distribution of the outcome and covariates. Thus, in very general settings, we can write the linear projection model \\(Y_i = \\X_i'\\bfbeta + e_i\\) where \\(\\bfbeta = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}]\\) and conclude that \\(\\E[\\X_{i}e_{i}] = 0\\) by definition, not by assumption.\nThe projection error is uncorrelated with the covariates, so does this mean that the CEF is linear? Unfortunately, no. Recall that while independence implies uncorrelated, the reverse does not hold. So when we look at the CEF, we have \\[\n\\E[Y_{i} \\mid \\X_{i}] = \\X_{i}'\\bfbeta + \\E[e_{i} \\mid \\X_{i}],\n\\] and the last term \\(\\E[e_{i} \\mid \\X_{i}]\\) would only be 0 if the errors were independent of the covariates, so \\(\\E[e_{i} \\mid \\X_{i}] = \\E[e_{i}] = 0\\). But nowhere in the linear projection model did we assume this. So while we can (almost) always write the outcome as \\(Y_i = \\X_i'\\bfbeta + e_i\\) and have those projection errors be uncorrelated with the covariates, it will require additional assumptions to ensure that the true CEF is, in fact, linear \\(\\E[Y_{i} \\mid \\X_{i}] = \\X_{i}'\\bfbeta\\).\nLet’s take a step back. What have we shown here? In a nutshell, we have shown that a population linear regression exists under very general conditions, and we can write the coefficients of that population linear regression as a function of expectations of the joint distribution of the data. We did not assume that the CEF was linear nor that the projection errors were normal.\nWhy do we care about this? The ordinary least squares estimator, the workhorse regression estimator, targets this quantity of interest in large samples, regardless of whether the true CEF is linear or not. Thus, even when a linear CEF assumption is incorrect, OLS still targets a perfectly valid quantity of interest: the coefficients from this population linear projection."
  },
  {
    "objectID": "06_linear_model.html#linear-cefs-without-assumptions",
    "href": "06_linear_model.html#linear-cefs-without-assumptions",
    "title": "5  Linear regression",
    "section": "5.3 Linear CEFs without assumptions",
    "text": "5.3 Linear CEFs without assumptions\nWhat is the relationship between the best linear predictor (which we just saw generally exists) and the CEF? To draw the connection, remember a vital property of the conditional expectation: it is the function of \\(\\X_i\\) that best predicts \\(Y_{i}\\). The population regression was the best linear predictor, but the CEF is the best predictor among all nicely behaved functions of \\(\\X_{i}\\), linear or nonlinear. In particular, if we label \\(L_2\\) to be the set of all functions of the covariates \\(g()\\) that have finite squared expectation, \\(\\E[g(\\X_{i})^{2}] &lt; \\infty\\), then we can show that the CEF has the lowest squared prediction error in this class of functions: \\[\n\\mu(\\X) = \\E[Y_{i} \\mid \\X_{i}] = \\argmin_{g(\\X_i) \\in L_2}\\; \\E\\left[(Y_{i} - g(\\X_{i}))^{2}\\right],\n\\]\nSo we have established that the CEF is the best predictor and the population linear regression \\(m(\\X_{i})\\) is the best linear predictor. These two facts allow us to connect the CEF and the population regression.\n\nTheorem 5.1 If \\(\\mu(\\X_{i})\\) is a linear function of \\(\\X_i\\), then \\(\\mu(\\X_{i}) = m(\\X_{i}) = \\X_i'\\bfbeta\\).\n\nThis theorem says that if the true CEF is linear, it equals the population linear regression. The proof of this is straightforward: the CEF is the best predictor, so if it is linear, it must also be the best linear predictor.\nIn general, we are in the business of learning about the CEF, so we are unlikely to know if it genuinely is linear or not. In some situations, however, we can show that the CEF is linear without any additional assumptions. These will be situations when the covariates take on a finite number of possible values. Suppose we are interested in the CEF of poll wait times for Black (\\(X_i = 1\\)) vs. non-Black (\\(X_i = 0\\)) voters. In this case, there are two possible values of the CEF, \\(\\mu(1) = \\E[Y_{i}\\mid X_{i}= 1]\\), the average wait time for Black voters, and \\(\\mu(0) = \\E[Y_{i}\\mid X_{i} = 0]\\), the average wait time for non-Black voters. Notice that we can write the CEF as \\[\n\\mu(x) = x \\mu(1) + (1 - x) \\mu(0) = \\mu(0) + x\\left(\\mu(1) - \\mu(0)\\right)= \\beta_0 + x\\beta_1,\n\\] which is clearly a linear function of \\(x\\). Based on this derivation, we can see that the coefficients of this linear CEF have a clear interpretation:\n\n\\(\\beta_0 = \\mu(0)\\): the expected wait time for a Black voter.\n\\(\\beta_1 = \\mu(1) - \\mu(0)\\): the difference in average wait times between Black and non-Black voters. Notice that it matters how \\(X_{i}\\) is defined here since the intercept will always be the average outcome when \\(X_i = 0\\), and the slope will always be the difference in means between the \\(X_i = 1\\) group and the \\(X_i = 0\\) group.\n\nWhat about a categorical covariate with more than two levels? For instance, we might be interested in wait times by party identification, where \\(X_i = 1\\) indicates Democratic voters, \\(X_i = 2\\) indicates Republican voters, and \\(X_i = 3\\) indicates independent voters. How can we write the CEF of wait times as a linear function of this variable? That would assume that the difference between Democrats and Republicans is the same as for Independents and Republicans. With more than two levels, we can represent a categorical variable as a vector of binary variables, \\(\\X_i = (X_{i1}, X_{i2})\\), where \\[\n\\begin{aligned}\n  X_{{i1}} &= \\begin{cases}\n                1&\\text{if Republican} \\\\\n                   0 & \\text{if not Republican}\n              \\end{cases} \\\\\nX_{{i2}} &= \\begin{cases}\n                1&\\text{if independent} \\\\\n                   0 & \\text{if not independent}\n              \\end{cases} \\\\\n\\end{aligned}\n\\] These two indicator variables encode the same information as the original three-level variable, \\(X_{i}\\). If I know the values of \\(X_{i1}\\) and \\(X_{i2}\\), I know exactly what party to which \\(i\\) belongs. Thus, the CEFs for \\(X_i\\) and the pair of indicator variables, \\(\\X_i\\), are precisely the same, but the latter admits a lovely linear representation, \\[\n\\E[Y_i \\mid X_{i1}, X_{i2}] = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2},\n\\] where\n\n\\(\\beta_0 = \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 0]\\) is the average wait time for the group who does not get an indicator variable (Democrats in this case).\n\\(\\beta_1 = \\E[Y_{i} \\mid X_{i1} = 1, X_{i2} = 0] - \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 0]\\) is the difference in means between Republican voters and Democratic voters, or the difference between the first indicator group and the baseline group.\n\\(\\beta_2 = \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 1] - \\E[Y_{i} \\mid X_{i1} = 0, X_{i2} = 0]\\) is the difference in means between independent voters and Democratic voters, or the difference between the second indicator group and the baseline group.\n\nThis approach easily generalizes to categorical variables with an arbitrary number of levels.\nWhat have we shown? The CEF will be linear without additional assumptions when there is a categorical covariate. We can show that this continues to hold even when we have multiple categorical variables. We now have two binary covariates: \\(X_{i1}=1\\) indicating a Black voter, and \\(X_{i2} = 1\\) indicating an urban voter. With these two binary variables, there are four possible values of the CEF: \\[\n\\mu(x_1, x_2) = \\begin{cases}\n\\mu_{00} & \\text{if } x_1 = 0 \\text{ and } x_2 = 0 \\text{ (non-Black, rural)} \\\\\n\\mu_{10} & \\text{if } x_1 = 1 \\text{ and } x_2 = 0 \\text{ (Black, rural)} \\\\\n\\mu_{01} & \\text{if } x_1 = 0 \\text{ and } x_2 = 1 \\text{ (non-Black, urban)} \\\\\n\\mu_{11} & \\text{if } x_1 = 1 \\text{ and } x_2 = 1 \\text{ (Black, urban)}\n\\end{cases}\n\\] We can write this as \\[\n\\mu(x_{1}, x_{2}) = (1 - x_{1})(1 - x_{2})\\mu_{00} + x_{1}(1 -x_{2})\\mu_{10} + (1-x_{1})x_{2}\\mu_{01} + x_{1}x_{2}\\mu_{11},\n\\] which we can rewrite as \\[\n\\mu(x_1, x_2) = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3,\n\\] where\n\n\\(\\beta_0 = \\mu_{00}\\): average wait times for rural non-Black voters.\n\\(\\beta_1 = \\mu_{10} - \\mu_{00}\\): difference in means for rural Black vs. rural non-Black voters.\n\\(\\beta_2 = \\mu_{01} - \\mu_{00}\\): difference in means for urban non-Black vs. rural non-Black voters.\n\\(\\beta_3 = (\\mu_{11} - \\mu_{01}) - (\\mu_{10} - \\mu_{00})\\): difference in urban racial difference vs rural racial difference.\n\nThus, we can write the CEF with two binary covariates as linear when the linear specification includes a multiplicative interaction between them (\\(x_1x_2\\)). This result holds for all pairs of binary covariates, and we can generalize the interpretation of the coefficients in the CEF as\n\n\\(\\beta_0 = \\mu_{00}\\): average outcome when both variables are 0.\n\\(\\beta_1 = \\mu_{10} - \\mu_{00}\\): difference in average outcomes for the first covariate when the second covariate is 0.\n\\(\\beta_2 = \\mu_{01} - \\mu_{00}\\): difference in average outcomes for the second covariate when the first covariate is 0.\n\\(\\beta_3 = (\\mu_{11} - \\mu_{01}) - (\\mu_{10} - \\mu_{00})\\): change in the “effect” of the first (second) covariate when the second (first) covariate goes from 0 to 1.\n\nThis result also generalizes to an arbitrary number of binary covariates. If we have \\(p\\) binary covariates, then the CEF will be linear with all two-way interactions, \\(x_1x_2\\), all three-way interactions, \\(x_1x_2x_3\\), up to the \\(p\\)-way interaction \\(x_1\\times\\cdots\\times x_p\\). Furthermore, we can generalize to arbitrary numbers of categorical variables by expanding each into a series of binary variables and then including all interactions between the resulting binary variables.\nWe have established that when we have a set of categorical covariates, the true CEF will be linear, and we have seen the various ways to represent that CEF. Notice that when we use, for example, ordinary least squares, we are free to choose how to include our variables. That means that we could run a regression of \\(Y_i\\) on \\(X_{i1}\\) and \\(X_{i2}\\) without an interaction term. This model will only be correct if \\(\\beta_3\\) is equal to 0, and so the interaction term is irrelevant. Because of this ability to choose our models, it’s helpful to have a language for models that capture the linear CEF appropriately. We call a model saturated if there are as many coefficients as the CEF’s unique values. A saturated model, by its nature, can always be written as a linear function without assumptions. The above examples show how to construct saturated models in various situations."
  },
  {
    "objectID": "06_linear_model.html#interpretation-of-the-regression-coefficients",
    "href": "06_linear_model.html#interpretation-of-the-regression-coefficients",
    "title": "5  Linear regression",
    "section": "5.4 Interpretation of the regression coefficients",
    "text": "5.4 Interpretation of the regression coefficients\nWe have seen how to interpret population regression coefficients when the CEF is linear without assumptions. How do we interpret the population coefficients \\(\\bfbeta\\) in other settings?\nLet’s start with the simplest case, where every entry in \\(\\X_{i}\\) represents a different covariate and no covariate is any function of another (we’ll see why this caveat is necessary below). In this simple case, the \\(k\\)th coefficient, \\(\\beta_{k}\\), will represent the change in the predicted outcome for a one-unit change in the \\(k\\)th covariate \\(X_{ik}\\), holding all other covariates fixed. We can see this from \\[\n\\begin{aligned}\n  m(x_{1} + 1, x_{2}) & = \\beta_{0} + \\beta_{1}(x_{1} + 1) + \\beta_{2}x_{2} \\\\\n  m(x_{1}, x_{2}) &= \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2},\n\\end{aligned}\n\\] so that the change in the predicted outcome for increasing \\(X_{i1}\\) by one unit is \\[\nm(x_{1} + 1, x_{2}) - m(x_{1}, x_{2}) = \\beta_1\n\\] Notice that nothing changes in this interpretation if we add more covariates to the vector, \\[\nm(x_{1} + 1, \\bfx_{2}) - m(x_{1}, \\bfx_{2}) = \\beta_1,\n\\] the coefficient on a particular variable is the change in the predicted outcome for a one-unit change in the covariate holding all other covariates constant. Each coefficient summarizes the “all else equal” difference in the predicted outcome for each covariate.\n\n5.4.1 Polynomial functions of the covariates\nThe interpretation of the population regression coefficients becomes more complicated when we include nonlinear functions of the covariates. In that case, multiple coefficients control how a change in a covariate will change the predicted value of \\(Y_i\\). Suppose that we have a quadratic function of \\(X_{i1}\\), \\[\nm(x_1, x_1^2, x_{2}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{1}^{2} + \\beta_{3}x_{2},\n\\] and try to look at a one-unit change in \\(x_1\\), \\[\n\\begin{aligned}\n  m(x_{1} + 1, (x_{1} + 1)^{2}, x_{2}) & = \\beta_{0} + \\beta_{1}(x_{1} + 1) + \\beta_{2}(x_{1} + 1)^{2}+ \\beta_{3}x_{2} \\\\\n  m(x_{1}, x_{1}^{2}, x_{2}) &= \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{1}^{2} + \\beta_{3}x_{2},\n\\end{aligned}\n\\] resulting in \\(\\beta_1 + \\beta_2(2x_{1} + 1)\\). This formula might be an interesting quantity, but we will more commonly use the derivative of \\(m(\\bfx)\\) with respect to \\(x_1\\) as a measure of the marginal effect of \\(X_{i1}\\) on the predicted value of \\(Y_i\\) (holding all other variables constant), where “marginal” here means the change in prediction for a very small change in \\(X_{i1}\\).1 In the case of the quadratic covariate, we have \\[\n\\frac{\\partial m(x_{1}, x_{1}^{2}, x_{2})}{\\partial x_{1}} = \\beta_{1} + 2\\beta_{2}x_{1},\n\\] so the marginal effect on prediction varies as a function of \\(x_1\\). From this, we can see that the individual interpretations of the coefficients are less interesting: \\(\\beta_1\\) is the marginal effect when \\(X_{i1} = 0\\) and \\(\\beta_2 / 2\\) describes how a one-unit change in \\(X_{i1}\\) changes the marginal effect. As is hopefully clear, it will often be more straightforward to visualize the nonlinear predictor function (perhaps using the orthogonalization techniques in Section 5.5).\n\n\n5.4.2 Interactions\nAnother common nonlinear function of the covariates is when we include interaction terms or covariates that are products of two other covariates, \\[\nm(x_{1}, x_{2}, x_{1}x_{2}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}x_{2}.\n\\] In these situations, we can also use the derivative of the BLP to measure the marginal effect of one variable or the other on the predicted value of \\(Y_i\\). In particular, we have \\[\n\\begin{aligned}\n  \\frac{\\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\\partial x_1} &= \\beta_1 + \\beta_3x_2, \\\\\n  \\frac{\\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\\partial x_2} &= \\beta_2 + \\beta_3x_1.\n\\end{aligned}\n\\] Here, the coefficients are slightly more interpretable:\n\n\\(\\beta_1\\): the marginal effect of \\(X_{i1}\\) on predicted \\(Y_i\\) when \\(X_{i2} = 0\\).\n\\(\\beta_2\\): the marginal effect of \\(X_{i2}\\) on predicted \\(Y_i\\) when \\(X_{i1} = 0\\).\n\\(\\beta_3\\): the change in the marginal effect of \\(X_{i1}\\) due to a one-unit change in \\(X_{i2}\\) OR the change in the marginal effect of \\(X_{i2}\\) due to a one-unit change in \\(X_{i1}\\).\n\nIf we add more covariates to this BLP, these interpretations change to “holding all other covariates constant.”\nInteractions are a routine part of social science research because they allow us to assess how the relationship between the outcome and an independent variable varies by the values of another variable. In the context of our study of voter wait times, if \\(X_{i1}\\) is income and \\(X_{i2}\\) is the Black/non-Black voter indicator, then \\(\\beta_3\\) represents the change in the slope of the wait time-income relationship between Black and non-Black voters."
  },
  {
    "objectID": "06_linear_model.html#sec-fwl",
    "href": "06_linear_model.html#sec-fwl",
    "title": "5  Linear regression",
    "section": "5.5 Multiple regression from bivariate regression",
    "text": "5.5 Multiple regression from bivariate regression\nWhen we have a regression of an outcome on two covariates, it is helpful to understand how the coefficients of one variable relate to the other. For example, if we have the following best linear projection: \\[\n(\\alpha, \\beta, \\gamma) = \\argmin_{(a,b,c) \\in \\mathbb{R}^{3}} \\; \\E[(Y_{i} - (a + bX_{i} + cZ_{i}))^{2}]\n\\tag{5.1}\\] Is there some way to understand the \\(\\beta\\) coefficient here regarding simple linear regression? As it turns out, yes. From the above results, we know that the intercept has a simple form: \\[\n\\alpha = \\E[Y_i] - \\beta\\E[X_i] - \\gamma\\E[Z_i].\n\\] Let’s investigate the first order condition for \\(\\beta\\): \\[\n\\begin{aligned}\n  0 &= \\E[Y_{i}X_{i}] - \\alpha\\E[X_{i}] - \\beta\\E[X_{i}^{2}] - \\gamma\\E[X_{i}Z_{i}] \\\\\n    &= \\E[Y_{i}X_{i}] - \\E[Y_{i}]\\E[X_{i}] + \\beta\\E[X_{i}]^{2} + \\gamma\\E[X_{i}]\\E[Z_{i}] - \\beta\\E[X_{i}^{2}] - \\gamma\\E[X_{i}Z_{i}] \\\\\n  &= \\cov(Y, X) - \\beta\\V[X_{i}] - \\gamma \\cov(X_{i}, Z_{i})\n\\end{aligned}\n\\] We can see from this that if \\(\\cov(X_{i}, Z_{i}) = 0\\), then the coefficient on \\(X_i\\) will be the same as in the simple regression case, \\(\\cov(Y_{i}, X_{i})/\\V[X_{i}]\\). When \\(X_i\\) and \\(Z_i\\) are uncorrelated, we sometimes call them orthogonal.\nTo write a simple formula for \\(\\beta\\) when the covariates are not orthogonal, we will orthogonalize \\(X_i\\) by obtaining the prediction errors from a population linear regression of \\(X_i\\) on \\(Z_i\\): \\[\n\\widetilde{X}_{i} = X_{i} - (\\delta_{0} + \\delta_{1}Z_{i}) \\quad\\text{where}\\quad (\\delta_{0}, \\delta_{1}) = \\argmin_{(d_{0},d_{1}) \\in \\mathbb{R}^{2}} \\; \\E[(X_{i} - (d_{0} + d_{1}Z_{i}))^{2}]\n\\] Given the properties of projection errors, we know that this orthogonalized version of \\(X_{i}\\) will be uncorrelated with \\(Z_{i}\\) since \\(\\E[\\widetilde{X}_{i}Z_{i}] = 0\\). Remarkably, the coefficient on \\(X_i\\) from the “long” BLP in Equation 5.1 is the same as the regression of \\(Y_i\\) on this orthogonalized \\(\\widetilde{X}_i\\), \\[\n\\beta = \\frac{\\text{cov}(Y_{i}, \\widetilde{X}_{i})}{\\V[\\widetilde{X}_{i}]}\n\\]\nWe can expand this idea to when there are several other covariates. Suppose now that we are interested in a regression of \\(Y_i\\) on \\(\\X_i\\) and we are interested in the coefficient on the \\(k\\)th covariate. Let \\(\\X_{i,-k}\\) be the vector of covariates omitting the \\(k\\)th entry and let \\(m_k(\\X_{i,-k})\\) represent the BLP of \\(X_{ik}\\) on these other covariates. We can define \\(\\widetilde{X}_{ik} = X_{ik} - m_{k}(\\X_{i,-k})\\) as the \\(k\\)th variable orthogonalized with respect to the rest of the variables and we can write the coefficient on \\(X_{ik}\\) as \\[\n\\beta_k = \\frac{\\cov(Y_i, \\widetilde{X}_{ik})}{\\V[\\widetilde{X}_{ik}]}.\n\\] Thus, the population regression coefficient in the BLP is the same as from a bivariate regression of the outcome on the projection error for \\(X_{ik}\\) projected on all other covariates. One interpretation of coefficients in a population multiple regression is they represent the relationship between the outcome and the covariate after removing the linear relationships of all other variables."
  },
  {
    "objectID": "06_linear_model.html#omitted-variable-bias",
    "href": "06_linear_model.html#omitted-variable-bias",
    "title": "5  Linear regression",
    "section": "5.6 Omitted variable bias",
    "text": "5.6 Omitted variable bias\nIn many situations, we might need to choose whether to include a variable in a regression or not, so it can be helpful to understand how this choice might affect the population coefficients on the other variables in the regression. Suppose we have a variable \\(Z_i\\) that we may add to our regression which currently has \\(\\X_i\\) as the covariates. We can write this new projection as \\[\nm(\\X_i, Z_i) = \\X_i'\\bfbeta + Z_i\\gamma, \\qquad m(\\X_{i}) = \\X_i'\\bs{\\delta},\n\\] where we often refer to \\(m(\\X_i, Z_i)\\) as the long regression and \\(m(\\X_i)\\) as the short regression.\nWe know from the definition of the BLP that we can write the short coefficients as \\[\n\\bs{\\delta} = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1} \\E[\\X_{i}Y_{i}].\n\\] Letting \\(e_i = Y_i - m(\\X_{i}, Z_{i})\\) be the projection errors from the long regression, we can write this as \\[\n\\begin{aligned}\n  \\bs{\\delta} &= \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1} \\E[\\X_{i}(\\X_{i}'\\bfbeta + Z_{i}\\gamma + e_{i})] \\\\\n              &= \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}(\\E[\\X_{i}\\X_{i}']\\bfbeta + \\E[\\X_{i}Z_{i}]\\gamma + \\E[\\X_{i}e_{i}]) \\\\\n              &= \\bfbeta + \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Z_{i}]\\gamma\n\\end{aligned}\n\\] Notice that the vector in the second term is the linear projection coefficients of a population linear regression of \\(Z_i\\) on the \\(\\X_i\\). If we call these coefficients \\(\\bs{\\pi}\\), then the short coefficients are \\[\n\\bs{\\delta} = \\bfbeta + \\bs{\\pi}\\gamma.\n\\]\nWe can rewrite this to show that the difference between the coefficients in these two projections is \\(\\bs{\\delta} - \\bfbeta= \\bs{\\pi}\\gamma\\) or the product of the coefficient on the “excluded” \\(Z_i\\) and the coefficient of the included \\(\\X_i\\) on the excluded. Most textbooks refer to this difference as the omitted variable bias of omitting \\(Z_i\\) under the idea that \\(\\bfbeta\\) is the true target of inference. But the result is much broader than this since it just tells us how to relate the coefficients of two nested projections.\nThe last two results (multiple regressions from bivariate and omitted variable bias) are sometimes presented as results for the ordinary least squares estimator that we will show in the next chapter. We introduce them here as features of a particular population quantity, the linear projection or population linear regression."
  },
  {
    "objectID": "06_linear_model.html#drawbacks-of-the-blp",
    "href": "06_linear_model.html#drawbacks-of-the-blp",
    "title": "5  Linear regression",
    "section": "5.7 Drawbacks of the BLP",
    "text": "5.7 Drawbacks of the BLP\nThe best linear predictor is, of course, a linear approximation to the CEF, and this approximation could be quite poor if the true CEF is highly nonlinear. A more subtle issue with the BLP is that it is sensitive to the marginal distribution of the covariates when the CEF is nonlinear. Let’s return to our example of voter wait times and income. In Figure 5.3, we show the true CEF and the BLP when we restrict income below $50,000 or above $100,000. The BLP can vary quite dramatically here. This figure is an extreme example, but the essential point will still hold as the marginal distribution of \\(X_i\\) changes.\n\n\n\n\n\nFigure 5.3: Linear projections for when truncating income distribution below $50k and above $100k."
  },
  {
    "objectID": "06_linear_model.html#footnotes",
    "href": "06_linear_model.html#footnotes",
    "title": "5  Linear regression",
    "section": "",
    "text": "Notice the choice of language here. The marginal effect is on the predicted value of \\(Y_i\\), not on \\(Y_i\\) itself. So these marginal effects are associational, not necessarily causal quantities.↩︎"
  },
  {
    "objectID": "07_least_squares.html#deriving-the-ols-estimator",
    "href": "07_least_squares.html#deriving-the-ols-estimator",
    "title": "6  The mechanics of least squares",
    "section": "6.1 Deriving the OLS estimator",
    "text": "6.1 Deriving the OLS estimator\nIn the last chapter on the linear model and the best linear projection, we operated purely in the population, not samples. We derived the population regression coefficients \\(\\bfbeta\\), representing the coefficients on the line of best fit in the population. We now take these as our quantity of interest.\n\n\n\n\n\n\nAssumption\n\n\n\nThe variables \\(\\{(Y_1, \\X_1), \\ldots, (Y_i,\\X_i), \\ldots, (Y_n, \\X_n)\\}\\) are i.i.d. draws from a common distribution \\(F\\).\n\n\nRecall the population linear coefficients (or best linear predictor coefficients) that we derived in the last chapter, \\[\n\\bfbeta = \\argmin_{\\mb{b} \\in \\real^k}\\; \\E\\bigl[ \\bigl(Y_{i} - \\mb{X}_{i}'\\mb{b} \\bigr)^2\\bigr] = \\left(\\E[\\X_{i}\\X_{i}']\\right)^{-1}\\E[\\X_{i}Y_{i}]\n\\]\nWe will consider two different ways to derive the OLS estimator for these coefficients, both of which are versions of the plug-in principle. The first approach is to use the closed-form representation of the coefficients and replace any expectations with sample means, \\[\n\\bhat = \\left(\\frac{1}{n} \\sum_{i=1}^n \\X_i\\X_i' \\right)^{-1} \\left(\\frac{1}{n} \\sum_{i=1}^n \\X_{i}Y_{i} \\right),\n\\] which exists if \\(\\sum_{i=1}^n \\X_i\\X_i'\\) is positive definite and thus invertible. We will return to this assumption below.\nIn a simple bivariate linear projection model \\(m(X_{i}) = \\beta_0 + \\beta_1X_{i}\\), we saw that the population slope was \\(\\beta_1= \\text{cov}(Y_{i},X_{i})/ \\V[X_{i}]\\) and this approach would have our estimator for the slope be the ratio of the sample covariance of \\(Y_i\\) and \\(X_i\\) to the sample variance of \\(X_i\\), or \\[\n\\widehat{\\beta}_{1} = \\frac{\\widehat{\\sigma}_{Y,X}}{\\widehat{\\sigma}^{2}_{X}} = \\frac{ \\frac{1}{n-1}\\sum_{i=1}^{n} (Y_{i} - \\overline{Y})(X_{i} - \\overline{X})}{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\Xbar)^{2}}.\n\\]\nThis plug-in approach is widely applicable and tends to have excellent properties in large samples under iid data. But this approach also hides some of the geometry of the setting.\nThe second approach applies the plug-in principle not to the closed-form expression for the coefficients but to the optimization problem itself. We call this the least squares estimator because it minimizes the empirical (or sample) squared prediction error, \\[\n\\bhat = \\argmin_{\\mb{b} \\in \\real^k}\\; \\frac{1}{n} \\sum_{i=1}^{n}\\bigl(Y_{i} - \\mb{X}_{i}'\\mb{b} \\bigr)^2 = \\argmin_{\\mb{b} \\in \\real^k}\\; SSR(\\mb{b}),\n\\] where, \\[\nSSR(\\mb{b}) = \\sum_{i=1}^{n}\\bigl(Y_{i} - \\mb{X}_{i}'\\mb{b} \\bigr)^2\n\\] is the sum of the squared residuals. To distinguish it from other, more complicated least squares estimators, we call this the ordinary least squares estimator or OLS.\nLet’s solve this minimization problem! We can write down the first-order conditions as \\[\n0=\\frac{\\partial SSR(\\bhat)}{\\partial \\bfbeta} = 2 \\left(\\sum_{i=1}^{n} \\X_{i}Y_{i}\\right) - 2\\left(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\right)\\bhat.\n\\] We can rearrange this system of equations to \\[\n\\left(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\right)\\bhat = \\left(\\sum_{i=1}^{n} \\X_{i}Y_{i}\\right).\n\\] To obtain the solution for \\(\\bhat\\), notice that \\(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\) is a \\((k+1) \\times (k+1)\\) matrix and \\(\\bhat\\) and \\(\\sum_{i=1}^{n} \\X_{i}Y_{i}\\) are both \\(k+1\\) length column vectors. If \\(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\) is invertible, then we can multiply both sides of this equation by that inverse to arrive at \\[\n\\bhat = \\left(\\sum_{i=1}^n \\X_i\\X_i' \\right)^{-1} \\left(\\sum_{i=1}^n \\X_{i}Y_{i} \\right),\n\\] which is the same expression as the plug-in estimator (after canceling the \\(1/n\\) terms). To confirm that we have found a minimum, we also need to check the second-order condition, \\[\n\\frac{\\partial^{2} SSR(\\bhat)}{\\partial \\bfbeta\\bfbeta'} = 2\\left(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\right) &gt; 0.\n\\] What does it mean for a matrix to be “positive”? In matrix algebra, this condition means that the matrix \\(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\) is positive definite, a condition that we discuss in Section 6.4.\nUsing the plug-in or least squares approaches, we arrive at the same estimator for the best linear predictor/population linear regression coefficients.\n\nTheorem 6.1 If the \\(\\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\) is positive definite, then the ordinary least squares estimator is \\[\n\\bhat = \\left(\\sum_{i=1}^n \\X_i\\X_i' \\right)^{-1} \\left(\\sum_{i=1}^n \\X_{i}Y_{i} \\right).\n\\]\n\n\n\n\n\n\n\nFormula for the OLS slopes\n\n\n\nAlmost all regression will contain an intercept term usually represented as a constant 1 in the covariate vector. It is also possible to separate the intercept to arrive at the set of coefficients on the “real” covariates: \\[\nY_{i} = \\alpha + \\X_{i}'\\bfbeta + \\e_{i}.\n\\] Defined this way, we can write the OLS estimator for the “slopes” on \\(\\X_i\\) as the OLS estimator with all variables demeaned \\[\n\\bhat = \\left(\\frac{1}{n} \\sum_{i=1}^{n} (\\X_{i} - \\overline{\\X})(\\X_{i} - \\overline{\\X})'\\right) \\left(\\frac{1}{n} \\sum_{i=1}^{n}(\\X_{i} - \\overline{\\X})(Y_{i} - \\overline{Y})\\right)\n\\] which is the inverse of the sample covariance matrix of \\(\\X_i\\) times the sample covariance of \\(\\X_i\\) and \\(Y_i\\). The intercept is \\[\n\\widehat{\\alpha} = \\overline{Y} - \\overline{\\X}'\\bhat.\n\\]\n\n\nWhen dealing with actual data, we refer to the prediction errors \\(\\widehat{e}_{i} = Y_i - \\X_i'\\bhat\\) as the residuals and the predicted value itself, \\(\\widehat{Y}_i = \\X_{i}'\\bhat\\), is also called the fitted value. With the population linear regression, we saw that the projection errors \\(e_i = Y_i - \\X_i'\\bfbeta\\) were mean zero and uncorrelated with the covariates \\(\\E[\\X_{i}e_{i}] = 0\\). The residuals have a similar property with respect to the covariates in the sample: \\[\n\\sum_{i=1}^n \\X_i\\widehat{e}_i = 0.\n\\] The residuals are exactly uncorrelated with the covariates (when the covariates include a constant/intercept term), which is mechanically true of the OLS estimator.\nFigure 6.2 shows how OLS works in the bivariate case. Here we see three possible regression lines and the sum of the squared residuals for each line. OLS aims to find the line that minimizes the function on the right.\n\n\n\n\n\nFigure 6.2: Different possible lines and their corresponding sum of squared residuals."
  },
  {
    "objectID": "07_least_squares.html#model-fit",
    "href": "07_least_squares.html#model-fit",
    "title": "6  The mechanics of least squares",
    "section": "6.2 Model fit",
    "text": "6.2 Model fit\nWe have learned how to use OLS to obtain an estimate of the best linear predictor, but we may ask how good that prediction is. Does using \\(\\X_i\\) help us predict \\(Y_i\\)? To investigate this, we can consider two different prediction errors: those using covariates and those that do not.\nWe have already seen the prediction error when using the covariates; it is just the sum of the squared residuals, \\[\nSSR = \\sum_{i=1}^n (Y_i - \\X_{i}'\\bhat)^2.\n\\] Recall that the best predictor for \\(Y_i\\) without any covariates is simply its sample mean \\(\\overline{Y}\\), and so the prediction error without covariates is what we call the total sum of squares, \\[\nTSS = \\sum_{i=1}^n (Y_i - \\overline{Y})^2.\n\\] Figure 6.3 shows the difference between these two types of prediction errors.\n\n\n\n\n\nFigure 6.3: Total sum of squares vs. the sum of squared residuals.\n\n\n\n\nWe can use the proportion reduction in prediction error from adding those covariates to measure how much those covariates improve the regression’s predictive ability. This value, called the coefficient of determination or \\(R^2\\), is simply \\[\nR^2 = \\frac{TSS - SSR}{TSS} = 1-\\frac{SSR}{TSS},\n\\] which is the reduction in error moving from \\(\\overline{Y}\\) to \\(\\X_i'\\bhat\\) as the predictor relative to the prediction error using \\(\\overline{Y}\\). We can think of this as the fraction of the total prediction error eliminated by using \\(\\X_i\\) to predict \\(Y_i\\). One thing to note is that OLS will always improve in-sample fit so that \\(TSS \\geq SSR\\) even if \\(\\X_i\\) is unrelated to \\(Y_i\\). This phantom improvement occurs because the whole point of OLS is to minimize the SSR, and it will do that even if it is just chasing noise.\nSince regression always improves in-sample fit, \\(R^2\\) will fall between 0 and 1. A value 0 zero would indicate exactly 0 estimated coefficients on all covariates (except the intercept) so that \\(Y_i\\) and \\(\\X_i\\) are perfectly orthogonal in the data (this is very unlikely to occur because there will likely be some minimal but nonzero relationship by random chance). A value of 1 indicates a perfect linear fit."
  },
  {
    "objectID": "07_least_squares.html#matrix-form-of-ols",
    "href": "07_least_squares.html#matrix-form-of-ols",
    "title": "6  The mechanics of least squares",
    "section": "6.3 Matrix form of OLS",
    "text": "6.3 Matrix form of OLS\nWhile we derived the OLS estimator above, there is a much more common representation of the estimator that relies on vectors and matrices. We usually write the linear model for a generic unit, \\(Y_i = \\X_i'\\bfbeta + e_i\\), but obviously, there are \\(n\\) of these equations, \\[\n\\begin{aligned}\n  Y_1 &= \\X_1'\\bfbeta + e_1 \\\\\n  Y_2 &= \\X_2'\\bfbeta + e_2 \\\\\n  &\\vdots \\\\\n  Y_n &= \\X_n'\\bfbeta + e_n \\\\\n\\end{aligned}\n\\] We can write this system of equations in a more compact form using matrix algebra. In particular, let’s combine the variables here into random vectors/matrices: \\[\n\\mb{Y} = \\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n  \\end{pmatrix}, \\quad\n  \\mathbb{X} = \\begin{pmatrix}\n\\X'_1 \\\\\n\\X'_2 \\\\\n\\vdots \\\\\n\\X'_n\n  \\end{pmatrix} =\n  \\begin{pmatrix}\n    1 & X_{11} & X_{12} & \\cdots & X_{1k} \\\\\n    1 & X_{21} & X_{22} & \\cdots & X_{2k} \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n    1 & X_{n1} & X_{n2} & \\cdots & X_{nk} \\\\\n  \\end{pmatrix},\n  \\quad\n  \\mb{e} = \\begin{pmatrix}\ne_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n\n  \\end{pmatrix}\n\\] Then we can write the above system of equations as \\[\n\\mb{Y} = \\mathbb{X}\\bfbeta + \\mb{e},\n\\] where notice now that \\(\\mathbb{X}\\) is an \\(n \\times (k+1)\\) matrix and \\(\\bfbeta\\) is a \\(k+1\\) length column vector.\nA critical link between the definition of OLS above to the matrix notation comes from representing sums in matrix form. In particular, we have \\[\n\\begin{aligned}\n  \\sum_{i=1}^n \\X_i\\X_i' &= \\Xmat'\\Xmat \\\\\n  \\sum_{i=1}^n \\X_iY_i &= \\Xmat'\\mb{Y},\n\\end{aligned}\n\\] which means we can write the OLS estimator in the more recognizable form as \\[\n\\bhat = \\left( \\mathbb{X}'\\mathbb{X} \\right)^{-1} \\mathbb{X}'\\mb{Y}.\n\\]\nOf course, we can also define the vector of residuals, \\[\n\\widehat{\\mb{e}} = \\mb{Y} - \\mathbb{X}\\bhat = \\left[\n\\begin{array}{c}\n    Y_1 \\\\\n    Y_2 \\\\\n    \\vdots \\\\\n    Y_n\n    \\end{array}\n\\right] -\n\\left[\n\\begin{array}{c}\n   1\\widehat{\\beta}_0 + X_{11}\\widehat{\\beta}_1 + X_{12}\\widehat{\\beta}_2 + \\dots + X_{1k}\\widehat{\\beta}_k \\\\\n   1\\widehat{\\beta}_0 + X_{21}\\widehat{\\beta}_1 + X_{22}\\widehat{\\beta}_2 + \\dots + X_{2k}\\widehat{\\beta}_k \\\\\n   \\vdots \\\\\n   1\\widehat{\\beta}_0 + X_{n1}\\widehat{\\beta}_1 + X_{n2}\\widehat{\\beta}_2 + \\dots + X_{nk}\\widehat{\\beta}_k\n\\end{array}\n\\right],\n\\] and so the sum of the squared residuals, in this case, becomes \\[\nSSR(\\bfbeta) = \\Vert\\mb{Y} - \\mathbb{X}\\bfbeta\\Vert^{2} = (\\mb{Y} - \\mathbb{X}\\bfbeta)'(\\mb{Y} - \\mathbb{X}\\bfbeta),\n\\] where the double vertical lines mean the Euclidean norm of the argument, \\(\\Vert \\mb{z} \\Vert = \\sqrt{\\sum_{i=1}^n z_i^{2}}\\). The OLS minimization problem, then, is \\[\n\\bhat = \\argmin_{\\mb{b} \\in \\mathbb{R}^{(k+1)}}\\; \\Vert\\mb{Y} - \\mathbb{X}\\mb{b}\\Vert^{2}\n\\] Finally, we can write the orthogonality of the covariates and the residuals as \\[\n\\mathbb{X}'\\widehat{\\mb{e}} = \\sum_{i=1}^{n} \\X_{i}\\widehat{e}_{i} = 0.\n\\]"
  },
  {
    "objectID": "07_least_squares.html#sec-rank",
    "href": "07_least_squares.html#sec-rank",
    "title": "6  The mechanics of least squares",
    "section": "6.4 Rank, linear independence, and multicollinearity",
    "text": "6.4 Rank, linear independence, and multicollinearity\nWhen introducing the OLS estimator, we noted that it would exist when \\(\\sum_{i=1}^n \\X_i\\X_i'\\) is positive definite or that there is “no multicollinearity.” This assumption is equivalent to saying that the matrix \\(\\mathbb{X}\\) is full column rank, meaning that \\(\\text{rank}(\\mathbb{X}) = (k+1)\\), where \\(k+1\\) is the number of columns of \\(\\mathbb{X}\\). Recall from matrix algebra that the column rank is the number of linearly independent columns in the matrix, and linear independence means that \\(\\mathbb{X}\\mb{b} = 0\\) if and only if \\(\\mb{b}\\) is a column vector of 0s. In other words, we have \\[\nb_{1}\\mathbb{X}_{1} + b_{2}\\mathbb{X}_{2} + \\cdots + b_{k+1}\\mathbb{X}_{k+1} = 0 \\quad\\iff\\quad b_{1} = b_{2} = \\cdots = b_{k+1} = 0,\n\\] where \\(\\mathbb{X}_j\\) is the \\(j\\)th column of \\(\\mathbb{X}\\). Thus, full column rank says that all the columns are linearly independent or that there is no “multicollinearity.”\nHow could this be violated? Suppose we accidentally included a linear function of one variable so that \\(\\mathbb{X}_2 = 2\\mathbb{X}_1\\). Then we have, \\[\n\\begin{aligned}\n  \\mathbb{X}\\mb{b} &= b_{1}\\mathbb{X}_{1} + b_{2}2\\mathbb{X}_1+ b_{3}\\mathbb{X}_{3}+ \\cdots + b_{k+1}\\mathbb{X}_{k+1} \\\\\n  &= (b_{1} + 2b_{2})\\mathbb{X}_{1} + b_{3}\\mathbb{X}_{3} + \\cdots + b_{k+1}\\mathbb{X}_{k+1}\n\\end{aligned}\n\\] In this case, this expression equals 0 when \\(b_3 = b_4 = \\cdots = b_{k+1} = 0\\) and \\(b_1 = -2b_2\\). Thus, the collection of columns is linearly dependent, so we know that the rank of \\(\\mathbb{X}\\) must be less than full column rank (that is, less than \\(k+1\\)). Hopefully, it is also clear that if we removed the problematic column \\(\\mathbb{X}_2\\), the resulting matrix would have \\(k\\) linearly independent columns, implying that \\(\\mathbb{X}\\) is rank \\(k\\).\nWhy does this rank condition matter for the OLS estimator? A key property of full column rank matrices is that \\(\\Xmat\\) is of full column rank if and only if \\(\\Xmat'\\Xmat\\) is non-singular and a matrix is invertible if and only if it is non-singular. Thus, the columns of \\(\\Xmat\\) being linearly independent means that the inverse \\((\\Xmat'\\Xmat)^{-1}\\) exists and so does \\(\\bhat\\). Further, this full rank condition also implies that \\(\\Xmat'\\Xmat = \\sum_{i=1}^{n}\\X_{i}\\X_{i}'\\) is positive definite, implying that the estimator is truly finding the minimal sum of squared residuals.\nWhat are common situations that lead to violations of no multicollinearity? We have seen one above, with one variable being a linear function of another. But this problem can come out in more subtle ways. Suppose that we have a set of dummy variables corresponding to a single categorical variable, like the region of the country. In the US, this might mean we have \\(X_{i1} = 1\\) for units in the West (0 otherwise), \\(X_{i2} = 1\\) for units in the Midwest (0 otherwise), \\(X_{i3} = 1\\) for units in the South (0 otherwise), and \\(X_{i4} = 1\\) for units in the Northeast (0 otherwise). Each unit has to be in one of these four regions, so there is a linear dependence between these variables, \\[\nX_{i4} = 1 - X_{i1} - X_{i2} - X_{i3}.\n\\] That is, if I know that you are not in the West, Midwest, or South regions, I know that you are in the Northeast. We would get a linear dependence if we tried to include all of these variables in our regression with an intercept. (Note the 1 in the relationship between \\(X_{i4}\\) and the other variables, that’s why there will be linear dependence when including a constant.) Thus, we usually omit one dummy variable from each categorical variable. In that case, the coefficients on the remaining dummies are differences in means between that category and the omitted one (perhaps conditional on other variables included, if included). So if we omitted \\(X_{i4}\\), then the coefficient on \\(X_{i1}\\) would be the difference in mean outcomes between units in the West and Northeast regions.\nAnother way collinearity can occur is if you include both an intercept term and a variable that does not vary. This issue can often happen if we mistakenly subset our data to, say, the West region but still include the West dummy variable in the regression.\nFinally, note that most statistical software packages will “solve” the multicollinearity by arbitrarily removing as many linearly dependent covariates as is necessary to achieve full rank. R will show the estimated coefficients as NA in those cases."
  },
  {
    "objectID": "07_least_squares.html#ols-coefficients-for-binary-and-categorical-regressors",
    "href": "07_least_squares.html#ols-coefficients-for-binary-and-categorical-regressors",
    "title": "6  The mechanics of least squares",
    "section": "6.5 OLS coefficients for binary and categorical regressors",
    "text": "6.5 OLS coefficients for binary and categorical regressors\nSuppose that the covariates include just the intercept and a single binary variable, \\(\\X_i = (1\\; X_{i})'\\), where \\(X_i \\in \\{0,1\\}\\). In this case, the OLS coefficient on \\(X_i\\), \\(\\widehat{\\beta_{1}}\\), is exactly equal to the difference in sample means of \\(Y_i\\) in the \\(X_i = 1\\) group and the \\(X_i = 0\\) group: \\[\n\\widehat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} X_{i}Y_{i}}{\\sum_{i=1}^{n} X_{i}} - \\frac{\\sum_{i=1}^{n} (1 - X_{i})Y_{i}}{\\sum_{i=1}^{n} 1- X_{i}} = \\overline{Y}_{X =1} - \\overline{Y}_{X=0}\n\\] This result is not an approximation. It holds exactly for any sample size.\nWe can generalize this idea to discrete variables more broadly. Suppose we have our region variables from the last section and include in our covariates a constant and the dummies for the West, Midwest, and South regions. Then the coefficient on the West dummy will be \\[\n\\widehat{\\beta}_{\\text{west}} = \\overline{Y}_{\\text{west}} - \\overline{Y}_{\\text{northeast}},\n\\] which is exactly the difference in sample means of \\(Y_i\\) between the West region and units in the “omitted region,” the Northeast.\nNote that these interpretations only hold when the regression consists solely of the binary variable or the set of categorical dummy variables. These exact relationships fail when other covariates are added to the model."
  },
  {
    "objectID": "07_least_squares.html#projection-and-geometry-of-least-squares",
    "href": "07_least_squares.html#projection-and-geometry-of-least-squares",
    "title": "6  The mechanics of least squares",
    "section": "6.6 Projection and geometry of least squares",
    "text": "6.6 Projection and geometry of least squares\nOLS has a very nice geometric interpretation that can add a lot of intuition for various aspects of the method. In this geometric approach, we view \\(\\mb{Y}\\) as an \\(n\\)-dimensional vector in \\(\\mathbb{R}^n\\). As we saw above, OLS in matrix form is about finding a linear combination of the covariate matrix \\(\\Xmat\\) closest to this vector in terms of the Euclidean distance (which is just the sum of squares).\nLet \\(\\mathcal{C}(\\Xmat) = \\{\\Xmat\\mb{b} : \\mb{b} \\in \\mathbb{R}^(k+1)\\}\\) be the column space of the matrix \\(\\Xmat\\). This set is all linear combinations of the columns of \\(\\Xmat\\) or the set of all possible linear predictions we could obtain from \\(\\Xmat\\). Notice that the OLS fitted values, \\(\\Xmat\\bhat\\), are in this column space. If, as we assume, \\(\\Xmat\\) has full column rank of \\(k+1\\), then the column space \\(\\mathcal{C}(\\Xmat)\\) will be a \\(k+1\\)-dimensional surface inside of the larger \\(n\\)-dimensional space. If \\(\\Xmat\\) has two columns, the column space will be a plane.\nAnother interpretation of the OLS estimator is that it finds the linear predictor as the closest point in the column space of \\(\\Xmat\\) to the outcome vector \\(\\mb{Y}\\). This is called the projection of \\(\\mb{Y}\\) onto \\(\\mathcal{C}(\\Xmat)\\). Figure 6.4 shows this projection for a case with \\(n=3\\) and 2 columns in \\(\\Xmat\\). The shaded blue region represents the plane of the column space of \\(\\Xmat\\), and we can see that \\(\\Xmat\\bhat\\) is the closest point to \\(\\mb{Y}\\) in that space. That’s the whole idea of the OLS estimator: find the linear combination of the columns of \\(\\Xmat\\) (a point in the column space) that minimizes the Euclidean distance between that point and the outcome vector (the sum of squared residuals).\n\n\n\nFigure 6.4: Projection of Y on the column space of the covariates.\n\n\nThis figure shows that the residual vector, which is the difference between the \\(\\mb{Y}\\) vector and the projection \\(\\Xmat\\bhat\\), is perpendicular or orthogonal to the column space of \\(\\Xmat\\). This orthogonality is a consequence of the residuals being orthogonal to all the columns of \\(\\Xmat\\), \\[\n\\Xmat'\\mb{e} = 0,\n\\] as we established above. Being orthogonal to all the columns means it will also be orthogonal to all linear combinations of the columns."
  },
  {
    "objectID": "07_least_squares.html#projection-and-annihilator-matrices",
    "href": "07_least_squares.html#projection-and-annihilator-matrices",
    "title": "6  The mechanics of least squares",
    "section": "6.7 Projection and annihilator matrices",
    "text": "6.7 Projection and annihilator matrices\nNow that we have the idea of projection to the column space of \\(\\Xmat\\), we can define a way to project any vector into that space. The \\(n\\times n\\) projection matrix, \\[\n\\mb{P}_{\\Xmat} = \\Xmat (\\Xmat'\\Xmat)^{-1} \\Xmat',\n\\] projects a vector into \\(\\mathcal{C}(\\Xmat)\\). In particular, we can see that this gives us the fitted values for \\(\\mb{Y}\\): \\[\n\\mb{P}_{\\Xmat}\\mb{Y} = \\Xmat (\\Xmat'\\Xmat)^{-1} \\Xmat'\\mb{Y} = \\Xmat\\bhat.\n\\] Because we sometimes write the linear predictor as \\(\\widehat{\\mb{Y}} = \\Xmat\\bhat\\), the projection matrix is also called the hat matrix. With either name, multiplying a vector by \\(\\mb{P}_{\\Xmat}\\) gives the best linear predictor of that vector as a function of \\(\\Xmat\\). Intuitively, any vector that is already a linear combination of the columns of \\(\\Xmat\\) (so is in \\(\\mathcal{C}(\\Xmat)\\)) should be unaffected by this projection: the closest point in \\(\\mathcal{C}(\\Xmat)\\) to a point already in \\(\\mathcal{C}(\\Xmat)\\) is itself. We can also see this algebraically for any linear combination \\(\\Xmat\\mb{c}\\), \\[\n\\mb{P}_{\\Xmat}\\Xmat\\mb{c} = \\Xmat (\\Xmat'\\Xmat)^{-1} \\Xmat'\\Xmat\\mb{c} = \\Xmat\\mb{c},\n\\] because \\((\\Xmat'\\Xmat)^{-1} \\Xmat'\\Xmat\\) simplifies to the identity matrix. In particular, the projection of \\(\\Xmat\\) onto itself is just itself: \\(\\mb{P}_{\\Xmat}\\Xmat = \\Xmat\\).\nThe second matrix related to projection is the annihilator matrix, \\[\n\\mb{M}_{\\Xmat} = \\mb{I}_{n} - \\mb{P}_{\\Xmat},\n\\] which projects any vector into the orthogonal complement to the column space of \\(\\Xmat\\), \\[\n\\mathcal{C}^{\\perp}(\\Xmat) = \\{\\mb{c} \\in \\mathbb{R}^n\\;:\\; \\Xmat\\mb{c} = 0 \\}.\n\\] This matrix is called the annihilator matrix because if you apply it to any linear combination of \\(\\Xmat\\), you get 0: \\[\n\\mb{M}_{\\Xmat}\\Xmat\\mb{c} = \\Xmat\\mb{c} - \\mb{P}_{\\Xmat}\\Xmat\\mb{c} = \\Xmat\\mb{c} - \\Xmat\\mb{c} = 0,\n\\] and in particular, \\(\\mb{M}_{\\Xmat}\\Xmat = 0\\). Why should we care about this matrix? Perhaps a more evocative name might be the residual maker since it makes residuals when applied to \\(\\mb{Y}\\), \\[\n\\mb{M}_{\\Xmat}\\mb{Y} = (\\mb{I}_{n} - \\mb{P}_{\\Xmat})\\mb{Y} = \\mb{Y} - \\mb{P}_{\\Xmat}\\mb{Y} = \\mb{Y} - \\Xmat\\bhat = \\widehat{\\mb{e}}.\n\\]\nThere are several fundamental properties of the projection matrix that are useful:\n\n\\(\\mb{P}_{\\Xmat}\\) and \\(\\mb{M}_{\\Xmat}\\) are idempotent, which means that when applied to itself, it simply returns itself: \\(\\mb{P}_{\\Xmat}\\mb{P}_{\\Xmat} = \\mb{P}_{\\Xmat}\\) and \\(\\mb{M}_{\\Xmat}\\mb{M}_{\\Xmat} = \\mb{M}_{\\Xmat}\\).\n\\(\\mb{P}_{\\Xmat}\\) and \\(\\mb{M}_{\\Xmat}\\) are symmetric \\(n \\times n\\) matrices so that \\(\\mb{P}_{\\Xmat}' = \\mb{P}_{\\Xmat}\\) and \\(\\mb{M}_{\\Xmat}' = \\mb{M}_{\\Xmat}\\).\nThe rank of \\(\\mb{P}_{\\Xmat}\\) is \\(k+1\\) (the number of columns of \\(\\Xmat\\)) and the rank of \\(\\mb{M}_{\\Xmat}\\) is \\(n - k - 1\\).\n\nWe can use the projection and annihilator matrices to arrive at an orthogonal decomposition of the outcome vector: \\[\n\\mb{Y} = \\Xmat\\bhat + \\widehat{\\mb{e}} = \\mb{P}_{\\Xmat}\\mb{Y} + \\mb{M}_{\\Xmat}\\mb{Y}.\n\\]"
  },
  {
    "objectID": "07_least_squares.html#residual-regression",
    "href": "07_least_squares.html#residual-regression",
    "title": "6  The mechanics of least squares",
    "section": "6.8 Residual regression",
    "text": "6.8 Residual regression\nThere are many situations where we can partition the covariates into two groups, and we might wonder if it is possible how to express or calculate the OLS coefficients for just one set of covariates. In particular, let the columns of \\(\\Xmat\\) be partitioned into \\([\\Xmat_{1} \\Xmat_{2}]\\), so that the linear prediction we are estimating is \\[\n\\mb{Y} = \\Xmat_{1}\\bfbeta_{1} + \\Xmat_{2}\\bfbeta_{2} + \\mb{e},\n\\] with estimated coefficients and residuals \\[\n\\mb{Y} = \\Xmat_{1}\\bhat_{1} + \\Xmat_{2}\\bhat_{2} + \\widehat{\\mb{e}}.\n\\]\nWe now document another way to obtain the estimator \\(\\bhat_1\\) from this regression using a technique called residual regression, partitioned regression, or the Frisch-Waugh-Lovell theorem.\n\n\n\n\n\n\nResidual regression approach\n\n\n\nThe residual regression approach is:\n\nUse OLS to regress \\(\\mb{Y}\\) on \\(\\Xmat_2\\) and obtain residuals \\(\\widetilde{\\mb{e}}_2\\).\nUse OLS to regress each column of \\(\\Xmat_1\\) on \\(\\Xmat_2\\) and obtain residuals \\(\\widetilde{\\Xmat}_1\\).\nUse OLS to regress \\(\\widetilde{\\mb{e}}_{2}\\) on \\(\\widetilde{\\Xmat}_1\\).\n\n\n\n\nTheorem 6.2 (Frisch-Waugh-Lovell) The OLS coefficients from a regression of \\(\\widetilde{\\mb{e}}_{2}\\) on \\(\\widetilde{\\Xmat}_1\\) are equivalent to the coefficients on \\(\\Xmat_{1}\\) from the regression of \\(\\mb{Y}\\) on both \\(\\Xmat_{1}\\) and \\(\\Xmat_2\\).\n\nOne implication of this theorem is that the regression coefficient for a given variable captures the relationship between the residual variation in the outcome and that variable after accounting for the other covariates. In particular, this coefficient focuses on the variation orthogonal to those other covariates.\nWhile perhaps unexpected, this result may not appear particularly useful. We can just run the long regression, right? This trick can be handy when \\(\\Xmat_2\\) consists of dummy variables (or “fixed effects”) for a categorical variable with many categories. For example, suppose \\(\\Xmat_2\\) consists of indicators for the county of residence for a respondent. In that case, that will have over 3,000 columns, meaning that direct calculation of the \\(\\bhat = (\\bhat_{1}, \\bhat_{2})\\) will require inverting a matrix that is bigger than \\(3,000 \\times 3,000\\). Computationally, this process will be very slow. But above, we saw that predictions of an outcome on a categorical variable are just the sample mean within each level of the variable. Thus, in this case, the residuals \\(\\widetilde{\\mb{e}}_2\\) and \\(\\Xmat_1\\) can be computed by demeaning the outcome and \\(\\Xmat_1\\) within levels of the dummies in \\(\\Xmat_2\\), which can be considerably faster computationally.\nFinally, there are data visualization reasons to use residual regression. It is often difficult to see if the linear functional form for some covariate is appropriate once you begin to control for other variables. One can check the relationship using this approach with a scatterplot of \\(\\widetilde{\\mb{e}}_2\\) on \\(\\Xmat_1\\) (when it is a single column)."
  },
  {
    "objectID": "07_least_squares.html#outliers-leverage-points-and-influential-observations",
    "href": "07_least_squares.html#outliers-leverage-points-and-influential-observations",
    "title": "6  The mechanics of least squares",
    "section": "6.9 Outliers, leverage points, and influential observations",
    "text": "6.9 Outliers, leverage points, and influential observations\nGiven that OLS finds the coefficients that minimize the sum of the squared residuals, it is helpful to ask how much impact each residual has on that solution. Let \\(\\bhat_{(-i)}\\) be the OLS estimates if we omit unit \\(i\\). Intuitively, influential observations should significantly impact the estimated coefficients so that \\(\\bhat_{(-i)} - \\bhat\\) is large in absolute value.\nUnder what conditions will we have influential observations? OLS tries to minimize the sum of squared residuals, so it will move more to shrink larger residuals than smaller ones. Where are large residuals likely to occur? Well, notice that any OLS regression line with a constant will go through the means of the outcome and the covariates: \\(\\overline{Y} = \\overline{\\X}\\bhat\\). Thus, by definition, this means that when an observation is close to the average of the covariates, \\(\\overline{\\X}\\), it cannot have that much influence because OLS forces the regression line to go through \\(\\overline{Y}\\). Thus, we should look for influential points that have two properties:\n\nHave high leverage, where leverage roughly measures how far \\(\\X_i\\) is from \\(\\overline{\\X}\\), and\nBe an outlier in the sense of having a large residual (if left out of the regression).\n\nWe’ll take each of these in turn.\n\n6.9.1 Leverage points\nWe can define the leverage of an observation by \\[\nh_{ii} = \\X_{i}'\\left(\\Xmat'\\Xmat\\right)^{-1}\\X_{i},\n\\] which is the \\(i\\)th diagonal entry of the projection matrix, \\(\\mb{P}_{\\Xmat}\\). Notice that \\[\n\\widehat{\\mb{Y}} = \\mb{P}_{\\Xmat}\\mb{Y} \\qquad \\implies \\qquad \\widehat{Y}_i = \\sum_{j=1}^n h_{ij}Y_j,\n\\] so that \\(h_{ij}\\) is the importance of observation \\(j\\) for the fitted value for observation \\(i\\). The leverage, then, is the importance of the observation for its own fitted value. We can also interpret these values in terms of the distribution of \\(\\X_{i}\\). Roughly speaking, these values are the weighted distance \\(\\X_i\\) is from \\(\\overline{\\X}\\), where the weights normalize to the empirical variance/covariance structure of the covariates (so that the scale of each covariate is roughly the same). We can see this most clearly when we fit a simple linear regression (with one covariate and an intercept) with OLS when the leverage is \\[\nh_{ii} = \\frac{1}{n} + \\frac{(X_i - \\overline{X})^2}{\\sum_{j=1}^n (X_j - \\overline{X})^2}\n\\]\nLeverage values have three key properties:\n\n\\(0 \\leq h_{ii} \\leq 1\\)\n\\(h_{ii} \\geq 1/n\\) if the model contains an intercept\n\\(\\sum_{i=1}^{n} h_{ii} = k + 1\\)\n\n\n\n6.9.2 Outliers and leave-one-out regression\nIn the context of OLS, an outlier is an observation with a large prediction error for a particular OLS specification. Figure 6.5 shows an example of an outlier.\n\n\n\n\n\nFigure 6.5: An example of an outlier.\n\n\n\n\nIntuitively, it seems as though we could use the residual \\(\\widehat{e}_i\\) to assess the prediction error for a given unit. But the residuals are not valid predictions because the OLS estimator is designed to make those as small as possible (in machine learning parlance, these were in the training set). In particular, if an outlier is influential, we already noted that it might “pull” the regression line toward it, and the resulting residual might be pretty small.\nTo assess prediction errors more cleanly, we can use leave-one-out regression (LOO), which regresses \\(\\mb{Y}_{(-i)}\\) on \\(\\Xmat_{(-i)}\\), where these omit unit \\(i\\): \\[\n\\bhat_{(-i)} = \\left(\\Xmat'_{(-i)}\\Xmat_{(-i)}\\right)^{-1}\\Xmat_{(-i)}\\mb{Y}_{(-i)}.\n\\] We can then calculate LOO prediction errors as \\[\n\\widetilde{e}_{i} = Y_{i} - \\X_{i}'\\bhat_{(-i)}.\n\\] Calculating these LOO prediction errors for each unit appears to be computationally costly because it seems as though we have to fit OLS \\(n\\) times. Fortunately, there is a closed-form expression for the LOO coefficients and prediction errors in terms of the original regression, \\[\n\\bhat_{(-i)} = \\bhat - \\left( \\Xmat'\\Xmat\\right)^{-1}\\X_i\\widetilde{e}_i \\qquad \\widetilde{e}_i = \\frac{\\widehat{e}_i}{1 - h_{ii}}.\n\\tag{6.1}\\] We can see from this that the LOO prediction errors will differ from the residuals when the leverage of a unit is high. This makes sense! We said earlier that observations with low leverage would be close to \\(\\overline{\\X}\\), where the outcome values have relatively little impact on the OLS fit (because the regression line must go through \\(\\overline{Y}\\)).\n\n\n6.9.3 Influence points\nAn influence point is an observation that has the power to change the coefficients and fitted values for a particular OLS specification. Figure 6.6 shows an example of such an influence point.\n\n\n\n\n\nFigure 6.6: An example of an influence point.\n\n\n\n\nOne measure of influence, called DFBETA\\(_i\\), measures how much \\(i\\) changes the estimated coefficient vector \\[\n\\bhat - \\bhat_{(-i)} = \\left( \\Xmat'\\Xmat\\right)^{-1}\\X_i\\widetilde{e}_i,\n\\] so there is one value for each observation-covariate pair. When divided by the standard error of the estimated coefficients, this is called DFBETAS (where the “S” is for standardized). These are helpful if we focus on a particular coefficient.\nWhen we want to summarize how much an observation matters for the fit, we can use a compact measure of the influence of an observation by comparing the fitted value from the entire sample to the fitted value from the leave-one-out regression. Using the DFBETA above, we have \\[\n\\widehat{Y}_i - \\X_{i}\\bhat_{(-1)} = \\X_{i}'(\\bhat -\\bhat_{(-1)}) = \\X_{i}'\\left( \\Xmat'\\Xmat\\right)^{-1}\\X_i\\widetilde{e}_i = h_{ii}\\widetilde{e}_i,\n\\] so the influence of an observation is its leverage times how much of an outlier it is. This value is sometimes called DFFIT (difference in fit). One transformation of this quantity, Cook’s distance, standardizes this by the sum of the squared residuals: \\[\nD_i = \\frac{n-k-1}{k+1}\\frac{h_{ii}\\widetilde{e}_{i}^{2}}{\\widehat{\\mb{e}}'\\widehat{\\mb{e}}}.\n\\] Various rules exist for establishing cutoffs for identifying an observation as “influential” based on these metrics, but they tend to be ad hoc. In any case, it’s better to focus on the holistic question of “how much does this observation matter for my substantive interpretation” rather than the narrow question of a particular threshold.\nIt’s all well and good to find influential points, but what should you do about it? The first thing to check is that the data is not corrupted somehow. Sometimes influence points occur because of a coding or data entry error. If you have control over that coding, you should fix those errors. You may consider removing the observation if the error appears in the data acquired from another source. Still, when writing up your analyses, you should be extremely transparent about this choice. Another approach is to consider a transformation of the dependent or independent variables, like the natural logarithm, that might dampen the effects of outliers. Finally, consider using methods that are robust to outliers."
  },
  {
    "objectID": "08_ols_properties.html#large-sample-properties-of-ols",
    "href": "08_ols_properties.html#large-sample-properties-of-ols",
    "title": "7  The statistics of least squares",
    "section": "7.1 Large-sample properties of OLS",
    "text": "7.1 Large-sample properties of OLS\nAs we saw in Chapter 3, we need two key ingredients to conduct statistical inference with the OLS estimator: a consistent estimate of the variance of \\(\\bhat\\) and the approximate distribution of \\(\\bhat\\) in large samples. Remember that since \\(\\bhat\\) is a vector, the variance of that estimator will actually be a variance-covariance matrix. To obtain these two ingredients, we will first establish the consistency of OLS and then use the central limit theorem to derive its asymptotic distribution, which will include its variance.\nWe begin by setting out the assumptions we will need for establishing the large-sample properties of OLS, which are the same as the assumptions needed to ensure that the best linear predictor, \\(\\bhat = \\E[\\X_{i}\\X_{i}']^{-1}\\E[\\X_{i}Y_{i}]\\), is well-defined and unique.\n\n\n\n\n\n\nLinear projection assumptions\n\n\n\nThe linear projection model makes the following assumptions:\n\n\\(\\{(Y_{i}, \\X_{i})\\}_{i=1}^n\\) are iid random vectors\n\\(\\E[Y^{2}_{i}] &lt; \\infty\\) (finite outcome variance)\n\\(\\E[\\Vert \\X_{i}\\Vert^{2}] &lt; \\infty\\) (finite variances and covariances of covariates)\n\\(\\E[\\X_{i}\\X_{i}']\\) is positive definite (no linear dependence in the covariates)\n\n\n\nRecall that these are mild conditions on the joint distribution of \\((Y_{i}, \\X_{i})\\) and in particular, we are not assuming linearity of the CEF, \\(\\E[Y_{i} \\mid \\X_{i}]\\), nor are we assuming any specific distribution for the data.\nWe can helpfully decompose the OLS estimator into the actual BLP coefficient plus estimation error as \\[\n\\bhat = \\left( \\frac{1}{n} \\sum_{i=1}^n \\X_i\\X_i' \\right)^{-1} \\left( \\frac{1}{n} \\sum_{i=1}^n \\X_iY_i \\right) = \\bfbeta + \\underbrace{\\left( \\frac{1}{n} \\sum_{i=1}^n \\X_i\\X_i' \\right)^{-1} \\left( \\frac{1}{n} \\sum_{i=1}^n \\X_ie_i \\right)}_{\\text{estimation error}}.\n\\]\nThis decomposition will help us quickly establish the consistency of \\(\\bhat\\). By the law of large numbers, we know that sample means will converge in probability to population expectations, so we have \\[\n\\frac{1}{n} \\sum_{i=1}^n \\X_i\\X_i' \\inprob \\E[\\X_i\\X_i'] \\equiv \\mb{Q}_{\\X\\X} \\qquad \\frac{1}{n} \\sum_{i=1}^n \\X_ie_i \\inprob \\E[\\X_{i} e_{i}] = \\mb{0},\n\\] which implies that \\[\n\\bhat \\inprob \\bfbeta + \\mb{Q}_{\\X\\X}^{-1}\\E[\\X_ie_i] = \\bfbeta,\n\\] by the continuous mapping theorem (the inverse is a continuous function). The linear projection assumptions ensure that LLN applies to these sample means and that \\(\\E[\\X_{i}\\X_{i}']\\) is invertible.\n\nTheorem 7.1 Under the above linear projection assumptions, the OLS estimator is consistent for the best linear projection coefficients, \\(\\bhat \\inprob \\bfbeta\\).\n\nThus, OLS should be close to the population linear regression in large samples under relatively mild conditions. Remember that this might not equal the conditional expectation if the CEF is nonlinear. We can say here that OLS converges to the best linear approximation to the CEF. Of course, this also means that if the CEF is linear, then OLS will consistently estimate the coefficients of the CEF.\nTo emphasize here: the only assumption we made about the dependent variable is that it has finite variance and is iid. Under this assumption, the outcome could be continuous, categorical, binary, or event count.\nNext, we would like to establish an asymptotic normality result for the OLS coefficients. We first review some key ideas about the central limit theorem.\n\n\n\n\n\n\nCLT reminder\n\n\n\nSuppose that we have a function of the data iid random vectors \\(\\X_1, \\ldots, \\X_n\\), \\(g(\\X_{i})\\) where \\(\\E[g(\\X_{i})] = 0\\) and so \\(\\V[g(\\X_{i})] = \\E[g(\\X_{i})g(\\X_{i})']\\). Then if \\(\\E[\\Vert g(\\X_{i})\\Vert^{2}] &lt; \\infty\\), the CLT implies that \\[\n\\sqrt{n}\\left(\\frac{1}{n} \\sum_{i=1}^{n} g(\\X_{i}) - \\E[g(\\X_{i})]\\right) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} g(\\X_{i}) \\indist \\N(0, \\E[g(\\X_{i})g(\\X_{i}')])\n\\tag{7.1}\\]\n\n\nWe now manipulate our decomposition to arrive at the stabilized version of the estimator, \\[\n\\sqrt{n}\\left( \\bhat - \\bfbeta\\right) = \\left( \\frac{1}{n} \\sum_{i=1}^n \\X_i\\X_i' \\right)^{-1} \\left( \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\X_ie_i \\right).\n\\] We have already established that the first term on the right-hand side will converge in probability to \\(\\mb{Q}_{\\X\\X}^{-1}\\). Notice that \\(\\E[\\X_{i}e_{i}] = 0\\), so we can apply Equation 7.1 to the second term. The covariance matrix of \\(\\X_ie_{i}\\) is \\[\n\\mb{\\Omega} = \\V[\\X_{i}e_{i}] = \\E[\\X_{i}e_{i}(\\X_{i}e_{i})'] = \\E[e_{i}^{2}\\X_{i}\\X_{i}'].\n\\] The CLT will imply that \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\X_ie_i \\indist \\N(0, \\mb{\\Omega}).\n\\] Combining these facts with Slutsky’s Theorem implies the following theorem.\n\nTheorem 7.2 Suppose that the linear projection assumptions hold and, in addition, we have \\(\\E[Y_{i}^{4}] &lt; \\infty\\) and \\(\\E[\\lVert\\X_{i}\\rVert^{4}] &lt; \\infty\\). Then the OLS estimator is asymptotically normal with \\[\n\\sqrt{n}\\left( \\bhat - \\bfbeta\\right) \\indist \\N(0, \\mb{V}_{\\bfbeta}),\n\\] where \\[\n\\mb{V}_{\\bfbeta} = \\mb{Q}_{\\X\\X}^{-1}\\mb{\\Omega}\\mb{Q}_{\\X\\X}^{-1} = \\left( \\E[\\X_i\\X_i'] \\right)^{-1}\\E[e_i^2\\X_i\\X_i']\\left( \\E[\\X_i\\X_i'] \\right)^{-1}.\n\\]\n\nThus, if the sample size is large enough, we can approximate the distribution of \\(\\bhat\\) with a multivariate normal with mean \\(\\bfbeta\\) and covariance matrix \\(\\mb{V}_{\\bfbeta}/n\\). In particular, the square root of the \\(j\\)th diagonals of this matrix will be standard errors for \\(\\widehat{\\beta}_j\\). Knowing the shape of the OLS estimator’s multivariate distribution will allow us to conduct hypothesis tests and generate confidence intervals for both individual coefficients and groups of coefficients. But first, we need an estimate of the covariance matrix!"
  },
  {
    "objectID": "08_ols_properties.html#variance-estimation-for-ols",
    "href": "08_ols_properties.html#variance-estimation-for-ols",
    "title": "7  The statistics of least squares",
    "section": "7.2 Variance estimation for OLS",
    "text": "7.2 Variance estimation for OLS\nThe asymptotic normality of OLS from the last section is of limited value without some way to estimate the covariance matrix, \\[\n\\mb{V}_{\\bfbeta} = \\mb{Q}_{\\X\\X}^{-1}\\mb{\\Omega}\\mb{Q}_{\\X\\X}^{-1}.\n\\] Since each term here is a population mean, this is an ideal place to drop a plug-in estimator. In particular, let’s use the following estimators: \\[\n\\begin{aligned}\n  \\mb{Q}_{\\X\\X} &= \\E[\\X_{i}\\X_{i}'] & \\widehat{\\mb{Q}}_{\\X\\X} &= \\frac{1}{n} \\sum_{i=1}^{n} \\X_{i}\\X_{i}' = \\frac{1}{n}\\Xmat'\\Xmat \\\\\n  \\mb{\\Omega} &= \\E[e_i^2\\X_i\\X_i'] & \\widehat{\\mb{\\Omega}} & = \\frac{1}{n}\\sum_{i=1}^n\\widehat{e}_i^2\\X_i\\X_i'.\n\\end{aligned}\n\\] Under the assumptions of Theorem 7.2, the LLN will imply that these are consistent for their targets, \\(\\widehat{\\mb{Q}}_{\\X\\X} \\inprob \\mb{Q}_{\\X\\X}\\) and \\(\\widehat{\\mb{\\Omega}} \\inprob \\mb{\\Omega}\\). We can plug these into the variance formula to arrive at \\[\n\\begin{aligned}\n  \\widehat{\\mb{V}}_{\\bfbeta} &= \\widehat{\\mb{Q}}_{\\X\\X}^{-1}\\widehat{\\mb{\\Omega}}\\widehat{\\mb{Q}}_{\\X\\X}^{-1} \\\\\n  &= \\left( \\frac{1}{n} \\Xmat'\\Xmat \\right)^{-1} \\left( \\frac{1}{n} \\sum_{i=1}^n\\widehat{e}_i^2\\X_i\\X_i' \\right) \\left( \\frac{1}{n} \\Xmat'\\Xmat \\right)^{-1},\n\\end{aligned}\n\\] which by the continuous mapping theorem is consistent, \\(\\widehat{\\mb{V}}_{\\bfbeta} \\inprob \\mb{V}_{\\bfbeta}\\).\nThis estimator is sometimes called the robust variance estimator or, more accurately, the heteroskedasticity-consistent (HC) variance estimator. How is this robust? Consider the standard homoskedasticity assumption that most statistical software packages make when estimating OLS variances: the variance of the errors does not depend on the covariates: \\(\\V[e_{i}^{2} \\mid \\X_{i}] = \\V[e_{i}^{2}]\\). This assumption is stronger than we need, and we can rely on a weaker assumption that the squared errors are uncorrelated with a specific function of the covariates: \\[\n\\E[e_{i}^{2}\\X_{i}\\X_{i}'] = \\E[e_{i}^{2}]\\E[\\X_{i}\\X_{i}'] = \\sigma^{2}\\mb{Q}_{\\X\\X},\n\\] where \\(\\sigma^2\\) is the variance of the residuals (since \\(\\E[e_{i}] = 0\\)). Homoskedasticity simplifies the asymptotic variance of the stabilized estimator, \\(\\sqrt{n}(\\bhat - \\bfbeta)\\), to \\[\n\\mb{V}^{\\texttt{lm}}_{\\bfbeta} = \\mb{Q}_{\\X\\X}^{-1}\\sigma^{2}\\mb{Q}_{\\X\\X}\\mb{Q}_{\\X\\X}^{-1} = \\sigma^2\\mb{Q}_{\\X\\X}^{-1}.\n\\] We already have an estimator for \\(\\mb{Q}_{\\X\\X}\\), but we need one for \\(\\sigma^2\\). We can easily use the SSR, \\[\n\\widehat{\\sigma}^{2} = \\frac{1}{n-k-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2},\n\\] where we use \\(n-k-1\\) in the denominator instead of \\(n\\) to correct for the residuals being slightly less variable than the actual errors (because OLS mechanically attempts to make the residuals small). For consistent variance estimation, \\(n-k -1\\) or \\(n\\) can be used, since either way \\(\\widehat{\\sigma}^2 \\inprob \\sigma^2\\). Thus, under homoskedasticity, we have \\[\n\\widehat{\\mb{V}}_{\\bfbeta}^{\\texttt{lm}} = \\widehat{\\sigma}^{2}\\left(\\Xmat'\\Xmat\\right)^{{-1}},\n\\] which is the standard variance estimator used by lm() in R or reg in Stata.\nNow that we have two estimators, \\(\\widehat{\\mb{V}}_{\\bfbeta}\\) and \\(\\widehat{\\mb{V}}_{\\bfbeta}^{\\texttt{lm}}\\), how do they compare? Notice that the HC variance estimator and the homoskedasticity variance estimator will both be consistent when homoskedasticity holds. But as the “heteroskedasticity-consistent” label implies, only the HC variance estimator will be consistent when homoskedasticity fails to hold. So \\(\\widehat{\\mb{V}}_{\\bfbeta}\\) has the advantage of being consistent regardless of this assumption. This advantage comes at a cost, however. When homoskedasticity is correct, \\(\\widehat{\\mb{V}}_{\\bfbeta}^{\\texttt{lm}}\\) incorporates that assumption into the estimator where the HC variance estimator has to estimate it. The HC estimator will have higher variance (the variance estimator will be more variable!) when homoskedasticity actually does hold.\n\nNow that we have established the asymptotic normality of the OLS estimator and developed a consistent estimator of its variance, we can proceed with all of the statistical inference tools we discussed in Part I of this guide. Define the estimated heteroskedasticity-consistent standard errors as \\[\n\\widehat{\\se}(\\widehat{\\beta}_{j}) = \\sqrt{\\frac{[\\widehat{\\mb{V}}_{\\bfbeta}]_{jj}}{n}},\n\\] where \\([\\widehat{\\mb{V}}_{\\bfbeta}]_{jj}\\) is the \\(j\\)th diagonal entry of the HC variance estimator. Note that we divide by \\(\\sqrt{n}\\) here because \\(\\widehat{\\mb{V}}_{\\bfbeta}\\) is a consistent estimator of the stabilized estimator \\(\\sqrt{n}(\\bhat - \\bfbeta)\\) not the estimator itself.\nHypothesis tests and confidence intervals for individual coefficients are almost precisely the same as with the general case presented in Part I. For a two-sided test of \\(H_0: \\beta_j = b\\) versus \\(H_1: \\beta_j \\neq b\\), we can build the t-statistic and conclude that, under the null, \\[\n\\frac{\\widehat{\\beta}_j - b}{\\widehat{\\se}(\\widehat{\\beta}_{j})} \\indist \\N(0, 1).\n\\] Typically, statistical software will helpfully provide the t-statistic for the null of no (partial) linear relationship between \\(X_{ij}\\) and \\(Y_i\\), \\[\nt = \\frac{\\widehat{\\beta}_{j}}{\\widehat{\\se}(\\widehat{\\beta}_{j})},\n\\] which measures how large the estimated coefficient is in standard errors. With \\(\\alpha = 0.05\\), asymptotic normality would imply that we reject this null when \\(t &gt; 1.96\\). We can form asymptotically-valid confidence intervals with \\[\n\\left[\\widehat{\\beta}_{j} - z_{\\alpha/2}\\;\\widehat{\\se}(\\widehat{\\beta}_{j}),\\;\\widehat{\\beta}_{j} + z_{\\alpha/2}\\;\\widehat{\\se}(\\widehat{\\beta}_{j})\\right].\n\\] For reasons we will discuss below, standard software typically relies on the \\(t\\) distribution instead of the normal for hypothesis testing and confidence intervals. Still, this difference is of little consequence in large samples."
  },
  {
    "objectID": "08_ols_properties.html#inference-for-multiple-parameters",
    "href": "08_ols_properties.html#inference-for-multiple-parameters",
    "title": "7  The statistics of least squares",
    "section": "7.3 Inference for multiple parameters",
    "text": "7.3 Inference for multiple parameters\nWith multiple coefficients, we might have hypotheses that involve more than one coefficient. As an example, let’s focus on a regression with an interaction between two covariates, \\[\nY_i = \\beta_0 + X_i\\beta_1 + Z_i\\beta_2 + X_iZ_i\\beta_3 + e_i.\n\\] Suppose we wanted to test the hypothesis that \\(X_i\\) does not affect the best linear predictor for \\(Y_i\\). That would be \\[\nH_{0}: \\beta_{1} = 0 \\text{ and } \\beta_{3} = 0\\quad\\text{vs}\\quad H_{1}: \\beta_{1} \\neq 0 \\text{ or } \\beta_{3} \\neq 0,\n\\] where we usually write the null more compactly as \\(H_0: \\beta_1 = \\beta_3 = 0\\).\nTo test this null hypothesis, we need a test statistic that discriminates the two hypotheses: it should be large when the alternative is true and small when the null is true. With a single coefficient, we usually test the null hypothesis of \\(H_0: \\beta_j = b_0\\) with the \\(t\\)-statistic, \\[\nt = \\frac{\\widehat{\\beta}_{j} - b_{0}}{\\widehat{\\se}(\\widehat{\\beta}_{j})},\n\\] and we usually take the absolute value, \\(|t|\\), as our measure of how far our estimate is from the null. But notice that we could also use the square of the \\(t\\) statistic, which is \\[\nt^{2} = \\frac{\\left(\\widehat{\\beta}_{j} - b_{0}\\right)^{2}}{\\V[\\widehat{\\beta}_{j}]} = \\frac{n\\left(\\widehat{\\beta}_{j} - b_{0}\\right)^{2}}{[\\mb{V}_{\\bfbeta}]_{[jj]}}.\n\\tag{7.2}\\]\nSo here’s another way to differentiate the null from the alternative: the squared distance between them divided by the variance of the estimate.\nCan we generalize this idea to hypotheses about multiple parameters? Adding the sum of squared distances for each component of the null hypothesis is straightforward. For our interaction example, that would be \\[\n\\widehat{\\beta}_1^2 + \\widehat{\\beta}_3^2,\n\\] but remember that some of the estimated coefficients are noisier than others, so we should account for the uncertainty, just like we did for the \\(t\\)-statistic.\nWith multiple parameters and multiple coefficients, the variances will now require matrix algebra. We can write any hypothesis about linear functions of the coefficients as \\(H_{0}: \\mb{L}\\bfbeta = \\mb{c}\\). For example, in the interaction case, we have \\[\n\\mb{L} =\n\\begin{pmatrix}\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\qquad\n\\mb{c} =\n\\begin{pmatrix}\n  0 \\\\\n  0\n\\end{pmatrix}\n\\] Thus, \\(\\mb{L}\\bfbeta = \\mb{0}\\) is equivalent to \\(\\beta_1 = 0\\) and \\(\\beta_3 = 0\\). Notice that with other \\(\\mb{L}\\) matrices, we could represent more complicated hypotheses like \\(2\\beta_1 - \\beta_2 = 34\\), though we mostly stick to simpler functions. Let \\(\\widehat{\\bs{\\theta}} = \\mb{L}\\bhat\\) be the OLS estimate of the function of the coefficients. By the delta method (discussed in Section 3.9), we have \\[\n\\sqrt{n}\\left(\\mb{L}\\bhat - \\mb{L}\\bfbeta\\right) \\indist \\N(0, \\mb{L}'\\mb{V}_{\\bfbeta}\\mb{L}).\n\\] We can now generalize the squared \\(t\\) statistic in Equation 7.2. In particular, we will take the distances \\(\\mb{L}\\bhat - \\mb{c}\\) weighted by the variance-covariance matrix \\(\\mb{L}'\\mb{V}_{\\bfbeta}\\mb{L}\\), \\[\nW = n(\\mb{L}\\bhat - \\mb{c})'(\\mb{L}'\\mb{V}_{\\bfbeta}\\mb{L})^{-1}(\\mb{L}\\bhat - \\mb{c}),\n\\] which is called the Wald test statistic. This statistic generalizes the ideas of the t-statistic to multiple parameters. With the t-statistic, we recenter to have mean 0 and divide by the standard error to get a variance of 1. If we ignore the middle variance weighting, we have \\((\\mb{L}\\bhat - \\mb{c})'(\\mb{L}\\bhat - \\mb{c})\\) which is just the sum of the squared deviations of the estimates from the null. Including the \\((\\mb{L}'\\mb{V}_{\\bfbeta}\\mb{L})^{-1}\\) weight has the effect of rescaling the distribution of \\(\\mb{L}\\bhat - \\mb{c}\\) to make it rotationally symmetric around 0 (so the resulting dimensions are uncorrelated) with each dimension having an equal variance of 1. In this way, the Wald statistic transforms the random vectors to be mean-centered and have variance 1 (just the t-statistic), but also to have the resulting random variables in the vector be uncorrelated.1\nWhy transform the data in this way? Figure 7.1 shows the contour plot of a hypothetical joint distribution of two coefficients from an OLS regression. We might want to know how far different points in the distribution are from the mean, which in this case is \\((1, 2)\\). Without considering the joint distribution, the circle is obviously closer to the mean than the triangle. However, looking at where the two points are on the distribution, the circle is at a lower contour than the triangle, meaning it is more extreme than the triangle for this particular distribution. The Wald statistic, then, takes into consideration how much of a “climb” it is for \\(\\mb{L}\\bhat\\) to get to \\(\\mb{c}\\) given the distribution of \\(\\mb{L}\\bhat\\).\n\n\n\n\n\nFigure 7.1: Hypothetical joint distribution of two slope coefficients. The circle is closer to the center of the distribution by the standard Euclidean distance, but the triangle is closer once you consider the joint distribution.\n\n\n\n\nIf \\(\\mb{L}\\) only has one row, our Wald statistic is the same as the squared \\(t\\) statistic, \\(W = t^2\\). This fact will help us think about the asymptotic distribution of \\(W\\). Notice that as \\(n\\to\\infty\\), we know that by the asymptotic normality of \\(\\bhat\\), \\[\nt = \\frac{\\widehat{\\beta}_{j} - \\beta_{j}}{\\widehat{\\se}[\\widehat{\\beta}_{j}]} \\indist \\N(0,1)\n\\] so \\(t^2\\) will converge in distribution to a \\(\\chi^2_1\\) (since a \\(\\chi^2_1\\) is just one standard normal squared). After recentering and rescaling by the covariance matrix, \\(W\\) converges to the sum of \\(q\\) squared independent normals, where \\(q\\) is the number of rows of \\(\\mb{L}\\), or equivalently, the number of restrictions implied by the null hypothesis. Thus, under the null hypothesis of \\(\\mb{L}\\bhat = \\mb{c}\\), we have \\(W \\indist \\chi^2_{q}\\).\n\n\n\n\n\n\nChi-squared critical values\n\n\n\nWe can obtain critical values for the \\(\\chi^2_q\\) distribution using the qchisq() function in R. For example, if we wanted to obtain the critical value \\(w\\) that such that \\(\\P(W &gt; w_{\\alpha}) = \\alpha\\) for our two-parameter interaction example, we could use:\n\nqchisq(p = 0.95, df = 2)\n\n[1] 5.991465\n\n\n\n\nWe need to define the rejection region to use the Wald statistic in a test. Because we are squaring each distance in \\(W \\geq 0\\), larger values of \\(W\\) indicate more disagreement with the null in either direction. Thus, for an \\(\\alpha\\)-level test of the joint null, we only need a one-sided rejection region of the form \\(\\P(W &gt; w_{\\alpha}) = \\alpha\\). Obtaining these values is straightforward (see the above callout tip). For \\(q = 2\\) and a \\(\\alpha = 0.05\\), the critical value is roughly 6.\nThe Wald statistic is not a common test provided by standard statistical software functions like lm() in R, though it is fairly straightforward to implement “by hand.” Alternatively, packages like {aod} or {clubSandwich} have implementations of the test. What is reported by most software implementations of OLS (like lm() in R) is the F-statistic, which is \\[\nF = \\frac{W}{q},\n\\] which also typically uses the homoskedastic variance estimator \\(\\mb{V}^{\\texttt{lm}}_{\\bfbeta}\\) in \\(W\\). The p-values reported for such tests use the \\(F_{q,n-k-1}\\) distribution because this is the exact distribution of the \\(F\\) statistic when the errors are (a) homoskedastic and (b) normally distributed. When these assumptions do not hold, the \\(F\\) distribution is not really statistically justified, it is slightly more conservative than the \\(\\chi^2_q\\) distribution, and the inference will converge as \\(n\\to\\infty\\). So it might be justified as an ad hoc small sample adjustment to the Wald test. For example, if we used the \\(F_{q,n-k-1}\\) with the interaction example where \\(q=2\\) and say we have a sample size of \\(n = 100\\). In that case, the critical value for the F test with \\(\\alpha = 0.05\\) is\n\nqf(0.95, df1 = 2, df2 = 100 - 4)\n\n[1] 3.091191\n\n\nThis result implies a critical value of 6.182 on the scale of the Wald statistic (multiplying it by \\(q = 2\\)). Compared to the earlier critical value of 5.991 based on the \\(\\chi^2_2\\) distribution, we can see that the inferences will be very similar even in moderately-sized datasets.\nFinally, note that the F-statistic reported by lm() in R is the test of all the coefficients except the intercept being 0. In modern quantitative social sciences, this test is seldom substantively interesting."
  },
  {
    "objectID": "08_ols_properties.html#finite-sample-properties-with-a-linear-cef",
    "href": "08_ols_properties.html#finite-sample-properties-with-a-linear-cef",
    "title": "7  The statistics of least squares",
    "section": "7.4 Finite-sample properties with a linear CEF",
    "text": "7.4 Finite-sample properties with a linear CEF\nAll the above results have been large-sample properties, and we have not addressed finite-sample properties like the sampling variance or unbiasedness. Under the linear projection assumption above, OLS is generally biased without stronger assumptions. This section introduces the stronger assumption that will allow us to establish stronger properties for OLS. As usual, however, remember that these stronger assumptions can be wrong.\n\n\n\n\n\n\nAssumption: Linear Regression Model\n\n\n\n\nThe variables \\((Y_{i}, \\X_{i})\\) satisfy the linear CEF assumption. \\[\n\\begin{aligned}\n  Y_{i} &= \\X_{i}'\\bfbeta + e_{i} \\\\\n  \\E[e_{i}\\mid \\X_{i}] & = 0.\n\\end{aligned}\n\\]\nThe design matrix is invertible \\(\\E[\\X_{i}\\X_{i}'] &gt; 0\\) (positive definite).\n\n\n\nWe discussed the concept of a linear CEF extensively in Chapter 5. However, recall that the CEF might be linear mechanically if the model is saturated or when there are as many coefficients in the model as there are unique values of \\(\\X_i\\). When a model is not saturated, the linear CEF assumption is just that: an assumption. What can this assumption do? It can actually establish quite a few nice statistical properties in finite samples.\nOne note before we proceed. When focusing on the finite sample inference for OLS, it is customary to focus on its properties conditional on the observed covariates, such as \\(\\E[\\bhat \\mid \\Xmat]\\) or \\(\\V[\\bhat \\mid \\Xmat]\\). The historical reason for this was that the researcher often chose these independent variables, so they were not random. Thus, you’ll sometimes see \\(\\Xmat\\) treated as “fixed” in some older texts, and they might even omit explicit conditioning statements.\n\nTheorem 7.3 Under the linear regression model assumption, OLS is unbiased for the population regression coefficients, \\[\n\\E[\\bhat \\mid \\Xmat] = \\bfbeta,\n\\] and its conditional sampling variance is \\[\n\\mb{\\V}_{\\bhat} = \\V[\\bhat \\mid \\Xmat] = \\left( \\Xmat'\\Xmat \\right)^{-1}\\left( \\sum_{i=1}^n \\sigma^2_i \\X_i\\X_i' \\right) \\left( \\Xmat'\\Xmat \\right)^{-1},\n\\] where \\(\\sigma^2_{i} = \\E[e_{i}^{2} \\mid \\Xmat]\\).\n\n\nProof. To prove the conditional unbiasedness, recall that we can write the OLS estimator as \\[\n\\bhat = \\bfbeta + (\\Xmat'\\Xmat)^{-1}\\Xmat'\\mb{e},\n\\] and so taking (conditional) expectations, we have, \\[\n\\E[\\bhat \\mid \\Xmat] = \\bfbeta + \\E[(\\Xmat'\\Xmat)^{-1}\\Xmat'\\mb{e} \\mid \\Xmat] = \\bfbeta + (\\Xmat'\\Xmat)^{-1}\\Xmat'\\E[\\mb{e} \\mid \\Xmat] = \\bfbeta,\n\\] because under the linear CEF assumption \\(\\E[\\mb{e}\\mid \\Xmat] = 0\\).\nFor the conditional sampling variance, we can use the same decomposition we have, \\[\n\\V[\\bhat \\mid \\Xmat] = \\V[\\bfbeta + (\\Xmat'\\Xmat)^{-1}\\Xmat'\\mb{e} \\mid \\Xmat] = (\\Xmat'\\Xmat)^{-1}\\Xmat'\\V[\\mb{e} \\mid \\Xmat]\\Xmat(\\Xmat'\\Xmat)^{-1}.\n\\] Since \\(\\E[\\mb{e}\\mid \\Xmat] = 0\\), we know that \\(\\V[\\mb{e}\\mid \\Xmat] = \\E[\\mb{ee}' \\mid \\Xmat]\\), which is a matrix with diagonal entries \\(\\E[e_{i}^{2} \\mid \\Xmat] = \\sigma^2_i\\) and off-diagonal entries \\(\\E[e_{i}e_{j} \\Xmat] = \\E[e_{i}\\mid \\Xmat]\\E[e_{j}\\mid\\Xmat] = 0\\), where the first equality follows from the independence of the errors across units. Thus, \\(\\V[\\mb{e} \\mid \\Xmat]\\) is a diagonal matrix with \\(\\sigma^2_i\\) along the diagonal, which means \\[\n\\Xmat'\\V[\\mb{e} \\mid \\Xmat]\\Xmat = \\sum_{i=1}^n \\sigma^2_i \\X_i\\X_i',\n\\] establishing the conditional sampling variance.\n\nThus, for any realization of the covariates, \\(\\Xmat\\), OLS is unbiased for the true regression coefficients \\(\\bfbeta\\). By the law of iterated expectation, we also know that it is unconditionally unbiased2 as well since \\[\n\\E[\\bhat] = \\E[\\E[\\bhat \\mid \\Xmat]] = \\bfbeta.\n\\] The difference between these two statements usually isn’t incredibly meaningful.\nThere are a lot of variances flying around, so it’s helpful to review them. Above, we derived the asymptotic variance of \\(\\mb{Z}_{n} = \\sqrt{n}(\\bhat - \\bfbeta)\\), \\[\n\\mb{V}_{\\bfbeta} = \\left( \\E[\\X_i\\X_i'] \\right)^{-1}\\E[e_i^2\\X_i\\X_i']\\left( \\E[\\X_i\\X_i'] \\right)^{-1},\n\\] which implies that the approximate variance of \\(\\bhat\\) will be \\(\\mb{V}_{\\bfbeta} / n\\) because \\[\n\\bhat = \\frac{Z_n}{\\sqrt{n}} + \\bfbeta \\quad\\implies\\quad \\bhat \\overset{a}{\\sim} \\N(\\bfbeta, n^{-1}\\mb{V}_{\\bfbeta}),\n\\] where \\(\\overset{a}{\\sim}\\) means approximately asymptotically distributed as. Under the linear CEF, the conditional sampling variance of \\(\\bhat\\) has a similar form and will be similar to the\n\\[\n\\mb{V}_{\\bhat} = \\left( \\Xmat'\\Xmat \\right)^{-1}\\left( \\sum_{i=1}^n \\sigma^2_i \\X_i\\X_i' \\right) \\left( \\Xmat'\\Xmat \\right)^{-1} \\approx \\mb{V}_{\\bfbeta} / n.\n\\] In practice, these two derivations lead to basically the same variance estimator. Recall the heteroskedastic-consistent variance estimator \\[\n\\widehat{\\mb{V}}_{\\bfbeta} = \\left( \\frac{1}{n} \\Xmat'\\Xmat \\right)^{-1} \\left( \\frac{1}{n} \\sum_{i=1}^n\\widehat{e}_i^2\\X_i\\X_i' \\right) \\left( \\frac{1}{n} \\Xmat'\\Xmat \\right)^{-1},\n\\] is a valid plug-in estimator for the asymptotic variance and \\[\n\\widehat{\\mb{V}}_{\\bhat} = n^{-1}\\widehat{\\mb{V}}_{\\bfbeta}.\n\\] Thus, in practice, the asymptotic and finite-sample results under a linear CEF justify the same variance estimator.\n\n7.4.1 Linear CEF model under homoskedasticity\nIf we are willing to make a homoskedasticity assumption on the errors, we can derive even stronger results for OLS. Stronger assumptions typically lead to stronger conclusions, but those conclusions may not be robust to assumption violations. But homoskedasticity is such a historically important assumption that statistical software implementations of OLS like lm() in R assume it.\n\n\n\n\n\n\nAssumption: Homoskedasticity with a linear CEF\n\n\n\nIn addition to the linear CEF assumption, we further assume that \\[\n\\E[e_i^2 \\mid \\X_i] = \\E[e_i^2] = \\sigma^2,\n\\] or that variance of the errors does not depend on the covariates.\n\n\n\nTheorem 7.4 Under a linear CEF model with homoskedastic errors, the conditional sampling variance is \\[\n\\mb{V}^{\\texttt{lm}}_{\\bhat} = \\V[\\bhat \\mid \\Xmat] = \\sigma^2 \\left( \\Xmat'\\Xmat \\right)^{-1},\n\\] and the variance estimator \\[\n\\widehat{\\mb{V}}^{\\texttt{lm}}_{\\bhat} = \\widehat{\\sigma}^2 \\left( \\Xmat'\\Xmat \\right)^{-1} \\quad\\text{where,}\\quad \\widehat{\\sigma}^2 = \\frac{1}{n - k - 1} \\sum_{i=1}^n \\widehat{e}_i^2\n\\] is unbiased, \\(\\E[\\widehat{\\mb{V}}^{\\texttt{lm}}_{\\bhat} \\mid \\Xmat] = \\mb{V}^{\\texttt{lm}}_{\\bhat}\\).\n\n\nProof. Under homoskedasticity \\(\\sigma^2_i = \\sigma^2\\) for all \\(i\\). Recall that \\(\\sum_{i=1}^n \\X_i\\X_i' = \\Xmat'\\Xmat\\). Thus, the conditional sampling variance from Theorem 7.3, \\[\n\\begin{aligned}\n\\V[\\bhat \\mid \\Xmat] &= \\left( \\Xmat'\\Xmat \\right)^{-1}\\left( \\sum_{i=1}^n \\sigma^2 \\X_i\\X_i' \\right) \\left( \\Xmat'\\Xmat \\right)^{-1} \\\\ &= \\sigma^2\\left( \\Xmat'\\Xmat \\right)^{-1}\\left( \\sum_{i=1}^n \\X_i\\X_i' \\right) \\left( \\Xmat'\\Xmat \\right)^{-1} \\\\&= \\sigma^2\\left( \\Xmat'\\Xmat \\right)^{-1}\\left( \\Xmat'\\Xmat \\right) \\left( \\Xmat'\\Xmat \\right)^{-1} \\\\&= \\sigma^2\\left( \\Xmat'\\Xmat \\right)^{-1} = \\mb{V}^{\\texttt{lm}}_{\\bhat}.\n\\end{aligned}\n\\]\nFor unbiasedness, we just need to show that \\(\\E[\\widehat{\\sigma}^{2} \\mid \\Xmat] = \\sigma^2\\). Recall that we defined \\(\\mb{M}_{\\Xmat}\\) as the residual-maker because \\(\\mb{M}_{\\Xmat}\\mb{Y} = \\widehat{\\mb{e}}\\). We can use this to connect the residuals to the errors, \\[\n\\mb{M}_{\\Xmat}\\mb{e} = \\mb{M}_{\\Xmat}\\mb{Y} - \\mb{M}_{\\Xmat}\\Xmat\\bfbeta = \\mb{M}_{\\Xmat}\\mb{Y} = \\widehat{\\mb{e}},\n\\] so \\[\n\\V[\\widehat{\\mb{e}} \\mid \\Xmat] = \\mb{M}_{\\Xmat}\\V[\\mb{e} \\mid \\Xmat] = \\mb{M}_{\\Xmat}\\sigma^2,\n\\] where the first equality is because \\(\\mb{M}_{\\Xmat} = \\mb{I}_{n} - \\Xmat (\\Xmat'\\Xmat)^{-1} \\Xmat'\\) is constant conditional on \\(\\Xmat\\). Notice that the diagonal entries of this matrix are the variances of particular residuals \\(\\widehat{e}_i\\) and that the diagonal entries of the annihilator matrix are \\(1 - h_{ii}\\) (since the \\(h_{ii}\\) are the diagonal entries of \\(\\mb{P}_{\\Xmat}\\)). Thus, we have \\[\n\\V[\\widehat{e}_i \\mid \\Xmat] = \\E[\\widehat{e}_{i}^{2} \\mid \\Xmat] = (1 - h_{ii})\\sigma^{2}.\n\\] In the last chapter, we established one property of these leverage values in Section 6.9.1, namely \\(\\sum_{i=1}^n h_{ii} = k+ 1\\), so \\(\\sum_{i=1}^n 1- h_{ii} = n - k - 1\\) and we have \\[\n\\begin{aligned}\n  \\E[\\widehat{\\sigma}^{2} \\mid \\Xmat] &= \\frac{1}{n-k-1} \\sum_{i=1}^{n} \\E[\\widehat{e}_{i}^{2} \\mid \\Xmat] \\\\\n                                      &= \\frac{\\sigma^{2}}{n-k-1} \\sum_{i=1}^{n} 1 - h_{ii} \\\\\n                                      &= \\sigma^{2}.\n\\end{aligned}\n\\] This establishes \\(\\E[\\widehat{\\mb{V}}^{\\texttt{lm}}_{\\bhat} \\mid \\Xmat] = \\mb{V}^{\\texttt{lm}}_{\\bhat}\\).\n\nThus, under the linear CEF model and homoskedasticity of the errors, we have an unbiased variance estimator that is a simple function of the sum of squared residuals and the design matrix. Most statistical software packages estimate standard errors using \\(\\widehat{\\mb{V}}^{\\texttt{lm}}_{\\bhat}\\).\nThe final result we can derive for the linear CEF under homoskedasticity is an optimality result. We might ask ourselves if there is another estimator for \\(\\bfbeta\\) that would outperform OLS in the sense of having a lower sampling variance. Perhaps surprisingly, no linear estimator for \\(\\bfbeta\\) has a lower conditional variance, meaning that OLS is the best linear unbiased estimator, often jovially shortened to BLUE. This result is famously known as the Gauss-Markov Theorem.\n\nTheorem 7.5 Let \\(\\widetilde{\\bfbeta} = \\mb{AY}\\) be a linear and unbiased estimator for \\(\\bfbeta\\). Under the linear CEF model with homoskedastic errors, \\[\n\\V[\\widetilde{\\bfbeta}\\mid \\Xmat] \\geq \\V[\\bhat \\mid \\Xmat].\n\\]\n\n\nProof. Note that if \\(\\widetilde{\\bfbeta}\\) is unbiased then \\(\\E[\\widetilde{\\bfbeta} \\mid \\Xmat] = \\bfbeta\\) and so \\[\n\\bfbeta = \\E[\\mb{AY} \\mid \\Xmat] = \\mb{A}\\E[\\mb{Y} \\mid \\Xmat] = \\mb{A}\\Xmat\\bfbeta,\n\\] which implies that \\(\\mb{A}\\Xmat = \\mb{I}_n\\). Rewrite the competitor as \\(\\widetilde{\\bfbeta} = \\bhat + \\mb{BY}\\) where, \\[\n\\mb{B} = \\mb{A} - \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat'.\n\\] and note that \\(\\mb{A}\\Xmat = \\mb{I}_n\\) implies that \\(\\mb{B}\\Xmat = 0\\). We now have \\[\n\\begin{aligned}\n  \\widetilde{\\bfbeta} &= \\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\mb{Y} \\\\\n                      &= \\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\Xmat\\bfbeta + \\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\mb{e} \\\\\n                      &= \\bfbeta + \\mb{B}\\Xmat\\bfbeta + \\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\mb{e} \\\\\n  &= \\bfbeta + \\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\mb{e}\n\\end{aligned}\n\\] The variance of the competitor is, thus, \\[\n\\begin{aligned}\n  \\V[\\widetilde{\\bfbeta} \\mid \\Xmat]\n  &= \\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\V[\\mb{e}\\mid \\Xmat]\\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)' \\\\\n  &= \\sigma^{2}\\left( \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat' + \\mb{B}\\right)\\left( \\Xmat\\left(\\Xmat'\\Xmat\\right)^{-1} + \\mb{B}'\\right) \\\\\n  &= \\sigma^{2}\\left(\\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat'\\Xmat\\left(\\Xmat'\\Xmat\\right)^{-1} + \\left(\\Xmat'\\Xmat\\right)^{-1}\\Xmat'\\mb{B}' + \\mb{B}\\Xmat\\left(\\Xmat'\\Xmat\\right)^{-1} + \\mb{BB}'\\right)\\\\\n  &= \\sigma^{2}\\left(\\left(\\Xmat'\\Xmat\\right)^{-1} + \\mb{BB}'\\right)\\\\\n  &\\geq \\sigma^{2}\\left(\\Xmat'\\Xmat\\right)^{-1} \\\\\n  &= \\V[\\bhat \\mid \\Xmat]\n\\end{aligned}\n\\] The first equality comes from the properties of covariance matrices; the second is due to homoskedasticity; the fourth is due to \\(\\mb{B}\\Xmat = 0\\), which implies that \\(\\Xmat'\\mb{B}' = 0\\) as well. The fifth inequality holds because matrix products of the form \\(\\mb{BB}'\\) are positive definite if \\(\\mb{B}\\) is of full rank (which we have assumed it is).\n\nIn this proof, we saw that the variance of the competing estimator had variance \\(\\sigma^2\\left(\\left(\\Xmat'\\Xmat\\right)^{-1} + \\mb{BB}'\\right)\\) which we argued was “greater than 0” in the matrix sense, which is also called positive definite. What does this mean practically? Remember that any positive definite matrix must have strictly positive diagonal entries and that the diagonal entries of \\(\\V[\\bhat \\mid \\Xmat]\\) and \\(V[\\widetilde{\\bfbeta}\\mid \\Xmat]\\) are the variances of the individual parameters, \\(\\V[\\widehat{\\beta}_{j} \\mid \\Xmat]\\) and \\(\\V[\\widetilde{\\beta}_{j} \\mid \\Xmat]\\). Thus, the variances of the individual parameters will be larger for \\(\\widetilde{\\bfbeta}\\) than for \\(\\bhat\\).\nMany textbooks cite the Gauss-Markov theorem as a critical advantage of OLS over other methods, but it’s essential to recognize its limitations. It requires linearity and homoskedastic error assumptions, which can be false in many applications.\nFinally, note that while we have shown this result for linear estimators, Hansen (2022) proves a more general version of this result that applies to any unbiased estimator."
  },
  {
    "objectID": "08_ols_properties.html#the-normal-linear-model",
    "href": "08_ols_properties.html#the-normal-linear-model",
    "title": "7  The statistics of least squares",
    "section": "7.5 The normal linear model",
    "text": "7.5 The normal linear model\nFinally, we add the strongest and thus least loved of the classical linear regression assumption: (conditional) normality of the errors. The historical reason to use this assumption was that finite-sample inference hits a roadblock without some knowledge of the sampling distribution of \\(\\bhat\\). Under the linear CEF model, we saw that it was unbiased, and under homoskedasticity, we could produce an unbiased estimator of the conditional variance. But to do hypothesis testing or generate confidence intervals, we need to be able to make probability statements about the estimator, and for that, we need to know its exact distribution. When the sample size is large, we can rely on the CLT and know it is approximately normal. But in small samples, what do we do? Historically, we decided to assume (conditional) normality of the errors to proceed with some knowledge that we were wrong but hopefully not too wrong.\n\n\n\n\n\n\nThe normal linear regression model\n\n\n\nIn addition to the linear CEF assumption, we assume that \\[\ne_i \\mid \\Xmat \\sim \\N(0, \\sigma^2).\n\\]\n\n\nA couple of things to point out:\n\nThe assumption here is not that \\((Y_{i}, \\X_{i})\\) are jointly normal (though this would be sufficient for the assumption to hold), but rather that \\(Y_i\\) is normally distributed conditional on \\(\\X_i\\).\nNotice that the normal regression model has the homoskedasticity assumption baked in.\n\n\nTheorem 7.6 Under the normal linear regression model, we have \\[\n\\begin{aligned}\n  \\bhat \\mid \\Xmat &\\sim \\N\\left(\\bfbeta, \\sigma^{2}\\left(\\Xmat'\\Xmat\\right)^{-1}\\right) \\\\\n  \\frac{\\widehat{\\beta}_{j} - \\beta_{j}}{[\\widehat{\\mb{V}}^{\\texttt{lm}}_{\\bhat}]_{jj}/\\sqrt{n}} &\\sim t_{n-k-1} \\\\\n  W/q &\\sim F_{q, n-k-1}.\n\\end{aligned}\n\\]\n\nThis theorem says that in the normal linear regression model, the coefficients follow a normal distribution, the t-statistics follow a \\(t\\)-distribution, and a transformation of the Wald statistic follows an \\(F\\) distribution. These are exact results and do not rely on large-sample approximations. Under the assumption of conditional normality of the errors, they are as valid for \\(n = 5\\) as for \\(n = 500,000\\).\nFew people believe errors follow a normal distribution, so why even present these results? Unfortunately, most statistical software implementations of OLS implicitly assume this when calculating p-values for tests or constructing confidence intervals. That is, the p-value associated with the \\(t\\)-statistic that lm() outputs in R relies on the \\(t_{n-k-1}\\) distribution, and the critical values used to construct confidence intervals with confint() use that distribution as well. When normality does not hold, there is no principled reason to use the \\(t\\) or the \\(F\\) distributions in this way. But we might hold our nose and use this ad hoc procedure under two rationalizations:\n\n\\(\\bhat\\) is asymptotically normal, but this approximation might be poor in smaller finite samples. The \\(t\\) distribution will make inference more conservative in these cases (wider confidence intervals, smaller test rejection regions), which might help offset the poor approximation of the normal in small samples.\nAs \\(n\\to\\infty\\), the \\(t_{n-k-1}\\) will converge to a standard normal distribution, so the ad hoc adjustment will not matter much for medium to large samples.\n\nThese arguments are not very convincing since it’s unclear whether the \\(t\\) approximation will be any better than the normal in finite samples. But it’s the best we can do to console ourselves as we find more data.\n\n\n\n\nHansen, Bruce E. 2022. “A Modern Gauss–Markov Theorem.” Econometrica 90 (3): 1283–94. https://doi.org/10.3982/ECTA19255."
  },
  {
    "objectID": "08_ols_properties.html#footnotes",
    "href": "08_ols_properties.html#footnotes",
    "title": "7  The statistics of least squares",
    "section": "",
    "text": "The form of the Wald statistic is that of a weighted inner product, \\(\\mb{x}'\\mb{Ay}\\), where \\(\\mb{A}\\) is a symmetric positive-definite weighting matrix.↩︎\nWe are basically ignoring some edge cases when it comes to discrete covariates here. In particular, we assume that \\(\\Xmat'\\Xmat\\) is nonsingular with probability one. However, this can fail if we have a binary covariate since there is some chance (however slight) that the entire column will be all ones or all zeros, which would lead to a singular matrix \\(\\Xmat'\\Xmat\\). Practically this is not a big deal, but it does mean that we have to ignore this issue theoretically or focus on conditional unbiasedness.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "$$\n\\newcommand{\\bs}{\\boldsymbol}\n\\newcommand{\\mb}{\\mathbf}\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\V}{\\mathbb{V}}\n\\newcommand{\\var}{\\text{var}}\n\\newcommand{\\cov}{\\text{cov}}\n\\newcommand{\\N}{\\mathcal{N}}\n\\newcommand{\\Bern}{\\text{Bern}}\n\\newcommand{\\Bin}{\\text{Bin}}\n\\newcommand{\\Pois}{\\text{Pois}}\n\\newcommand{\\Unif}{\\text{Unif}}\n\\newcommand{\\se}{\\textsf{se}}\n\\newcommand{\\au}{\\underline{a}}\n\\newcommand{\\du}{\\underline{d}}\n\\newcommand{\\Au}{\\underline{A}}\n\\newcommand{\\Du}{\\underline{D}}\n\\newcommand{\\xu}{\\underline{x}}\n\\newcommand{\\Xu}{\\underline{X}}\n\\newcommand{\\Yu}{\\underline{Y}}\n\\renewcommand{\\P}{\\mathbb{P}}\n\\newcommand{\\U}{\\mb{U}}\n\\newcommand{\\Xbar}{\\overline{X}}\n\\newcommand{\\Ybar}{\\overline{Y}}\n\\newcommand{\\real}{\\mathbb{R}}\n\\newcommand{\\bbL}{\\mathbb{L}}\n\\renewcommand{\\u}{\\mb{u}}\n\\renewcommand{\\v}{\\mb{v}}\n\\newcommand{\\M}{\\mb{M}}\n\\newcommand{\\X}{\\mb{X}}\n\\newcommand{\\Xmat}{\\mathbb{X}}\n\\newcommand{\\bfx}{\\mb{x}}\n\\newcommand{\\y}{\\mb{y}}\n\\renewcommand{\\bfbeta}{\\bs{\\beta}}\n\\newcommand{\\e}{\\bs{\\epsilon}}\n\\newcommand{\\bhat}{\\widehat{\\bs{\\beta}}}\n\\newcommand{\\XX}{\\Xmat'\\Xmat}\n\\newcommand{\\XXinv}{\\left(\\XX\\right)^{-1}}\n\\newcommand{\\hatsig}{\\hat{\\sigma}^2}\n\\newcommand{\\red}[1]{\\textcolor{red!60}{#1}}\n\\newcommand{\\indianred}[1]{\\textcolor{indianred}{#1}}\n\\newcommand{\\blue}[1]{\\textcolor{blue!60}{#1}}\n\\newcommand{\\dblue}[1]{\\textcolor{dodgerblue}{#1}}\n\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\n\\newcommand{\\inprob}{\\overset{p}{\\to}}\n\\newcommand{\\indist}{\\overset{d}{\\to}}\n\\newcommand{\\eframe}{\\end{frame}}\n\\newcommand{\\bframe}{\\begin{frame}}\n\\newcommand{\\R}{\\textsf{\\textbf{R}}}\n\\newcommand{\\Rst}{\\textsf{\\textbf{RStudio}}}\n\\newcommand{\\rfun}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\rpack}[1]{\\textbf{#1}}\n\\newcommand{\\rexpr}[1]{\\texttt{\\color{magenta}{#1}}}\n\\newcommand{\\filename}[1]{\\texttt{\\color{blue}{#1}}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n$$\n\n\n\n\n\nHansen, Bruce E. 2022. “A Modern Gauss–Markov\nTheorem.” Econometrica 90 (3): 1283–94. https://doi.org/10.3982/ECTA19255.\n\n\nSenn, Stephen. 2012. “Tea for Three: Of Infusions and Inferences\nand Milk in First.” Significance 9 (6): 30–33.\nhttps://doi.org/https://doi.org/10.1111/j.1740-9713.2012.00620.x."
  }
]