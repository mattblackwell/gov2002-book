<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A User’s Guide to Statistical Inference and Regression - 4&nbsp; Hypothesis tests</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06_linear_model.html" rel="next">
<link href="./03_asymptotics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A User’s Guide to Statistical Inference and Regression</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mattblackwell/gov2002-book/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Inference</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_estimation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_hypothesis_tests.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Regression</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_linear_model.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_least_squares.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">The mechanics of least squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_ols_properties.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The statistics of least squares</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-lady-tasting-tea" id="toc-the-lady-tasting-tea" class="nav-link active" data-scroll-target="#the-lady-tasting-tea"><span class="toc-section-number">4.1</span>  The lady tasting tea</a></li>
  <li><a href="#hypotheses" id="toc-hypotheses" class="nav-link" data-scroll-target="#hypotheses"><span class="toc-section-number">4.2</span>  Hypotheses</a></li>
  <li><a href="#the-procedure-of-hypothesis-testing" id="toc-the-procedure-of-hypothesis-testing" class="nav-link" data-scroll-target="#the-procedure-of-hypothesis-testing"><span class="toc-section-number">4.3</span>  The procedure of hypothesis testing</a></li>
  <li><a href="#testing-errors" id="toc-testing-errors" class="nav-link" data-scroll-target="#testing-errors"><span class="toc-section-number">4.4</span>  Testing errors</a></li>
  <li><a href="#determining-the-rejection-region" id="toc-determining-the-rejection-region" class="nav-link" data-scroll-target="#determining-the-rejection-region"><span class="toc-section-number">4.5</span>  Determining the rejection region</a></li>
  <li><a href="#hypothesis-tests-of-the-sample-mean" id="toc-hypothesis-tests-of-the-sample-mean" class="nav-link" data-scroll-target="#hypothesis-tests-of-the-sample-mean"><span class="toc-section-number">4.6</span>  Hypothesis tests of the sample mean</a></li>
  <li><a href="#the-wald-test" id="toc-the-wald-test" class="nav-link" data-scroll-target="#the-wald-test"><span class="toc-section-number">4.7</span>  The Wald test</a></li>
  <li><a href="#p-values" id="toc-p-values" class="nav-link" data-scroll-target="#p-values"><span class="toc-section-number">4.8</span>  p-values</a></li>
  <li><a href="#power-analysis" id="toc-power-analysis" class="nav-link" data-scroll-target="#power-analysis"><span class="toc-section-number">4.9</span>  Power analysis</a></li>
  <li><a href="#exact-tests-under-normal-data" id="toc-exact-tests-under-normal-data" class="nav-link" data-scroll-target="#exact-tests-under-normal-data"><span class="toc-section-number">4.10</span>  Exact tests under normal data</a></li>
  <li><a href="#confidence-intervals-and-hypothesis-tests" id="toc-confidence-intervals-and-hypothesis-tests" class="nav-link" data-scroll-target="#confidence-intervals-and-hypothesis-tests"><span class="toc-section-number">4.11</span>  Confidence intervals and hypothesis tests</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mattblackwell/gov2002-book/edit/main/04_hypothesis_tests.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mattblackwell/gov2002-book/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
$$
\newcommand{\bs}{\boldsymbol}
\newcommand{\mb}{\mathbf}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\text{Bern}}
\newcommand{\Bin}{\text{Bin}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Unif}{\text{Unif}}
\newcommand{\se}{\textsf{se}}
\newcommand{\au}{\underline{a}}
\newcommand{\du}{\underline{d}}
\newcommand{\Au}{\underline{A}}
\newcommand{\Du}{\underline{D}}
\newcommand{\xu}{\underline{x}}
\newcommand{\Xu}{\underline{X}}
\newcommand{\Yu}{\underline{Y}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\U}{\mb{U}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\bbL}{\mathbb{L}}
\renewcommand{\u}{\mb{u}}
\renewcommand{\v}{\mb{v}}
\newcommand{\M}{\mb{M}}
\newcommand{\X}{\mb{X}}
\newcommand{\Xmat}{\mathbb{X}}
\newcommand{\bfx}{\mb{x}}
\newcommand{\y}{\mb{y}}
\renewcommand{\bfbeta}{\bs{\beta}}
\newcommand{\e}{\bs{\epsilon}}
\newcommand{\bhat}{\widehat{\bs{\beta}}}
\newcommand{\XX}{\Xmat'\Xmat}
\newcommand{\XXinv}{\left(\XX\right)^{-1}}
\newcommand{\hatsig}{\hat{\sigma}^2}
\newcommand{\red}[1]{\textcolor{red!60}{#1}}
\newcommand{\indianred}[1]{\textcolor{indianred}{#1}}
\newcommand{\blue}[1]{\textcolor{blue!60}{#1}}
\newcommand{\dblue}[1]{\textcolor{dodgerblue}{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\inprob}{\overset{p}{\to}}
\newcommand{\indist}{\overset{d}{\to}}
\newcommand{\eframe}{\end{frame}}
\newcommand{\bframe}{\begin{frame}}
\newcommand{\R}{\textsf{\textbf{R}}}
\newcommand{\Rst}{\textsf{\textbf{RStudio}}}
\newcommand{\rfun}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\rpack}[1]{\textbf{#1}}
\newcommand{\rexpr}[1]{\texttt{\color{magenta}{#1}}}
\newcommand{\filename}[1]{\texttt{\color{blue}{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hypothesis tests</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Up to now, we have discussed the properties of estimators that allow us to characterize their distributions in finite and large samples. These properties might let us say that, for example, our estimated difference in means is equal to a true average treatment effect on average across repeated samples or that it will converge to the true value in large samples. These properties, however, are properties of repeated samples. As researchers, we will only have access to a single sample. <strong>Statistical inference</strong> is the process of using our single sample to learn about population parameters. Several ways to conduct inference are connected, but one of the most ubiquitous in the sciences is the hypothesis test, which is a kind of statistical thought experiment.</p>
<section id="the-lady-tasting-tea" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-lady-tasting-tea"><span class="header-section-number">4.1</span> The lady tasting tea</h2>
<p>The lady tasting tea exemplifies the core ideas behind hypothesis testing due to R.A. Fisher.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Fisher had prepared tea for his colleague, the algologist Muriel Bristol. Knowing that she preferred milk in her tea, he poured milk into a tea cup and then poured the hot tea into the milk. Bristol rejected the cup, stating that she preferred pouring the tea first, then milk. Fisher was skeptical at the idea anyone could tell the difference between a cup poured milk-first or tea-first. So he and another colleague, William Roach, devised a test to see if Bristol could distinguish the two preparation methods.</p>
<p>Fisher and Roach prepared 8 cups of tea, four milk-first and four tea-first. They then presented the cups to Bristol in a random order (though she knew there were 4 of each type), and she proceeded to identify all of the cups correctly. At first glance, this seems like good evidence that she can tell the difference between the two types, but a skeptic like Fisher raised the question: “could she have just been randomly guessing and got lucky?” This led Fisher to a <strong>statistical thought experiment</strong>: what would the probability of guessing the correct cups be <em>if</em> she were guessing randomly?</p>
<p>To calculate the probability of Bristol’s achievement, we can note that “randomly guessing” here would mean that she was selecting a group of 4 cups to be labeled milk-first from the 8 cups available. Using basic combinatorics, we can calculate there are 70 ways to choose 4 cups among 8, but only 1 of those arrangements would be correct. Thus, if randomly guessing means choosing among those 70 options with equal chance, then the probability of guessing the right set of cups is 1/70 or <span class="math inline">\(\approx 0.014\)</span>. The low probability implies that the hypothesis of random guessing may be implausible.</p>
<p>The story of the lady tasting tea encapsulates many of the core elements of hypothesis testing. Hypothesis testing is about taking our observed estimate (Bristol guessing all the cups correctly) and seeing how likely that observed estimate would be under some assumption or hypothesis about the data-generating process (Bristol was randomly guessing). When the observed estimate is unlikely under the maintained hypothesis, we might view this as evidence against that hypothesis. Thus, hypothesis tests help us assess evidence for particular guesses about the DGP.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the rest of this chapter, we’ll introduce the concepts following the notation in the past chapters. We’ll usually assume that we have a random (iid) sample of random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> from a distribution, <span class="math inline">\(F\)</span>. We’ll focus on estimating some parameter, <span class="math inline">\(\theta\)</span> of this distribution (like the mean, median, variance, etc.). We’ll refer to <span class="math inline">\(\Theta\)</span> as the set of possible values of <span class="math inline">\(\theta\)</span> or the <strong>parameter space</strong>.</p>
</div>
</div>
</section>
<section id="hypotheses" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="hypotheses"><span class="header-section-number">4.2</span> Hypotheses</h2>
<p>In the context of hypothesis testing, hypotheses are just statements about the population distribution. In particular, we will make statements that <span class="math inline">\(\theta = \theta_0\)</span> where <span class="math inline">\(\theta_0 \in \Theta\)</span> is the hypothesized value of <span class="math inline">\(\theta\)</span>. Hypotheses are ubiquitous in empirical work, but here are some examples to give you a flavor:</p>
<ul>
<li>The population proportion of US citizens that identify as Democrats is 0.33.</li>
<li>The population difference in average voter turnout between households who received get-out-the-vote mailers vs.&nbsp;those who did not is 0.</li>
<li>The difference in the average incidence of human rights abuse in countries that signed a human rights treaty vs.&nbsp;those countries that did not sign is 0.</li>
</ul>
<p>Each of these is a statement about the true DGP. The latter two are very common: when <span class="math inline">\(\theta\)</span> represents the difference in means between two groups, then <span class="math inline">\(\theta = 0\)</span> is the hypothesis of no actual difference in population means or no treatment effect (if the causal effect is identified).</p>
<p>The goal of hypothesis testing is to adjudicate between two complementary hypotheses.</p>
<div id="def-null" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1 </strong></span>The two hypotheses in a hypothesis test are called the <strong>null hypothesis</strong> and the <strong>alternative hypothesis</strong>, denoted as <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span>, respectively.</p>
</div>
<p>These hypotheses are complementary, so if the null hypothesis <span class="math inline">\(H_0: \theta \in \Theta_0\)</span>, then the alternative hypothesis is <span class="math inline">\(H_1: \theta \in \Theta_0^c\)</span>. The “null” in null hypothesis might seem odd until you realize that most null hypotheses are that there is no effect of some treatment or no difference in means. For example, suppose <span class="math inline">\(\theta\)</span> is the difference in mean support for expanding legal immigration between a treatment group that received a pro-immigrant message and some facts about immigration and a control group that just received the factual information. Then, the typical null hypothesis would be no difference in means or <span class="math inline">\(H_0: \theta = 0\)</span>, and the alternative would be <span class="math inline">\(H_1: \theta \neq 0\)</span>.</p>
<p>There are two types of tests that differ in the form of their null and alternative hypotheses. A <strong>two-sided test</strong> is of the form <span class="math display">\[
H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta \neq \theta_0,
\]</span> where the “two-sided” part refers to how the alternative contains values of <span class="math inline">\(\theta\)</span> above and below the null value <span class="math inline">\(\theta_0\)</span>. A <strong>one-sided test</strong> has the form <span class="math display">\[
H_0: \theta \leq \theta_0 \quad\text{versus}\quad H_1: \theta &gt; \theta_0,
\]</span> or <span class="math display">\[
H_0: \theta \geq \theta_0 \quad\text{versus}\quad H_1: \theta &lt; \theta_0.
\]</span> Two-sided tests are much more common in the social sciences, where we want to know if there is any evidence, positive or negative, against the presumption of no treatment effect or no relationship between two variables. One-sided tests are for situations where we only want evidence in one direction, which is rarely relevant to social science research. One-sided tests also have the downside of being misused to inflate the strength of evidence against the null and should be avoided. Unfortunately, the math of two-sided tests is also more complicated.</p>
</section>
<section id="the-procedure-of-hypothesis-testing" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="the-procedure-of-hypothesis-testing"><span class="header-section-number">4.3</span> The procedure of hypothesis testing</h2>
<p>At the most basic level, a <strong>hypothesis test</strong> is a rule that specifies values of the sample data for which we will decide to <strong>reject</strong> the null hypothesis. Let <span class="math inline">\(\mathcal{X}_n\)</span> be the range of the sample—that is, all possible vectors <span class="math inline">\((x_1, \ldots, x_n)\)</span> that have positive probability of occurring. Then, a hypothesis test describes a region of this space, <span class="math inline">\(R \subset \mathcal{X}_n\)</span>, called the <strong>rejection region</strong> where when <span class="math inline">\((X_1, \ldots, X_n) \in R\)</span> we will <strong>reject</strong> <span class="math inline">\(H_0\)</span> and when the data is outside this region, <span class="math inline">\((X_1, \ldots, X_n) \notin R\)</span> we <strong>retain</strong>, <strong>accept</strong>, or <strong>fail to reject</strong> the null hypothesis.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>How do we decide what the rejection region should be? Even though we define the rejection region in terms of the <strong>sample space</strong>, <span class="math inline">\(\mathcal{X}_n\)</span>, it’s unwieldy to work with the entire vector of data. Instead, we often formulate the rejection region in terms of a <strong>test statistic</strong>, <span class="math inline">\(T = T(X_1, \ldots, X_n)\)</span>, where the rejection region becomes <span class="math display">\[
R = \left\{(x_1, \ldots, x_n) : T(x_1, \ldots, x_n) &gt; c\right\},
\]</span> where <span class="math inline">\(c\)</span> is called the <strong>critical value</strong>. This expression says that the rejection region is the part of the sample space that makes the test statistic sufficiently large. We reject null hypotheses when the observed data is incompatible with those hypotheses, where the test statistic should be a measure of this incompatibility. Note that the test statistic is a random variable and has a distribution—we will exploit this to understand the different properties of a hypothesis test.</p>
<div id="exm-biden" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 </strong></span>Suppose that <span class="math inline">\((X_1, \ldots, X_n)\)</span> represents a sample of US citizens where <span class="math inline">\(X_i = 1\)</span> indicates support for the current US president and <span class="math inline">\(X_i = 0\)</span> means no support. We might be interested in the test of the null hypothesis that the president does not have the support of a majority of American citizens. Let <span class="math inline">\(\mu = \E[X_i] = \P(X_i = 1)\)</span>. Then, a one-sided test would compare the two hypotheses: <span class="math display">\[
H_0: \mu \leq 0.5 \quad\text{versus}\quad H_1: \mu &gt; 0.5.
\]</span> In this case, we might use the sample mean as the test statistic, so that <span class="math inline">\(T(X_1, \ldots, X_n) = \Xbar_n\)</span> and we have to find some threshold above 0.5 such that we would reject the null, <span class="math display">\[
R = \left\{(x_1, \ldots, x_n): \Xbar_n &gt; c\right\}.
\]</span> In words, how much support should we see for the current president before we reject the notion that they lack majority support? Below we will select the critical value, <span class="math inline">\(c\)</span>, to have beneficial statistical properties.</p>
</div>
<p>The structure of a reject region will depend on whether a test is one- or two-sided. One-sided tests will take the form <span class="math inline">\(T &gt; c\)</span>, whereas two-sided tests will take the form <span class="math inline">\(|T| &gt; c\)</span> since we want to count deviations from either side of the null hypothesis as evidence against that null.</p>
</section>
<section id="testing-errors" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="testing-errors"><span class="header-section-number">4.4</span> Testing errors</h2>
<p>Hypothesis tests end with a decision to reject the null hypothesis or not, but this might be an incorrect decision. In particular, there are two ways to make errors and two ways to be correct in this setting, as shown in <a href="#tbl-errors">Table&nbsp;<span>4.1</span></a>. The labels are confusing, but it’s helpful to remember that <strong>type I errors</strong> (said “type one”) are labeled so because they are the worse of the two types of errors. These errors occur when we reject a null (say there is a true treatment effect or relationship) when the null is true (there is no true treatment effect or relationship). Type I errors are what we see in the replication crisis: lots of “significant” effects that turn out later to be null. <strong>Type II errors</strong> (said “type two”) are considered less problematic: there is a true relationship, but we cannot detect it with our test (we cannot reject the null).</p>
<div id="tbl-errors" class="anchored">
<table class="table">
<caption>Table&nbsp;4.1: Typology of testing errors</caption>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(H_0\)</span> True</th>
<th><span class="math inline">\(H_0\)</span> False</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Retain <span class="math inline">\(H_0\)</span></td>
<td>Awesome</td>
<td>Type II error</td>
</tr>
<tr class="even">
<td>Reject <span class="math inline">\(H_0\)</span></td>
<td>Type I error</td>
<td>Great</td>
</tr>
</tbody>
</table>
</div>
<p>Ideally, we would minimize the chances of making either a type I or type II error. Unfortunately, because the test statistic is a random variable, we cannot remove the probability of an error altogether. Instead, we will derive tests with some guaranteed performance to minimize the probability of type I error. To derive this, we can define the <strong>power function</strong> of a test, <span class="math display">\[
\pi(\theta) = \P\left(  \text{Reject } H_0 \mid \theta \right) = \P\left( T \in R \mid \theta \right),
\]</span> which is the probability of rejection as a function of the parameter of interest, <span class="math inline">\(\theta\)</span>. The power function tells us, for example, how likely we are to reject the null of no treatment effect as we vary the actual size of the treatment effect.</p>
<p>We can define the probability of type I error from the power function.</p>
<div id="def-size" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2 </strong></span>The <strong>size</strong> of a hypothesis test with the null hypothesis <span class="math inline">\(H_0: \theta = \theta_0\)</span> is <span class="math display">\[
\pi(\theta_0) = \P\left( \text{Reject } H_0 \mid \theta_0 \right).
\]</span></p>
</div>
<p>You can think of the size of a test as the rate of false positives (or false discoveries) produced by the test. <a href="#fig-size-power">Figure&nbsp;<span>4.1</span></a> shows an example of rejection regions, size, and power for a one-sided test. In the left panel, we have the distribution of the test statistic under the null, with <span class="math inline">\(H_0: \theta = \theta_0\)</span>, and the rejection region is defined by values <span class="math inline">\(T &gt; c\)</span>. The shaded grey region is the probability of rejection under this null hypothesis or the size of the test. Sometimes, we will get extreme samples by random chance, even under the null, leading to false discoveries.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>In the right panel, we overlay the distribution of the test statistic under one particular alternative, <span class="math inline">\(\theta = \theta_1 &gt; \theta_0\)</span>. The red-shaded region is the probability of rejecting the null when this alternative is true or the power—it’s the probability of correctly rejecting the null when it is false. Intuitively, we can see that alternatives that produce test statistics closer to the rejection region will have higher power. This makes sense: detecting big deviations from the null should be easier than detecting minor ones.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-size-power" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="04_hypothesis_tests_files/figure-html/fig-size-power-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.1: Size of a test and power against an alternative.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><a href="#fig-size-power">Figure&nbsp;<span>4.1</span></a> also hints at a tradeoff between size and power. Notice that we could make the size smaller (lower the false positive rate) by increasing the critical value to <span class="math inline">\(c' &gt; c\)</span>. This would make the probability of being in the rejection region smaller, <span class="math inline">\(\P(T &gt; c' \mid \theta_0) &lt; \P(T &gt; c \mid \theta_0)\)</span>, leading to a lower-sized test. Unfortunately, it would also reduce power in the right panel since the probability of being in the rejection region will be lower under any alternative, <span class="math inline">\(\P(T &gt; c' \mid \theta_1) &lt; \P(T &gt; c \mid \theta_1)\)</span>. This means we usually cannot simultaneously reduce both types of errors.</p>
</section>
<section id="determining-the-rejection-region" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="determining-the-rejection-region"><span class="header-section-number">4.5</span> Determining the rejection region</h2>
<p>If we cannot simultaneously optimize a test’s size and power, how should we determine where the reject region is? That is, how should we decide what empirical evidence will be strong enough for us to reject the null? The standard approach to this problem in hypothesis testing is to control the size of a test (that is, control the rate of false positives) and try to maximize the power of the test subject to that constraint. So we say, “I’m willing to accept at most x%” of findings will be false positives and do whatever we can to maximize power subject to that constraint.</p>
<div id="def-level" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3 </strong></span>A test has <strong>significance level</strong> <span class="math inline">\(\alpha\)</span> if its size is less than or equal to <span class="math inline">\(\alpha\)</span>, or <span class="math inline">\(\pi(\theta_0) \leq \alpha\)</span>.</p>
</div>
<p>A test with a significance level of <span class="math inline">\(\alpha = 0.05\)</span> will have a false positive/type I error rate no larger than 0.05. This level is widespread in the social sciences, though you also will <span class="math inline">\(\alpha = 0.01\)</span> or <span class="math inline">\(\alpha = 0.1\)</span>. Frequentists justify this by saying this means that with <span class="math inline">\(\alpha = 0.05\)</span>, there will only be 5% of studies that will produce false discoveries.</p>
<p>Our task is to construct the rejection region so that the <strong>null distribution</strong> of the test statistic <span class="math inline">\(G_0(t) = \P(T \leq t \mid \theta_0)\)</span> has less than <span class="math inline">\(\alpha\)</span> probability in that region. One-sided tests like in <a href="#fig-size-power">Figure&nbsp;<span>4.1</span></a> are the easiest to show, even though we warned you not to use them. We want to choose <span class="math inline">\(c\)</span> that puts no more than <span class="math inline">\(\alpha\)</span> probability in the tail, or <span class="math display">\[
\P(T &gt; c \mid \theta_0) = 1 - G_0(c) \leq \alpha.
\]</span> Remembering that the smaller the value of <span class="math inline">\(c\)</span> we can use will maximize power, which implies that the critical value for the maximum power while maintaining the significance level is when <span class="math inline">\(1 - G_0(c) = \alpha\)</span>. We can use the <strong>quantile function</strong> of the null distribution to find the exact value of <span class="math inline">\(c\)</span> we need, <span class="math display">\[
c = G^{-1}_0(1 - \alpha),
\]</span> which is just fancy math to say, “the value at which <span class="math inline">\(1-\alpha\)</span> of the null distribution is below.”</p>
<p>The determination of the rejection region follows the same principles for two-sided tests, but it is slightly more complicated because we reject when the magnitude of the test statistic is large, <span class="math inline">\(|T| &gt; c\)</span>. <a href="#fig-two-sided">Figure&nbsp;<span>4.2</span></a> shows that basic setup. Notice that because there are two (disjoint) regions, we can write the size (false positive rate) as <span class="math display">\[
\pi(\theta_0) = G_0(-c) + 1 - G_0(c)
\]</span> In most cases that we will see, the null distribution for such a test will be symmetric around 0 (usually asymptotically standard normal, actually), which means that <span class="math inline">\(G_0(-c) = 1 - G_0(c)\)</span>, which implies that the size is <span class="math display">\[
\pi(\theta_0) = 2(1 - G_0(c)).
\]</span> Solving for the critical value that would make this <span class="math inline">\(\alpha\)</span> gives <span class="math display">\[
c = G^{-1}_0(1 - \alpha/2).
\]</span> Again, this formula can seem dense, but remember what you are doing: finding the value that puts <span class="math inline">\(\alpha/2\)</span> of the probability of the null distribution in each tail.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-two-sided" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="04_hypothesis_tests_files/figure-html/fig-two-sided-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.2: Rejection regions for a two-sided test.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="hypothesis-tests-of-the-sample-mean" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="hypothesis-tests-of-the-sample-mean"><span class="header-section-number">4.6</span> Hypothesis tests of the sample mean</h2>
<p>Let’s go through an extended example about hypothesis testing of a sample mean, sometimes called a <strong>one-sample test</strong>. Let’s say <span class="math inline">\(X_i\)</span> are feeling thermometer scores about “liberals” as a group on a scale of 0 to 100, with values closer to 0 indicating cooler feelings about liberals and values closer to 100 indicating warmer feelings about liberals. We want to know if the population average differs from a neutral value of 50. We can write this two-sided test as <span class="math display">\[
H_0: \mu = 50 \quad\text{versus}\quad H_1: \mu \neq 50,
\]</span> where <span class="math inline">\(\mu = \E[X_i]\)</span>. The standard test statistic for this type of test is the so-called <strong>t-statistic</strong>, <span class="math display">\[
T = \frac{\left( \Xbar_n - \mu_0 \right)}{\sqrt{s^2 / n}} =\frac{\left( \Xbar_n - 50 \right)}{\sqrt{s^2 / n}},
\]</span> where <span class="math inline">\(\mu_0\)</span> is the null value of interest and <span class="math inline">\(s^2\)</span> is the sample variance. If the null hypothesis is true, then by the CLT, we know that the t-statistic is asymptotically normal, <span class="math inline">\(T \indist \N(0, 1)\)</span>. Thus, we can approximate the null distribution with the standard normal!</p>
<p>Let’s create a test with level <span class="math inline">\(\alpha = 0.05\)</span>. Then we need to find the rejection region that puts <span class="math inline">\(0.05\)</span> probability in the tails of the null distribution, which we just saw was <span class="math inline">\(\N(0,1)\)</span>. Let <span class="math inline">\(\Phi()\)</span> be the CDF for the standard normal and let <span class="math inline">\(\Phi^{-1}()\)</span> be the quantile function for the standard normal. Drawing on what we developed above, you can find the value <span class="math inline">\(c\)</span> so that <span class="math inline">\(\P(|T| &gt; c \mid \mu_0)\)</span> is 0.05 with <span class="math display">\[
c = \Phi^{-1}(1 - 0.05/2) \approx 1.96,
\]</span> which means that a test where we reject when <span class="math inline">\(|T| &gt; 1.96\)</span> would have a level of 0.05 asymptotically.</p>
</section>
<section id="the-wald-test" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="the-wald-test"><span class="header-section-number">4.7</span> The Wald test</h2>
<p>We can generalize the hypothesis test for the sample mean to estimators more broadly. Let <span class="math inline">\(\widehat{\theta}_n\)</span> be an estimator for some parameter <span class="math inline">\(\theta\)</span> and let <span class="math inline">\(\widehat{\textsf{se}}[\widehat{\theta}_n]\)</span> be a consistent estimate of the standard error of the estimator, <span class="math inline">\(\textsf{se}[\widehat{\theta}_n] = \sqrt{\V[\widehat{\theta}_n]}\)</span>. We consider the two-sided test <span class="math display">\[
H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta \neq \theta_0.
\]</span></p>
<p>In many cases, our estimators will be asymptotically normal by a version of the CLT so that under the null hypothesis, we have <span class="math display">\[
T = \frac{\widehat{\theta}_n - \theta_0}{\widehat{\textsf{se}}[\widehat{\theta}_n]} \indist \N(0, 1).
\]</span> The <strong>Wald test</strong> rejects <span class="math inline">\(H_0\)</span> when <span class="math inline">\(|T| &gt; z_{\alpha/2}\)</span>, where <span class="math inline">\(z_{\alpha/2}\)</span> that puts <span class="math inline">\(\alpha/2\)</span> in the upper tail of the standard normal. That is, if <span class="math inline">\(Z \sim \N(0, 1)\)</span>, then <span class="math inline">\(z_{\alpha/2}\)</span> satisfies <span class="math inline">\(\P(Z \geq z_{\alpha/2}) = \alpha/2\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In R, you can find the <span class="math inline">\(z_{\alpha/2}\)</span> values easily with the <code>qnorm()</code> function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.05</span> <span class="sc">/</span> <span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.959964</code></pre>
</div>
</div>
</div>
</div>
<div id="thm-wald" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 </strong></span>Asymptotically, the Wald test has size <span class="math inline">\(\alpha\)</span> such that <span class="math display">\[
\P(|T| &gt; z_{\alpha/2} \mid \theta_0) \to \alpha.
\]</span></p>
</div>
<p>This result is very general, and it means that many, many hypothesis tests based on estimators will have the same form. The main difference across estimators will be how we calculate the estimated standard error.</p>
<div id="exm-two-props" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2 (Difference in proportions) </strong></span>In get-out-the-vote (GOTV) experiments, we might randomly assign a group of citizens to receive mailers encouraging them to vote, whereas a control group receives no message. We’ll define the turnout variables in the treatment group <span class="math inline">\(Y_{1}, Y_{2}, \ldots, Y_{n_t}\)</span> as iid draws from a Bernoulli distribution with success <span class="math inline">\(p_t\)</span>, which represents the population turnout rate among treated citizens. The outcomes in the control group <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{n_c}\)</span> are iid draws from another Bernoulli distribution with success <span class="math inline">\(p_c\)</span>, which represents the population turnout rate among citizens not receiving a mailer.</p>
<p>Our goal is to learn about the treatment effect of this treatment on whether or not the citizen votes, <span class="math inline">\(\tau = p_t - p_c\)</span>, and we will use the sample difference in means/proportions as our estimator, <span class="math inline">\(\widehat{\tau} = \Ybar - \Xbar\)</span>. To perform a Wald test, we need to know/estimate the standard error of this estimator. Notice that because these are independent samples, the variance is <span class="math display">\[
\V[\widehat{\tau}_n] =  \V[\Ybar - \Xbar] = \V[\Ybar] + \V[\Xbar] = \frac{p_t(1-p_t)}{n_t} + \frac{p_c(1-p_c)}{n_c},
\]</span> where the third equality comes from the fact that the underlying outcome variables <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_j\)</span> are binary. Obviously, we do not know the true population proportions <span class="math inline">\(p_t\)</span> and <span class="math inline">\(p_c\)</span> (that’s why we’re doing the test!), but we can estimate the standard error by replacing them with their estimates <span class="math display">\[
\widehat{\textsf{se}}[\widehat{\tau}] = \sqrt{\frac{\Ybar(1 -\Ybar)}{n_t} + \frac{\Xbar(1-\Xbar)}{n_c}}.
\]</span></p>
<p>The typical null hypothesis test, in this case, is “no treatment effect” vs.&nbsp;“some treatment effect” or <span class="math display">\[
H_0: \tau = p_t - p_c = 0 \quad\text{versus}\quad H_1: \tau \neq 0,
\]</span> which gives the following test statistic for the Wald test <span class="math display">\[
T = \frac{\Ybar - \Xbar}{\sqrt{\frac{\Ybar(1 -\Ybar)}{n_t} + \frac{\Xbar(1-\Xbar)}{n_c}}}.
\]</span> If we wanted a test with level <span class="math inline">\(\alpha = 0.01\)</span>, we would reject the null when <span class="math inline">\(|T| &gt; 2.58\)</span> since</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.01</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.575829</code></pre>
</div>
</div>
</div>
<div id="exm-diff-in-means" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3 (Difference in means) </strong></span>Let’s take a similar setting to the last example with randomly assigned treatment and control groups, but now the treatment is an appeal for donations, and the outcomes are continuous measures of how much a person donated to the political campaign. Now the treatment data <span class="math inline">\(Y_1, \ldots, Y_{n_t}\)</span> are iid draws from a population with mean <span class="math inline">\(\mu_t = \E[Y_i]\)</span> and population variance <span class="math inline">\(\sigma^2_t = \V[Y_i]\)</span>. The control data <span class="math inline">\(X_1, \ldots, X_{n_c}\)</span> are iid draws (independent of the <span class="math inline">\(Y_i\)</span>) from a population with mean <span class="math inline">\(\mu_c = \E[X_i]\)</span> and population variance <span class="math inline">\(\sigma^2_c = \V[X_i]\)</span>. The parameter of interest is similar to before: the population difference in means, <span class="math inline">\(\tau = \mu_t - \mu_c\)</span>, and we’ll form the usual hypothesis test of <span class="math display">\[
H_0: \tau = \mu_t - \mu_c = 0 \quad\text{versus}\quad H_1: \tau \neq 0.
\]</span></p>
<p>The only difference between this setting and the difference in proportions is the standard error here will be different because we cannot rely on the Bernoulli. Instead, we’ll use our knowledge of the sampling variance of the sample means and independence between the samples to derive <span class="math display">\[  
\V[\widehat{\tau}] = \V[\Ybar] + \V[\Xbar] = \frac{\sigma^2_t}{n_t} + \frac{\sigma^2_c}{n_c},
\]</span> where we can come up with an estimate of the unknown population variance with sample variances <span class="math display">\[  
\widehat{\se}[\widehat{\tau}] = \sqrt{\frac{s^2_t}{n_t} + \frac{s^2_c}{n_c}}.
\]</span> We can use this estimator to derive the Wald test statistic of <span class="math display">\[
T = \frac{\widehat{\tau} - 0}{\widehat{\se}[\widehat{\tau}]} = \frac{\Ybar - \Xbar}{\sqrt{\frac{s^2_t}{n_t} + \frac{s^2_c}{n_c}}},
\]</span> and if we want an asymptotically level of 0.05, we can reject when <span class="math inline">\(|T| &gt; 1.96\)</span>.</p>
</div>
</section>
<section id="p-values" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="p-values"><span class="header-section-number">4.8</span> p-values</h2>
<p>The hypothesis testing framework focuses on actually making a decision in the face of uncertainty. You choose a level of wrongness you are comfortable with (rate of false positives) and then decide null vs.&nbsp;alternative based firmly on the rejection region. When we’re not making a decision, we are somewhat artificially discarding information about the strength of evidence. We “accept” the null if <span class="math inline">\(T = 1.95\)</span> in the last example but reject it if <span class="math inline">\(T = 1.97\)</span> even though these two situations are actually very similar. Just reporting the reject/retain decision also fails to give us a sense of at what other levels we might have rejected the null. Again, this makes sense if we need to make a single decision: other tests don’t matter because we carefully considered our <span class="math inline">\(\alpha\)</span> level test. But in the lower-stakes world of the academic social sciences, we can afford to be more informative.</p>
<p>One alternative to reporting the reject/retain decision is to report a <strong>p-value</strong>.</p>
<div id="def-p-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4 </strong></span>The <strong>p-value</strong> of a test is the probability of observing a test statistic is at least as extreme as the observed test statistic in the direction of the alternative hypothesis.</p>
</div>
<p>The line “in the direction of the alternative hypothesis” deals with the unfortunate headache of one-sided versus two-sided tests. For a one-sided test where larger values of <span class="math inline">\(T\)</span> correspond to more evidence for <span class="math inline">\(H_1\)</span>, the p-value is <span class="math display">\[
\P(T(X_1,\ldots,X_n) &gt; T \mid \theta_0) = 1 - G_0(T),
\]</span> whereas for a (symmetric) two-sided test, we have <span class="math display">\[
\P(|T(X_1, \ldots, X_n)| &gt; |T| \mid \theta_0) = 2(1 - G_0(|T|)).
\]</span></p>
<p>In either case, the interpretation of the p-value is the same. It is the smallest size <span class="math inline">\(\alpha\)</span> at which a test would reject null. Presenting a p-value allows the reader to determine their own <span class="math inline">\(\alpha\)</span> level and determine quickly if the evidence would warrant rejecting <span class="math inline">\(H_0\)</span> in that case. Thus, the p-value is a more <strong>continuous</strong> measure of evidence against the null, where lower values are stronger evidence against the null because the observed result is less likely under the null.</p>
<p>There is a lot of controversy surrounding p-values but most of it focuses on arbitrary p-value cutoffs for determining statistical significance and sometimes publication decisions. These problems are not the fault of p-values but rather the hyper fixation on the reject/retain decision for arbitrary test levels like <span class="math inline">\(\alpha = 0.05\)</span>. It might be best to view p-values as a transformation of the test statistic onto a common scale between 0 and 1.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>People use many statistical shibboleths to purportedly identify people who don’t understand statistics and usually hinge on seemingly subtle differences in interpretation that are easy to miss. If you know the core concepts, the statistical shibboleths tend to be overblown, but it would be malpractice not to flag them for you.</p>
<p>The shibboleth with p-values is that sometimes people interpret them as “the probability that the null hypothesis is true.” Of course, this doesn’t make sense from our definition because the p-values <em>conditions</em> on the null hypothesis—it cannot tell us anything about the probability of that null hypothesis. Instead, the metaphor you should always carry is that hypothesis tests are statistical thought experiments and that p-values answer the question: how likely would my data be if the null were true?</p>
</div>
</div>
</section>
<section id="power-analysis" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="power-analysis"><span class="header-section-number">4.9</span> Power analysis</h2>
<p>Imagine you have spent a large research budget on a big experiment to test your amazing theory, and the results come back and… you fail to reject the null of no treatment effect. When this happens, there are two possible states of the world: the null is true, and you correctly identified that, or the null is false but the test had lower power to detect the true effect. Because of this uncertainty after the fact, it is common for researchers to conduct <strong>power analyses</strong> before running studies that try to forecast what sample size is necessary to ensure you can reject the null under a hypothesized effect size.</p>
<p>Generally power analyses involve calculating the power function <span class="math inline">\(\pi(\theta) = \P(T(X_1, \ldots, X_n) \in R \mid \theta)\)</span> for different values of <span class="math inline">\(\theta\)</span>. It might also involve sample size calculations for a particular alternative, <span class="math inline">\(\theta_1\)</span>. In that case, we try to find the sample size <span class="math inline">\(n\)</span> to make the power <span class="math inline">\(\pi(\theta_1)\)</span> as close to a particular value (often 0.8) as possible. It is possible to solve for this sample size in simple one-sided tests explicitly. Still, for more general situations or two-sided tests, we typically need numerical or simulation-based approaches to find the optimal sample size.</p>
<p>With Wald tests, we can characterize the power function quite easily, even if it does not allow us to back out sample size calculations easily.</p>
<div id="thm-power" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2 </strong></span>For a Wald test with an asymptotically normal estimator, the power function for a particular alternative <span class="math inline">\(\theta_1 \neq \theta_0\)</span> is <span class="math display">\[
\pi(\theta_1) = 1 - \Phi\left( \frac{\theta_0 - \theta_1}{\widehat{\se}[\widehat{\theta}_n]} + z_{\alpha/2} \right) + \Phi\left( \frac{\theta_0 - \theta_1}{\widehat{\se}[\widehat{\theta}_n]}-z_{\alpha/2} \right).
\]</span></p>
</div>
</section>
<section id="exact-tests-under-normal-data" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="exact-tests-under-normal-data"><span class="header-section-number">4.10</span> Exact tests under normal data</h2>
<p>The Wald test above relies on large sample approximations. In finite samples, these approximations may not be valid. Can we get <strong>exact</strong> inferences at any sample size? Yes, if we make stronger assumptions about the data. In particular, assume a <strong>parametric model</strong> for the data where <span class="math inline">\(X_1,\ldots,X_n\)</span> are i.i.d. samples from <span class="math inline">\(N(\mu,\sigma^2)\)</span>. Under null of <span class="math inline">\(H_0: \mu = \mu_0\)</span>, we can show that <span class="math display">\[
T_n = \frac{\Xbar_n - \mu_0}{s_n/\sqrt{n}} \sim t_{n-1},
\]</span> where <span class="math inline">\(t_{n-1}\)</span> is the <strong>Student’s t-distribution</strong> with <span class="math inline">\(n-1\)</span> degrees of freedom. This result implies the null distribution is <span class="math inline">\(t\)</span>, so we use quantiles of <span class="math inline">\(t\)</span> for critical values. For one-sided test <span class="math inline">\(c = G^{-1}_0(1 - \alpha)\)</span> but now <span class="math inline">\(G_0\)</span> is <span class="math inline">\(t\)</span> with <span class="math inline">\(n-1\)</span> df and so we use <code>qt()</code> instead of <code>qnorm()</code> to calculate these critical values.</p>
<p>The critical values for the <span class="math inline">\(t\)</span> distribution are always larger than the normal because the t has fatter tails, as shown in <a href="#fig-shape-of-t">Figure&nbsp;<span>4.3</span></a>. As <span class="math inline">\(n\to\infty\)</span>, however, the <span class="math inline">\(t\)</span> converges to the standard normal, and so it is asymptotically equivalent to the Wald test but slightly more conservative in finite samples. Oddly, most software packages calculate p-values and rejection regions based on the <span class="math inline">\(t\)</span> to exploit this conservativeness.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-shape-of-t" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="04_hypothesis_tests_files/figure-html/fig-shape-of-t-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.3: Normal versus t distribution.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="confidence-intervals-and-hypothesis-tests" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="confidence-intervals-and-hypothesis-tests"><span class="header-section-number">4.11</span> Confidence intervals and hypothesis tests</h2>
<p>At first glance, we may seem sloppy in using <span class="math inline">\(\alpha\)</span> in deriving a <span class="math inline">\(1 - \alpha\)</span> confidence interval in the last chapter and an <span class="math inline">\(\alpha\)</span>-level test in this chapter. In reality, we were foreshadowing the deep connection between the two: every <span class="math inline">\(1-\alpha\)</span> confidence interval contains all null hypotheses that we <strong>would not reject</strong> with an <span class="math inline">\(\alpha\)</span>-level test.</p>
<p>This connection is easiest to see with an asymptotically normal estimator, <span class="math inline">\(\widehat{\theta}_n\)</span>. Consider the hypothesis test of <span class="math display">\[
H_0: \theta = \theta_0 \quad \text{vs.}\quad H_1: \theta \neq \theta_0,
\]</span> using the test statistic, <span class="math display">\[
T = \frac{\widehat{\theta}_{n} - \theta_{0}}{\widehat{\se}[\widehat{\theta}_{n}]}.
\]</span> As we discussed in the earlier, an <span class="math inline">\(\alpha = 0.05\)</span> test would reject this null when <span class="math inline">\(|T| &gt; 1.96\)</span>, or when <span class="math display">\[
|\widehat{\theta}_{n} - \theta_{0}| &gt; 1.96 \widehat{\se}[\widehat{\theta}_{n}].
\]</span> Notice that will be true when <span class="math display">\[
\theta_{0} &lt; \widehat{\theta}_{n} - 1.96\widehat{\se}[\widehat{\theta}_{n}]\quad \text{ or  }\quad  \widehat{\theta}_{n} +  \widehat{\se}[\widehat{\theta}_{n}] &lt; \theta_{0}
\]</span> or, equivalently, that null hypothesis is outside of the 95% confidence interval, <span class="math display">\[\theta_0 \notin \left[\widehat{\theta}_{n} - 1.96\widehat{\se}[\widehat{\theta}_{n}], \widehat{\theta}_{n} + 1.96\widehat{\se}[\widehat{\theta}_{n}]\right].\]</span> Of course, our choice of the null hypothesis was arbitrary, which means that any null hypothesis outside the 95% confidence interval would be rejected by a <span class="math inline">\(\alpha = 0.05\)</span> level test of that null. And any null hypothesis inside the confidence interval is a null hypothesis that we would not reject.</p>
<p>This relationship holds more broadly. Any <span class="math inline">\(1-\alpha\)</span> confidence interval contains all possible parameter values that would not be rejected as the null hypothesis of an <span class="math inline">\(\alpha\)</span>-level hypothesis test. This connection can be handy for two reasons:</p>
<ol type="1">
<li>We can quickly determine if we would reject a null hypothesis at some level by inspecting if it falls in a confidence interval.</li>
<li>In some situations, determining a confidence interval might be difficult, but performing a hypothesis test is straightforward. Then, we can find the rejection region for the test and determine what null hypotheses would not be rejected at level <span class="math inline">\(\alpha\)</span> to formulate the <span class="math inline">\(1-\alpha\)</span> confidence interval. We call this process <strong>inverting a test</strong>. A critical application of this method is for formulating confidence intervals for treatment effects based on randomization inference in the finite population analysis of experiments.</li>
</ol>
<!--# TODO: add example of inverting a test?-->


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-Senn12" class="csl-entry" role="doc-biblioentry">
Senn, Stephen. 2012. <span>“Tea for Three: Of Infusions and Inferences and Milk in First.”</span> <em>Significance</em> 9 (6): 30–33. https://doi.org/<a href="https://doi.org/10.1111/j.1740-9713.2012.00620.x">https://doi.org/10.1111/j.1740-9713.2012.00620.x</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The analysis here largely comes from <span class="citation" data-cites="Senn12">Senn (<a href="references.html#ref-Senn12" role="doc-biblioref">2012</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Different people and different textbooks describe what to do when do not reject the null hypothesis in different ways. The terminology is not so important so long as you understand that rejecting the null does not mean the null is logically false, and “accepting” the null does not mean the null is logically true.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Eagle-eyed readers will notice that the null tested here is a point, while we previously defined the null in a one-sided test as a region <span class="math inline">\(H_0: \theta \leq \theta_0\)</span>. Technically, the size of the test will vary based on which of these nulls we pick. In this example, notice that any null to the left of <span class="math inline">\(\theta_0\)</span> will result in a lower size. And so, the null at the boundary, <span class="math inline">\(\theta_0\)</span>, will maximize the size of the test, making it the most “conservative” null to investigate. Technically, we should define the size of a test as <span class="math inline">\(\alpha = \sup_{\theta \in \Theta_0} \pi(\theta)\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03_asymptotics.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Asymptotics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06_linear_model.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>